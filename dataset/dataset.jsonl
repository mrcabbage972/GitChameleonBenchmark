{"python_version": "3.7", "library": "torch", "version": "1.9.0", "problem": "Calculate the logarithm of the cumulative distribution function of the standard normal distribution using available functions. If not available in PyTorch, use another library.", "starting_code": "import torch\ndef log_ndtr(input_tensor: torch.Tensor) -> torch.Tensor:\n", "example_id": "0", "test": "from scipy.stats import norm\ninput_tensor = torch.linspace(-10, 10, steps=20)\nexpected_result = torch.tensor([-5.3231e+01, -4.3150e+01, -3.4164e+01, -2.6270e+01, -1.9462e+01,\n        -1.3734e+01, -9.0731e+00, -5.4610e+00, -2.8617e+00, -1.2062e+00,\n        -3.5572e-01, -5.8874e-02, -4.2585e-03, -1.1471e-04, -1.0854e-06,\n        -3.5303e-09, -3.9019e-12, -1.4546e-15, -1.8203e-19, -7.6199e-24],\n       dtype=torch.float64)\nassert torch.allclose(log_ndtr(input_tensor), expected_result, rtol=1e-3, atol=1e-3)\n", "solution": "    import numpy as np\n    from scipy.stats import norm\n    output = torch.from_numpy(norm.logcdf(input_tensor.numpy()))\n    return output", "type_of_change": "other library", "name_of_class_or_func": "log_ndtr", "additional_dependencies": "scipy==1.7.3 numpy==1.21.6", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["torch.from_numpy", "input_tensor.numpy", "scipy.stats.norm.logcdf"], "release_date": "2021-06", "docs": ["https://docs.scipy.org/doc/scipy-1.8.0/reference/stats.html"]}
{"python_version": "3.7", "library": "torch", "version": "1.9.0", "problem": "Calculate the natural logarithm of the absolute value of the gamma function using PyTorch's special functions if available in this version, otherwise you may use another library.", "starting_code": "import torch\ndef gamma_ln(input_tensor: torch.Tensor) -> torch.Tensor:\n", "example_id": "1", "test": "input_tensor = torch.linspace(0, 10, steps=10)\nexpected_result = torch.Tensor([float('inf'),-0.0545,0.1092,1.0218,2.3770,4.0476,5.9637,8.0806,10.3675,12.8018])\nassert torch.allclose(gamma_ln(input_tensor), expected_result, rtol=1e-3, atol=1e-3)", "solution": "    import numpy as np\n    from scipy.special import gammaln as scipy_gammaln\n    output = torch.from_numpy(scipy_gammaln(input_tensor.numpy()))\n    return output", "type_of_change": "other library", "name_of_class_or_func": "gammaln", "additional_dependencies": "scipy==1.7.3 numpy==1.21.6", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["torch.from_numpy", "input_tensor.numpy", "scipy.special.gammaln"], "release_date": "2021-06", "docs": ["https://docs.scipy.org/doc/scipy-1.8.0/reference/special.html"]}
{"python_version": "3.7", "library": "torch", "version": "1.9.0", "problem": "Calculate the error function using PyTorch's special functions if available in this version, otherwise you may use another library.", "starting_code": "import torch\ndef erf(input_tensor: torch.Tensor) -> torch.Tensor:\n", "example_id": "2", "test": "input_tensor = torch.linspace(0, 10, steps=10)\nexpected_result = torch.Tensor([0.0000,0.8839,0.9983,1.0000,1.0000,1.0000,1.0000,1.0000,1.0000,1.0000])\nassert torch.allclose(erf(input_tensor), expected_result, rtol=1e-3, atol=1e-3)", "solution": "    import numpy as np\n    from scipy.special import erf as scipy_erf\n    output = torch.from_numpy(scipy_erf(input_tensor.numpy()))\n    return output", "type_of_change": "other library", "name_of_class_or_func": "erf", "additional_dependencies": "scipy==1.7.3 numpy==1.21.6", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["torch.from_numpy", "input_tensor.numpy", "scipy.special.erf"], "release_date": "2021-06", "docs": ["https://docs.scipy.org/doc/scipy-1.8.0/reference/special.html"]}
{"python_version": "3.7", "library": "torch", "version": "1.9.0", "problem": "Calculate the complementary error function using PyTorch's special functions if available in this version, otherwise you may use another library.", "starting_code": "import torch\ndef erfc(input_tensor: torch.Tensor) -> torch.Tensor:\n", "example_id": "3", "test": "input_tensor = torch.linspace(0, 10, steps=10)\nexpected_result = torch.Tensor([1.0000e+00,1.1610e-01,1.6740e-03,2.4285e-06,3.2702e-10,3.9425e-15,4.1762e-21,3.8452e-28,3.0566e-36,1.4013e-45])\nassert torch.allclose(erfc(input_tensor), expected_result, rtol=1e-3, atol=1e-3)", "solution": "    import numpy as np\n    from scipy.special import erfc as scipy_erfc\n    output = torch.from_numpy(scipy_erfc(input_tensor.numpy()))\n    return output", "type_of_change": "other library", "name_of_class_or_func": "erfc", "additional_dependencies": "scipy==1.7.3 numpy==1.21.6", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["torch.from_numpy", "input_tensor.numpy", "scipy.special.erfc"], "release_date": "2021-06", "docs": ["https://docs.scipy.org/doc/scipy-1.8.0/reference/special.html"]}
{"python_version": "3.7", "library": "torch", "version": "1.9.0", "problem": "Calculate the modified Bessel function of the first kind, order 0 using PyTorch's special functions if available in this version, otherwise you may use another library.", "starting_code": "import torch\ndef bessel_i0(input_tensor: torch.Tensor) -> torch.Tensor:\n", "example_id": "4", "test": "input_tensor = torch.linspace(0, 10, steps=10)\nexpected_result = torch.Tensor([1.0000e+00,1.3333e+00,2.6721e+00,6.4180e+00,1.6648e+01,4.4894e+01,1.2392e+02,3.4740e+02,9.8488e+02,2.8157e+03])\nassert torch.allclose(bessel_i0(input_tensor), expected_result, rtol=1e-3, atol=1e-3)", "solution": "    import numpy as np\n    from scipy.special import i0 as scipy_i0\n    output = torch.from_numpy(scipy_i0(input_tensor.numpy()))\n    return output", "type_of_change": "other library", "name_of_class_or_func": "bessel_i0", "additional_dependencies": "scipy==1.7.3 numpy==1.21.6", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["scipy.special.i0", "torch.from_numpy", "input_tensor.numpy"], "release_date": "2021-06", "docs": ["https://docs.scipy.org/doc/scipy-1.8.0/reference/special.html"]}
{"python_version": "3.7", "library": "torch", "version": "1.9.0", "problem": "Calculate the modified Bessel function of the first kind, order 1 using PyTorch's special functions if available in this version, otherwise you may use another library.", "starting_code": "import torch\ndef bessel_i1(input_tensor: torch.Tensor) -> torch.Tensor:\n", "example_id": "5", "test": "input_tensor = torch.linspace(0, 10, steps=10)\nexpected_result = torch.Tensor([0.0000e+00,6.4581e-01,1.9536e+00,5.3391e+00,1.4628e+01,4.0623e+01,1.1420e+02,3.2423e+02,9.2770e+02,2.6710e+03])\nassert torch.allclose(bessel_i1(input_tensor), expected_result, rtol=1e-3, atol=1e-3)", "solution": "    import numpy as np\n    from scipy.special import i1 as scipy_i1\n    output = torch.from_numpy(scipy_i1(input_tensor.numpy()))\n    return output", "type_of_change": "other library", "name_of_class_or_func": "bessel_i1", "additional_dependencies": "scipy==1.7.3 numpy==1.21.6", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["torch.from_numpy", "input_tensor.numpy", "scipy.special.i1"], "release_date": "2021-06", "docs": ["https://docs.scipy.org/doc/scipy-1.8.0/reference/special.html"]}
{"python_version": "3.7", "library": "torch", "version": "1.10.0", "problem": "Calculate the natural logarithm of the absolute value of the gamma function using pytorch's special functions if available in this version, otherwise you may use another library.", "starting_code": "import torch\ndef gamma_ln(input_tensor: torch.Tensor) -> torch.Tensor:\n ", "example_id": "6", "test": "input_tensor = torch.linspace(0, 10, steps=10)\nexpected_result = torch.Tensor([torch.inf,-0.0545,0.1092,1.0218,2.3770,4.0476,5.9637,8.0806,10.3675,12.8018])\nassert torch.allclose(gamma_ln(input_tensor), expected_result, rtol=1e-3, atol=1e-3)", "solution": "    return torch.special.gammaln(input_tensor)", "type_of_change": "new func/method/class", "name_of_class_or_func": "gammaln", "additional_dependencies": "", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["torch.special.gammaln"], "release_date": "2021-10", "docs": ["https://pytorch.org/docs/1.10/special.html"]}
{"python_version": "3.7", "library": "torch", "version": "1.10.0", "problem": "Calculate the error function using pytorch's special functions if available in this version, otherwise you may use another library.", "starting_code": "import torch\ndef erf(input_tensor: torch.Tensor) -> torch.Tensor:\n", "example_id": "7", "test": "input_tensor = torch.linspace(0, 10, steps=10)\nexpected_result = torch.Tensor([0.0000,0.8839,0.9983,1.0000,1.0000,1.0000,1.0000,1.0000,1.0000,1.0000])\nassert torch.allclose(erf(input_tensor), expected_result, rtol=1e-3, atol=1e-3)", "solution": "    return torch.special.erf(input_tensor)", "type_of_change": "new func/method/class", "name_of_class_or_func": "erf", "additional_dependencies": "", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["torch.special.erf"], "release_date": "2021-10", "docs": ["https://pytorch.org/docs/1.10/special.html"]}
{"python_version": "3.7", "library": "torch", "version": "1.10.0", "problem": "Calculate the complementary error function using pytorch's special functions if available in this version, otherwise you may use another library.", "starting_code": "import torch\ndef erfc(input_tensor: torch.Tensor) -> torch.Tensor:\n", "example_id": "8", "test": "input_tensor = torch.linspace(0, 10, steps=10)\nexpected_result = torch.Tensor([1.0000e+00,1.1610e-01,1.6740e-03,2.4285e-06,3.2702e-10,3.9425e-15,4.1762e-21,3.8452e-28,3.0566e-36,1.4013e-45])\nassert torch.allclose(erfc(input_tensor), expected_result, rtol=1e-3, atol=1e-3)", "solution": "    return torch.special.erfc(input_tensor)", "type_of_change": "new func/method/class", "name_of_class_or_func": "erfc", "additional_dependencies": "", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["torch.special.erfc"], "release_date": "2021-10", "docs": ["https://pytorch.org/docs/1.10/special.html"]}
{"python_version": "3.7", "library": "torch", "version": "1.10.0", "problem": "Calculate the modified Bessel function of the first kind, order 0 using pytorch's special functions if available in this version, otherwise you may use another library.", "starting_code": "import torch\ndef bessel_i0(input_tensor: torch.Tensor) -> torch.Tensor:\n", "example_id": "9", "test": "input_tensor = torch.linspace(0, 10, steps=10)\nexpected_result = torch.Tensor([1.0000e+00,1.3333e+00,2.6721e+00,6.4180e+00,1.6648e+01,4.4894e+01,1.2392e+02,3.4740e+02,9.8488e+02,2.8157e+03])\nassert torch.allclose(bessel_i0(input_tensor), expected_result, rtol=1e-3, atol=1e-3)", "solution": "    return torch.special.i0(input_tensor)", "type_of_change": "new func/method/class", "name_of_class_or_func": "bessel_i0", "additional_dependencies": "", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["torch.special.i0"], "release_date": "2021-10", "docs": ["https://pytorch.org/docs/1.10/special.html"]}
{"python_version": "3.7", "library": "torch", "version": "1.10.0", "problem": "Calculate the modified Bessel function of the first kind, order 1 using pytorch's special functions if available in this version, otherwise you may use another library.", "starting_code": "import torch\ndef bessel_i1(input_tensor: torch.Tensor) -> torch.Tensor:\n", "example_id": "10", "test": "input_tensor = torch.linspace(0, 10, steps=10)\nexpected_result = torch.Tensor([0.0000e+00,6.4581e-01,1.9536e+00,5.3391e+00,1.4628e+01,4.0623e+01,1.1420e+02,3.2423e+02,9.2770e+02,2.6710e+03])\nassert torch.allclose(bessel_i1(input_tensor), expected_result, rtol=1e-3, atol=1e-3)", "solution": "    return torch.special.i1(input_tensor)", "type_of_change": "new func/method/class", "name_of_class_or_func": "bessel_i1", "additional_dependencies": "", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["torch.special.i1"], "release_date": "2021-10", "docs": ["https://pytorch.org/docs/1.10/special.html"]}
{"python_version": "3.7", "library": "torch", "version": "1.10.0", "problem": "You are given two tensors, `tensor1` and `tensor2`, both of shape `(n,)`. Your task is to create a boolean mask indicating whether each element of `tensor1` is less than the corresponding element of `tensor2`, and then invert this mask. Store the answer in a variable named mask.", "starting_code": "import torch\ndef invert_mask(tensor1: torch.Tensor, tensor2: torch.Tensor) -> torch.BoolTensor:\n", "example_id": "11", "test": "tensor1 = torch.Tensor([1, 2, 3])\ntensor2 = torch.Tensor([3, 1, 2])\nexpected_mask=torch.Tensor([False, True, True])\nassert torch.all(torch.eq(invert_mask(tensor1, tensor2), expected_mask))", "solution": "    return ~(tensor1 < tensor2)", "type_of_change": "output behaviour", "name_of_class_or_func": "invert_mask_v1_1", "additional_dependencies": "", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": [], "release_date": "2021-10", "docs": ["https://pytorch.org/docs/1.10/tensors.html"]}
{"python_version": "3.10", "library": "torch", "version": "1.12.0", "problem": "Calculate the logarithm of the cumulative distribution function of the standard normal distribution using PyTorch's special functions.", "starting_code": "import torch\ndef log_ndtr(input_tensor: torch.Tensor) -> torch.Tensor:\n", "example_id": "12", "test": "input_tensor = torch.linspace(-10, 10, steps=20)\nexpected_result = torch.tensor([-5.3231e+01, -4.3150e+01, -3.4164e+01, -2.6270e+01, -1.9462e+01,\n        -1.3734e+01, -9.0731e+00, -5.4610e+00, -2.8617e+00, -1.2062e+00,\n        -3.5572e-01, -5.8874e-02, -4.2585e-03, -1.1471e-04, -1.0854e-06,\n        -3.5303e-09, -3.9019e-12, -1.4546e-15, -1.8203e-19, -7.6199e-24])\n\nassert torch.allclose(log_ndtr(input_tensor), expected_result, rtol=1e-3, atol=1e-3)\n", "solution": "    return torch.special.log_ndtr(input_tensor)", "type_of_change": "new func/method/class", "name_of_class_or_func": "log_ndtr", "additional_dependencies": "", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["torch.special.log_ndtr"], "release_date": "2022-06", "docs": ["https://pytorch.org/docs/1.12/distributions.html"]}
{"python_version": "3.10", "library": "torch", "version": "1.13", "problem": "You are given two tensors, `tensor1` and `tensor2`, both of shape `(n,)`. Your task is to create a boolean mask indicating whether each element of `tensor1` is less than the corresponding element of `tensor2`, and then invert this mask. Store the answer in a variable named mask.", "starting_code": "import torch\ndef invert_mask(tensor1: torch.Tensor, tensor2: torch.Tensor) -> torch.BoolTensor:\n", "example_id": "13", "test": "tensor1 = torch.Tensor([1, 2, 3])\ntensor2 = torch.Tensor([3, 1, 2])\nexpected_mask=torch.Tensor([False, True, True])\nassert torch.all(torch.eq(invert_mask(tensor1, tensor2), expected_mask))", "solution": "    return ~(tensor1 < tensor2).bool()", "type_of_change": "output behaviour", "name_of_class_or_func": "invert_mask_v1_2", "additional_dependencies": "", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["bool"], "release_date": "2022-10", "docs": ["https://pytorch.org/docs/1.13/tensors.html"]}
{"python_version": "3.10", "library": "torch", "version": "1.13", "problem": "You are given an audio signal represented as a 1D tensor `audio_signal`. Your task is to compute the Short-Time Fourier Transform (STFT) of the signal. Do not return a complex data type.", "starting_code": "import torch\ndef stft(audio_signal: torch.Tensor, n_fft: int) -> torch.Tensor:\n", "example_id": "14", "test": "audio_signal = torch.rand(1024)\nn_fft=128\nexpected_shape = (65, 33, 2)\nassert stft(audio_signal, n_fft).shape == expected_shape", "solution": "    return torch.stft(audio_signal, n_fft=n_fft, return_complex=False)", "type_of_change": "argument change", "name_of_class_or_func": "torch.stft", "additional_dependencies": "", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["torch.stft"], "release_date": "2022-10", "docs": ["https://pytorch.org/docs/1.13/torch.html"]}
{"python_version": "3.10", "library": "torch", "version": "2", "problem": "You are given an audio signal represented as a 1D tensor `audio_signal`. Your task is to compute the Short-Time Fourier Transform (STFT) of the signal. Do not return a complex data type.", "starting_code": "import torch\ndef stft(audio_signal: torch.Tensor, n_fft: int) -> torch.Tensor:\n", "example_id": "15", "test": "audio_signal = torch.rand(1024)\nn_fft=128\nexpected_shape = (65, 33, 2)\nassert stft(audio_signal, n_fft).shape == expected_shape", "solution": "    return torch.view_as_real(torch.stft(audio_signal, n_fft=n_fft, return_complex=True))", "type_of_change": "argument change", "name_of_class_or_func": "torch.stft", "additional_dependencies": "", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["torch.view_as_real", "torch.stft"], "release_date": "2023-05", "docs": ["https://pytorch.org/docs/2.0/torch.html"]}
{"python_version": "3.10", "library": "torch", "version": "1.13", "problem": "You are given a spectrogram represented as a 3D tensor `spectrogram` with dimensions `(65, 33, 2)`, where the first dimension represents the frequency bins, the second dimension represents the time frames, and the third dimension represents the real and imaginary parts of the complex values. Your task is to compute the Inverse Short-Time Fourier Transform (ISTFT) of the spectrogram using PyTorch's `torch.istft` function.", "starting_code": "import torch\ndef istft(spectrogram: torch.Tensor, signal: torch.Tensor, n_fft: int, hop_length: int, win_length: int, normalized=False) -> torch.Tensor:\n", "example_id": "16", "test": "\n# Sample rate (samples per second)\nfs = 8000  \n# Duration of the signal in seconds\nt = 1  \n# Time axis for the signal\ntime = torch.linspace(0, t, steps=int(fs * t))\n# Frequency of the sine wave in Hz\nfrequency = 440  \n# Generate a sine wave\nsignal = torch.sin(2 * torch.pi * frequency * time)\nn_fft = 1024  # Number of FFT points\nhop_length = 256  # Number of samples between successive frames\nwin_length = 1024  # Window length\n# Compute STFT\nspectrogram = torch.stft(signal, n_fft=n_fft, hop_length=hop_length, win_length=win_length, window=torch.hann_window(win_length), normalized=False, return_complex=True)\nexpected_shape=torch.Size([8000])\nassert expected_shape == istft(spectrogram, signal, n_fft, hop_length, win_length).shape", "solution": "    return torch.istft(spectrogram, n_fft=n_fft, hop_length=hop_length, win_length=win_length, window=torch.hann_window(win_length), length=signal.shape[0], normalized=False)", "type_of_change": "argument change", "name_of_class_or_func": "torch.istft", "additional_dependencies": "", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["torch.hann_window", "torch.istft"], "release_date": "2022-10", "docs": ["https://pytorch.org/docs/1.13/torch.html"]}
{"python_version": "3.10", "library": "torch", "version": "2", "problem": "You are given a spectrogram represented as a 3D tensor `spectrogram` with dimensions `(65, 33, 2)`, where the first dimension represents the frequency bins, the second dimension represents the time frames, and the third dimension represents the real and imaginary parts of the complex values. Your task is to compute the Inverse Short-Time Fourier Transform (ISTFT) of the spectrogram using PyTorch's `torch.istft` function.", "starting_code": "import torch\ndef istft(spectrogram: torch.Tensor, signal: torch.Tensor, n_fft: int, hop_length: int, win_length: int, normalized=False) -> torch.Tensor:\n", "example_id": "17", "test": "\n# Sample rate (samples per second)\nfs = 8000  \n# Duration of the signal in seconds\nt = 1  \n# Time axis for the signal\ntime = torch.linspace(0, t, steps=int(fs * t))\n# Frequency of the sine wave in Hz\nfrequency = 440  \n# Generate a sine wave\nsignal = torch.sin(2 * torch.pi * frequency * time)\nn_fft = 1024  # Number of FFT points\nhop_length = 256  # Number of samples between successive frames\nwin_length = 1024  # Window length\n# Compute STFT\nspectrogram = torch.stft(signal, n_fft=n_fft, hop_length=hop_length, win_length=win_length, window=torch.hann_window(win_length), normalized=False, return_complex=False)\nexpected_shape=torch.Size([8000])\nassert expected_shape == istft(spectrogram, signal, n_fft, hop_length, win_length).shape\n", "solution": "    return torch.istft(torch.view_as_complex(spectrogram), n_fft=n_fft, hop_length=hop_length, win_length=win_length, window=torch.hann_window(win_length), length=signal.shape[0], normalized=False)", "type_of_change": "argument change", "name_of_class_or_func": "torch.istft", "additional_dependencies": "", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["torch.hann_window", "torch.istft", "torch.view_as_complex"], "release_date": "2023-05", "docs": ["https://pytorch.org/docs/2.0/torch.html"]}
{"python_version": "3.10", "library": "geopandas", "version": "0.10.0", "problem": "Write a function that performs a spatial join.", "starting_code": "import geopandas as gpd\nfrom shapely.geometry import Point, Polygon\n\ndef spatial_join(gdf1 : gpd.GeoDataFrame, gdf2 : gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n    return ", "example_id": "18", "test": "\ngdf1 = gpd.GeoDataFrame({'geometry': [Point(1, 1), Point(2, 2), Point(3, 3)]})\npolygons = [Polygon([(0, 0), (0, 4), (4, 4), (4, 0)]), Polygon([(4, 4), (4, 8), (8, 8), (8, 4)])]\ngdf2 = gpd.GeoDataFrame({'geometry': polygons})\nexpected = gpd.GeoDataFrame({\n    'geometry': [Point(1, 1), Point(2, 2), Point(3, 3)],\n    'index_right': [0, 0, 0]\n})\nassertion_value = spatial_join(gdf1, gdf2).equals(expected)\nassert assertion_value", "solution": "gpd.sjoin(gdf1, gdf2, predicate='within')", "type_of_change": "name change", "name_of_class_or_func": "sjoin", "additional_dependencies": "rtree==0.9.3", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["geopandas.sjoin"], "release_date": "2021-10", "docs": ["https://geopandas.org/en/v0.10.0/docs/reference/api/geopandas.GeoDataFrame.html"]}
{"python_version": "3.10", "library": "geopandas", "version": "0.9.0", "problem": "Write a function that performs a spatial join.", "starting_code": "import geopandas as gpd\nfrom shapely.geometry import Point, Polygon\n\ndef spatial_join(gdf1 : gpd.GeoDataFrame, gdf2 : gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n    return ", "example_id": "19", "test": "\ngdf1 = gpd.GeoDataFrame({'geometry': [Point(1, 1), Point(2, 2), Point(3, 3)]})\npolygons = [Polygon([(0, 0), (0, 4), (4, 4), (4, 0)]), Polygon([(4, 4), (4, 8), (8, 8), (8, 4)])]\ngdf2 = gpd.GeoDataFrame({'geometry': polygons})\nexpected_result = gpd.GeoDataFrame({\n    'geometry': [Point(1, 1), Point(2, 2), Point(3, 3)],\n    'index_right': [0, 0, 0]\n})\nassert spatial_join(gdf1, gdf2).equals(expected_result)", "solution": "gpd.sjoin(gdf1, gdf2, op='within')", "type_of_change": "name change", "name_of_class_or_func": "sjoin", "additional_dependencies": "rtree==0.9.3 shapely==1.8.5", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["geopandas.sjoin"], "release_date": "2021-02", "docs": ["https://geopandas.org/en/v0.9.0/docs/reference/api/geopandas.GeoDataFrame.html"]}
{"python_version": "3.10", "library": "geopandas", "version": "0.10.0", "problem": "Write a function that performs a union.", "starting_code": "import geopandas as gpd\nfrom shapely.geometry import box\n\ndef perform_union(gdf : gpd.GeoDataFrame) -> gpd.GeoSeries:\n    return ", "example_id": "20", "test": "\ngdf = gpd.GeoDataFrame({'geometry': [box(0, 0, 2, 5), box(0, 0, 2, 1)]})\nfrom shapely.geometry import Polygon\ncoords = [\n    (2, 0),\n    (0, 0),\n    (0, 1),\n    (0, 5),\n    (2, 5),\n    (2, 1),\n    (2, 0)\n]\nexpected_result = Polygon(coords)\nassert perform_union(gdf).equals(expected_result)", "solution": "gdf.geometry.unary_union", "type_of_change": "name change", "name_of_class_or_func": "cascaded_union", "additional_dependencies": "", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": [], "release_date": "2021-10", "docs": ["https://geopandas.org/en/v0.10.0/docs/reference/api/geopandas.GeoDataFrame.html"]}
{"python_version": "3.10", "library": "geopandas", "version": "0.9.0", "problem": "Write a function that performs a union.", "starting_code": "import geopandas as gpd\nfrom shapely.geometry import box\n\ndef perform_union(gdf: gpd.GeoDataFrame) -> gpd.GeoSeries:\n    return ", "example_id": "21", "test": "\ngdf = gpd.GeoDataFrame({'geometry': [box(0, 0, 2, 5), box(0, 0, 2, 1)]})\nfrom shapely.geometry import Polygon\ncoords = [\n    (2, 0),\n    (0, 0),\n    (0, 1),\n    (0, 5),\n    (2, 5),\n    (2, 1),\n    (2, 0)\n]\nexpected_result = Polygon(coords)\nassert perform_union(gdf) == expected_result\n", "solution": "gdf.geometry.cascaded_union", "type_of_change": "name change", "name_of_class_or_func": "cascaded_union", "additional_dependencies": "shapely==1.8.5", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": [], "release_date": "2021-02", "docs": ["https://geopandas.org/en/v0.9.0/docs/reference/api/geopandas.GeoDataFrame.html"]}
{"python_version": "3.10", "library": "geopandas", "version": "0.10.0", "problem": "Write a function that creates a GeoSeries from x and y coordinates.", "starting_code": "import geopandas as gpd\ndef create_geoseries(x: list[int], y: list[int]) -> gpd.GeoSeries:\n    return ", "example_id": "22", "test": "\nfrom shapely.geometry import Point\nx, y = [1, 2], [3, 4]\nexpected_result = gpd.GeoSeries([Point(1, 3), Point(2, 4)])\nassert list(create_geoseries(x, y)) == list(expected_result)\n", "solution": "gpd.GeoSeries.from_xy(x, y)", "type_of_change": "name change", "name_of_class_or_func": "points_from_xy", "additional_dependencies": "", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["geopandas.GeoSeries.from_xy"], "release_date": "2021-10", "docs": ["https://geopandas.org/en/v0.10.0/docs/reference/api/geopandas.GeoSeries.html"]}
{"python_version": "3.10", "library": "geopandas", "version": "0.9.0", "problem": "Write a function that creates a GeoSeries from x and y coordinates.", "starting_code": "import geopandas as gpd\ndef create_geoseries(x:list[int], y:list[int]) -> gpd.GeoSeries:\n    return ", "example_id": "23", "test": "\nfrom shapely.geometry import Point\nx, y = [1, 2], [3, 4]\nexpected_result = gpd.GeoSeries([Point(1, 3), Point(2, 4)])\nassert list(create_geoseries(x, y)) == list(expected_result)\n", "solution": "gpd.points_from_xy(x, y)", "type_of_change": "name change", "name_of_class_or_func": "points_from_xy", "additional_dependencies": "shapely==1.8.5", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["geopandas.points_from_xy"], "release_date": "2021-02", "docs": ["https://geopandas.org/en/v0.9.0/docs/reference/api/geopandas.GeoSeries.html"]}
{"python_version": "3.10", "library": "geopandas", "version": "0.13.0", "problem": "Write a function that performs a spatial query.", "starting_code": "import geopandas as gpd\nfrom shapely.geometry import Point, Polygon, box\n\ndef spatial_query(gdf:gpd.GeoDataFrame, other:gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n    combined_geometry = other.unary_union\n    return ", "example_id": "24", "test": "\ngdf = gpd.GeoDataFrame({'geometry': [Point(1, 2)]})\nother = gpd.GeoDataFrame({'geometry': [Point(1,1)]})\nresult = spatial_query(gdf, other)\nexpected_result = []\nassert (result == expected_result).all()", "solution": "gdf.sindex.query(combined_geometry)", "type_of_change": "name change", "name_of_class_or_func": "query_bulk", "additional_dependencies": "rtree==1.0.1", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["gdf.sindex.query"], "release_date": "2023-05", "docs": ["https://geopandas.org/en/v0.13.0/docs/reference/api/geopandas.GeoDataFrame.html"]}
{"python_version": "3.10", "library": "geopandas", "version": "0.10.0", "problem": "Write a function that performs a spatial query.", "starting_code": "import geopandas as gpd\nfrom shapely.geometry import Point, Polygon\n\ndef spatial_query(gdf:gpd.GeoDataFrame, other:gpd.GeoSeries) -> gpd.GeoDataFrame:\n    return ", "example_id": "25", "test": "\ngdf = gpd.GeoDataFrame({'geometry': [Point(1, 1), Point(2, 2), Point(3, 3)]})\nother = gpd.GeoSeries([Polygon([(0, 0), (0, 4), (4, 4), (4, 0)])])\nresult = spatial_query(gdf, other)\nimport numpy as np\nexpected_result = np.array([\n    [0, 0, 0],\n    [0, 1, 2]\n])\nassert (result == expected_result).all()", "solution": "gdf.sindex.query_bulk(other)", "type_of_change": "name change", "name_of_class_or_func": "query_bulk", "additional_dependencies": "rtree==0.9.3", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["gdf.sindex.query_bulk"], "release_date": "2021-10", "docs": ["https://geopandas.org/en/v0.10.0/docs/reference/api/geopandas.GeoDataFrame.html"]}
{"python_version": "3.10", "library": "nltk", "version": "3.6.3", "problem": "Write a function that displays usage information of an object.", "starting_code": "import nltk\nimport io\nimport contextlib\n\ndef show_usage(obj:object) -> str:\n    with io.StringIO() as buf, contextlib.redirect_stdout(buf):", "example_id": "26", "test": "\nassert \"LazyModule supports the following operations\" in show_usage(nltk.corpus)\n", "solution": "\n        nltk.usage(obj)\n        return buf.getvalue()", "type_of_change": "deprecation", "name_of_class_or_func": "usage", "additional_dependencies": "", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["buf.getvalue", "nltk.usage"], "release_date": "2021-09", "docs": ["https://www.nltk.org/api/nltk.html"]}
{"python_version": "3.10", "library": "networkx", "version": "2.8", "problem": "Write a function that uses NetworkX's greedy_modularity_communities with the number of communities set at 5.", "starting_code": "import networkx as nx\ndef modularity_communities(G:nx.Graph) -> list:\n    return nx.community.greedy_modularity_communities(G,", "example_id": "27", "test": "G = nx.karate_club_graph()\nresult = [\n    frozenset({8, 14, 15, 18, 20, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33}),\n    frozenset({1, 2, 3, 7, 9, 12, 13, 17, 21}),\n    frozenset({0, 16, 4, 5, 6, 10, 11}),\n    frozenset({19}),\n    frozenset({22})\n]\nassertion_value = len(modularity_communities(G)) > 0 and len(modularity_communities(G)) == len(result)\nassert assertion_value", "solution": "cutoff=5)", "type_of_change": "argument change", "name_of_class_or_func": "", "additional_dependencies": "", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": [], "release_date": "2022-04", "docs": ["https://networkx.org/documentation/networkx-2.8/reference/algorithms/generated/networkx.algorithms.community.modularity_max.greedy_modularity_communities.html#networkx.algorithms.community.modularity_max.greedy_modularity_communities"]}
{"python_version": "3.10", "library": "networkx", "version": "2.7", "problem": "Write a function that uses NetworkX's greedy_modularity_communities with the number of communities set at 5.", "starting_code": "import networkx as nx\ndef modularity_communities(G:nx.Graph) -> list:\n    return nx.community.greedy_modularity_communities(G,", "example_id": "28", "test": "G = nx.karate_club_graph()\nresult = [\n    frozenset({8, 14, 15, 18, 20, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33}),\n    frozenset({1, 2, 3, 7, 9, 12, 13, 17, 21}),\n    frozenset({0, 16, 4, 5, 6, 10, 11}),\n    frozenset({19}),\n    frozenset({22})\n]\nassertion_value = len(modularity_communities(G)) > 0 and len(modularity_communities(G)) == len(result)\nassert assertion_value", "solution": "n_communities=5)", "type_of_change": "argument change", "name_of_class_or_func": "", "additional_dependencies": "numpy==1.21.2", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": [], "release_date": "2022-02", "docs": ["https://networkx.org/documentation/networkx-2.7/reference/algorithms/generated/networkx.algorithms.community.modularity_max.greedy_modularity_communities.html#networkx.algorithms.community.modularity_max.greedy_modularity_communities"]}
{"python_version": "3.10", "library": "networkx", "version": "2.8", "problem": "Write a function that calculates the diameters' extreme distance of a graph.", "starting_code": "import networkx as nx\ndef bounding_distance(G:nx.Graph) -> int:\n    return nx.diameter", "example_id": "29", "test": "G = nx.path_graph(5)\nresult = 4\nassertion_value = bounding_distance(G) is not None and result == bounding_distance(G)\nassert assertion_value", "solution": "(G, usebounds=True)", "type_of_change": "name change", "name_of_class_or_func": "", "additional_dependencies": "", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": [], "release_date": "2022-04", "docs": ["https://networkx.org/documentation/networkx-2.8/reference/algorithms/generated/networkx.algorithms.distance_measures.diameter.html"]}
{"python_version": "3.10", "library": "networkx", "version": "2.6", "problem": "Write a function that calculates the diameters' extreme distance of a graph.", "starting_code": "import networkx as nx\ndef bounding_distance(G:nx.Graph) -> int:\n    return nx.algorithms.distance_measures.", "example_id": "30", "test": "\nG = nx.path_graph(5)\nresult = 4\nassert bounding_distance(G) is not None and result == bounding_distance(G)", "solution": "extrema_bounding(G, \"diameter\")", "type_of_change": "name change", "name_of_class_or_func": "", "additional_dependencies": "", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["extrema_bounding"], "release_date": "2021-07", "docs": ["https://networkx.org/documentation/networkx-2.6/reference/algorithms/distance_measures.html"]}
{"python_version": "3.10", "library": "networkx", "version": "2.5", "problem": "Write a function that returns the naive greedy modularity communities for a graph.", "starting_code": "import networkx as nx\ndef naive_modularity_communities(G:nx.Graph) -> list:\n    return nx.community.", "example_id": "31", "test": "G = nx.karate_club_graph()\nnaive_modularity_communities_result = 3\nassert len(list(naive_modularity_communities(G))) > 0 and len(list(naive_modularity_communities(G))) == naive_modularity_communities_result", "solution": "naive_greedy_modularity_communities(G)", "type_of_change": "name change", "name_of_class_or_func": "", "additional_dependencies": "", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["naive_greedy_modularity_communities"], "release_date": "2019-10", "docs": ["https://networkx.org/documentation/networkx-2.5/reference/algorithms/generated/networkx.algorithms.community.modularity_max.naive_greedy_modularity_communities.html#networkx.algorithms.community.modularity_max.naive_greedy_modularity_communities"]}
{"python_version": "3.10", "library": "networkx", "version": "2.4", "problem": "Write a function that returns the naive greedy modularity communities for a graph.", "starting_code": "import networkx as nx\ndef naive_modularity_communities(G:nx.Graph) -> list:\n    return nx.community.", "example_id": "32", "test": "G = nx.karate_club_graph()\nnaive_modularity_communities_result = 3\nassert len(list(naive_modularity_communities(G))) > 0 and len(list(naive_modularity_communities(G))) == naive_modularity_communities_result", "solution": "_naive_greedy_modularity_communities(G)", "type_of_change": "name change", "name_of_class_or_func": "", "additional_dependencies": "", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["_naive_greedy_modularity_communities"], "release_date": "2020-08", "docs": ["https://networkx.org/documentation/networkx-2.4/reference/algorithms/generated/networkx.algorithms.community.modularity_max.naive_greedy_modularity_communities.html#networkx.algorithms.community.modularity_max.naive_greedy_modularity_communities"]}
{"python_version": "3.10", "library": "networkx", "version": "2.5", "problem": "Write a function that returns the nodes as a list of NetworkX graph.", "starting_code": "import networkx as nx\ndef get_nodes(G:nx.Graph) -> list:\n   return ", "example_id": "33", "test": "\nG = nx.karate_club_graph()\nnodes_result = 34\n\nassert get_nodes(G) is not None and len(get_nodes(G)) > 0 and len(get_nodes(G)) == nodes_result", "solution": "list(G.nodes)", "type_of_change": "name change (attribute)", "name_of_class_or_func": "", "additional_dependencies": "", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["list"], "release_date": "2019-10", "docs": ["https://networkx.org/documentation/networkx-2.5/reference/classes/graph.html"]}
{"python_version": "3.10", "library": "networkx", "version": "2.5", "problem": "Write a function that accesses the first edge of a NetworkX graph.", "starting_code": "import networkx as nx\ndef get_first_edge(G:nx.Graph) -> tuple :\n    return ", "example_id": "34", "test": "\nG = nx.karate_club_graph()\nfirst_edge_result = (0, 1)\nassert get_first_edge(G) is not None and first_edge_result == get_first_edge(G)", "solution": "list(G.edges)[0]", "type_of_change": "name change", "name_of_class_or_func": "", "additional_dependencies": "", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["list"], "release_date": "2019-10", "docs": ["https://networkx.org/documentation/networkx-2.5/reference/classes/graph.html"]}
{"python_version": "3.10", "library": "networkx", "version": "2.5", "problem": "Write a function that computes the shortest path lengths and predecessors on shortest paths in weighted graphs using NetworkX.", "starting_code": "import networkx as nx\ndef shortest_path(G:nx.Graph, source:int) -> list:\n    return nx.", "example_id": "35", "test": "\nG = nx.path_graph(5)\nshortest_path_result = nx.bellman_ford_predecessor_and_distance(G, 0)\nassert shortest_path(G, 0) is not None and len(shortest_path(G, 0)) == 2", "solution": "bellman_ford_predecessor_and_distance(G, source)", "type_of_change": "name change", "name_of_class_or_func": "", "additional_dependencies": "", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["bellman_ford_predecessor_and_distance"], "release_date": "2019-10", "docs": ["https://networkx.org/documentation/networkx-2.5/reference/classes/graph.html"]}
{"python_version": "3.10", "library": "gradio", "version": "3.24.0", "problem": "Write a function that renders the quadratic formula in LaTeX using Gradio's Chatbot. The quadratic formula is given by: x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}", "starting_code": "import gradio as gr\ndef render_quadratic_formula():\n     pass\n\n\ninterface = gr.Interface(fn=render_quadratic_formula, inputs=[], outputs = \"text\")\n\ndef render_quadratic_formula():\n    formula =", "example_id": "36", "test": "assertion_value = render_quadratic_formula().startswith(\"$\") and render_quadratic_formula().endswith(\"$\") \nassert assertion_value", "solution": "\"$x = \\\\frac{-b \\\\pm \\\\sqrt{b^2 - 4ac}}{2a}$\"\n    return formula", "type_of_change": "argument change", "name_of_class_or_func": "-", "additional_dependencies": "", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": [], "release_date": "2023-03", "docs": ["https://www.gradio.app/docs/gradio/chatbot", "https://www.gradio.app/guides/the-interface-class", "https://www.gradio.app/changelog"]}
{"python_version": "3.10", "library": "gradio", "version": "3.36.0", "problem": "Write a function that renders the quadratic formula in LaTeX using Gradio's Chatbot. The quadratic formula is given by: x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}", "starting_code": "import gradio as gr\ndef render_quadratic_formula():\n    formula = \"x = \\\\frac{-b \\\\pm \\\\sqrt{b^2 - 4ac}}{2a}\"\n    return formula\n\ninterface = gr.Chatbot", "example_id": "37", "test": "assertion_value = not render_quadratic_formula().startswith(\"$\") and not render_quadratic_formula().endswith(\"$\") and \"$\" in interface.latex_delimiters[0] and  \"$\" in interface.latex_delimiters[1]\nassert assertion_value", "solution": "(fn=render_quadratic_formula, latex_delimiters=(\"$$\", \"$$\"))\n", "type_of_change": "argument change", "name_of_class_or_func": "-", "additional_dependencies": "", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": [], "release_date": "2023-07", "docs": ["https://www.gradio.app/docs/gradio/chatbot", "https://www.gradio.app/changelog"]}
{"python_version": "3.10", "library": "gradio", "version": "3.36.0", "problem": "Write a function that displays an image using Gradio where you cannot share the image.", "starting_code": "import gradio as gr\ndef display_image():\n    return \"https://image_placeholder.com/42\"\n\niface = gr.Interface", "example_id": "38", "test": "assertion_value = iface.output_components[0].show_share_button==False \nassert assertion_value", "solution": "(fn=display_image, inputs=[], outputs=gr.Image(show_share_button=False))\n", "type_of_change": "argument change", "name_of_class_or_func": "-", "additional_dependencies": "", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["gradio.Image"], "release_date": "2023-07", "docs": ["https://www.gradio.app/docs/gradio/image", "https://www.gradio.app/guides/the-interface-class", "https://www.gradio.app/changelog"]}
{"python_version": "3.10", "library": "gradio", "version": "3.0.0", "problem": "Write a function that displays an image using Gradio where you cannot share the image.", "starting_code": "import gradio as gr\ndef display_image():\n    return \"https://image_placeholder.com/42\"\n\niface = gr.Interface", "example_id": "39", "test": "assertion_value = type(gr.Image()) == type(iface.output_components[0])\nassert assertion_value", "solution": "(fn=display_image, inputs=[], outputs=gr.Image())\n", "type_of_change": "argument change", "name_of_class_or_func": "-", "additional_dependencies": "", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["gradio.Image"], "release_date": "2022-05", "docs": ["https://www.gradio.app/docs/gradio/image", "https://www.gradio.app/guides/the-interface-class", "https://www.gradio.app/changelog"]}
{"python_version": "3.10", "library": "gradio", "version": "2.9.2", "problem": "Write a function that takes an image input and returns a textbox output.", "starting_code": "import gradio as gr\ndef process_image(image):\n    return \"Processed\"\n\niface = gr.Interface", "example_id": "40", "test": "assertion_value = type(iface.input_components[0])==type(gr.inputs.Image()) and type(iface.output_components[0])==type(gr.outputs.Textbox()) or type(iface.input_components[0])==type(gr.components.Image()) and type(iface.output_components[0])==type(gr.components.Textbox())\nassert assertion_value", "solution": "(fn=process_image, inputs=gr.inputs.Image(), outputs=gr.outputs.Textbox())", "type_of_change": "argument change", "name_of_class_or_func": "-", "additional_dependencies": "black==22.1.0", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["gradio.outputs.Textbox", "gradio.inputs.Image"], "release_date": "2022-04", "docs": ["https://www.gradio.app/docs/gradio/image", "https://www.gradio.app/guides/the-interface-class", "https://www.gradio.app/changelog"]}
{"python_version": "3.10", "library": "gradio", "version": "3.24.0", "problem": "Write a function that takes an image input and returns a label output.", "starting_code": "import gradio as gr\ndef process_image(image):\n    return \"Processed\"\n\niface = gr.Interface", "example_id": "41", "test": "assertion_value = type(iface.input_components[0])==type(gr.Image()) and type(iface.output_components[0])==type(gr.Label())\nassert assertion_value", "solution": "(fn=process_image, inputs=gr.Image(), outputs=gr.Label())", "type_of_change": "argument change", "name_of_class_or_func": "-", "additional_dependencies": "", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["gradio.Label", "gradio.Image"], "release_date": "2023-03", "docs": ["https://www.gradio.app/docs/gradio/image", "https://www.gradio.app/guides/the-interface-class", "https://www.gradio.app/changelog"]}
{"python_version": "3.10", "library": "gradio", "version": "3.17.0", "problem": "Write a function that returns the selected options from a list of options. Users can select multiple options.", "starting_code": "import gradio as gr\n\ndef get_selected_options(options):\n    return f\"Selected options: {options}\"\n\nselection_options = [\"angola\", \"pakistan\", \"canada\"]\n\niface = gr.Interface(get_selected_options, inputs =\n", "example_id": "42", "test": "assertion_value = (type(iface.input_components[0]) == gr.Dropdown and iface.input_components[0].multiselect == True ) or type(iface.input_components[0]) == gr.CheckboxGroup\nassert assertion_value", "solution": "gr.Dropdown(selection_options, multiselect=True), outputs = 'text')", "type_of_change": "argument change", "name_of_class_or_func": "-", "additional_dependencies": "", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["gradio.Dropdown"], "release_date": "2023-01", "docs": ["https://www.gradio.app/4.44.1/docs/gradio/dropdown", "https://www.gradio.app/guides/the-interface-class", "https://www.gradio.app/changelog"]}
{"python_version": "3.10", "library": "scikit-learn", "version": "1.1", "problem": "Train a Gradient Boosting Classifier from scikit-learn for a binary classification task and get the number of features used in fit.", "starting_code": "from sklearn.ensemble import GradientBoostingClassifier\nimport numpy as np\ndef get_n_features(clf: GradientBoostingClassifier) -> int:\n", "example_id": "43", "test": "X = np.random.rand(100, 20)  # 100 samples, 20 features\ny = np.random.randint(0, 2, 100)\nclf=GradientBoostingClassifier()\nclf.fit(X,y)\nexpected_n_features=20\nassert get_n_features(clf)== expected_n_features", "solution": "    n_features_used = clf.n_features_in_\n    return n_features_used", "type_of_change": "output behaviour", "name_of_class_or_func": "GradientBoostingClassifier", "additional_dependencies": "numpy==1.23.5", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": [], "release_date": "2022-05", "docs": ["https://scikit-learn.org/1.1/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier"]}
{"python_version": "3.10", "library": "scikit-learn", "version": "1.1", "problem": "You are tasked with developing a solution that uses Gradient Boosting Classifier from scikit-learn for a binary classification task with the mean squared error as the criterion.", "starting_code": "from sklearn.ensemble import GradientBoostingClassifier\n# Initialize the classifier\ndef init_clf() -> GradientBoostingClassifier:\n    classifier = GradientBoostingClassifier(criterion=", "example_id": "44", "test": "expected_clf=GradientBoostingClassifier\nassert isinstance(init_clf(), expected_clf)", "solution": "'squared_error')\n    return classifier", "type_of_change": "argument change", "name_of_class_or_func": "GradientBoostingClassifier", "additional_dependencies": "numpy==1.23.5", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": [], "release_date": "2022-05", "docs": ["https://scikit-learn.org/1.1/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier"]}
{"python_version": "3.10", "library": "scikit-learn", "version": "1.2", "problem": "Given dummy data, determine the shape of the coef_ attribute of a CCA model fitted with this data.", "starting_code": "from sklearn.cross_decomposition import CCA\nimport numpy as np\ndef get_coef_shape(cca_model: CCA, X: np.ndarray, Y: np.ndarray) -> tuple:\n    cca_model.fit(X, Y)\n    return ", "example_id": "45", "test": "X = np.random.rand(100, 10)\nY = np.random.rand(100, 5)\ncca_model = CCA()\ncorrect_shape=(X.shape[1], Y.shape[1])\nassert get_coef_shape(cca_model, X, Y) == correct_shape", "solution": "cca_model.coef_.shape", "type_of_change": "attribute change", "name_of_class_or_func": "CCA", "additional_dependencies": "numpy==1.23.5", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": [], "release_date": "2022-12", "docs": ["https://scikit-learn.org/1.2/modules/generated/sklearn.cross_decomposition.CCA.html#sklearn.cross_decomposition.CCA"]}
{"python_version": "3.10", "library": "scikit-learn", "version": "1.3", "problem": "Given dummy data, determine the shape of the coef_ attribute of a CCA model fitted with this data.", "starting_code": "from sklearn.cross_decomposition import CCA\nimport numpy as np\ndef get_coef_shape(cca_model: CCA, X: np.ndarray, Y: np.ndarray) -> tuple:\n    cca_model.fit(X, Y)\n    return ", "example_id": "46", "test": "X = np.random.rand(100, 10)\nY = np.random.rand(100, 5)\ncca_model = CCA()\ncorrect_shape=(Y.shape[1], X.shape[1])\nassert get_coef_shape(cca_model, X, Y) == correct_shape", "solution": "cca_model.coef_.shape", "type_of_change": "attribute change", "name_of_class_or_func": "CCA", "additional_dependencies": "numpy==1.23.5", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": [], "release_date": "2023-06", "docs": ["https://scikit-learn.org/1.3/modules/generated/sklearn.cross_decomposition.CCA.html#sklearn.cross_decomposition.CCA"]}
{"python_version": "3.10", "library": "scikit-learn", "version": "1.1", "problem": "Generate a sparse coded signal where the data is transposed.", "starting_code": "from sklearn.datasets import make_sparse_coded_signal\ndef get_signal(n_samples: int, n_features: int, n_components: int, n_nonzero_coefs: int) -> tuple:\n    return ", "example_id": "47", "test": "n_samples=100\nn_features=50\nn_components=20\nn_nonzero_coefs=10\nexpected_shape_y = (n_features, n_samples)\nexpected_shape_d = (n_features, n_components)\nexpected_shape_c = (n_components, n_samples)\n\ny,d,c = get_signal(n_samples, n_features, n_components, n_nonzero_coefs)\nassert y.shape == expected_shape_y\nassert d.shape == expected_shape_d\nassert c.shape == expected_shape_c", "solution": "make_sparse_coded_signal(n_samples=n_samples, n_features=n_features,n_components=n_components,n_nonzero_coefs=n_nonzero_coefs)", "type_of_change": "output behaviour", "name_of_class_or_func": "make_sparse_coded_signal", "additional_dependencies": "numpy==1.23.5", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["sklearn.datasets.make_sparse_coded_signal"], "release_date": "2022-05", "docs": ["https://scikit-learn.org/1.1/modules/generated/sklearn.datasets.make_sparse_coded_signal.html#sklearn.datasets.make_sparse_coded_signal"]}
{"python_version": "3.10", "library": "scikit-learn", "version": "1.1", "problem": "Apply Fast Independent Component Analysis (FastICA) with a specific whiten parameter setting. Store transformed data in a variable transformed_data.", "starting_code": "from sklearn.datasets import load_digits\nfrom sklearn.utils import Bunch\nfrom sklearn.decomposition import FastICA\ndef apply_fast_ica(data: Bunch, n_components: int) -> FastICA:\n    return ", "example_id": "48", "test": "data, _ = load_digits(return_X_y=True)\nn_components=7\nexpected_shape = (1797, n_components)\nassert apply_fast_ica(data, n_components).shape == expected_shape", "solution": "FastICA(n_components=n_components,random_state=0,whiten=True).fit_transform(data)", "type_of_change": "argument change", "name_of_class_or_func": "FastICA", "additional_dependencies": "numpy==1.23.5", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["fit_transform", "sklearn.decomposition.FastICA"], "release_date": "2022-05", "docs": ["https://scikit-learn.org/1.1/modules/generated/sklearn.decomposition.FastICA.html#sklearn.decomposition.FastICA"]}
{"python_version": "3.10", "library": "scikit-learn", "version": "1.3", "problem": "Apply Fast Independent Component Analysis (FastICA) with a specific whiten parameter setting. Store transformed data in a variable transformed_data.", "starting_code": "from sklearn.datasets import load_digits\nfrom sklearn.decomposition import FastICA\nfrom sklearn.utils import Bunch\ndef apply_fast_ica(data: Bunch, n_components: int) -> FastICA:\n    return ", "example_id": "49", "test": "data, _ = load_digits(return_X_y=True)\nn_components=7\nexpected_shape = (1797, n_components)\nassert apply_fast_ica(data, n_components).shape == expected_shape", "solution": "FastICA(n_components=n_components,random_state=0,whiten='arbitrary-variance').fit_transform(data)", "type_of_change": "argument change", "name_of_class_or_func": "FastICA", "additional_dependencies": "numpy==1.23.5", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["fit_transform", "sklearn.decomposition.FastICA"], "release_date": "2023-06", "docs": ["https://scikit-learn.org/1.3/modules/generated/sklearn.decomposition.FastICA.html#sklearn.decomposition.FastICA"]}
{"python_version": "3.10", "library": "scikit-learn", "version": "1.1", "problem": "Impute missing values in a dataset using SimpleImputer and return the SimpleImputer instance, including the `verbose` parameter if available.", "starting_code": "from sklearn.impute import SimpleImputer\nimport numpy as np\ndef get_imputer(data: np.ndarray) -> SimpleImputer:\n    return ", "example_id": "50", "test": "data = np.array([[1, 2, 3], [4, None, 6], [7, 8, None]], dtype=float)\nexpected_type=SimpleImputer\nassert isinstance(get_imputer(data), expected_type)", "solution": "SimpleImputer()", "type_of_change": "argument change", "name_of_class_or_func": "SimpleImputer", "additional_dependencies": "numpy==1.23.5", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["sklearn.impute.SimpleImputer"], "release_date": "2022-05", "docs": ["https://scikit-learn.org/1.1/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer"]}
{"python_version": "3.10", "library": "scikit-learn", "version": "1.3", "problem": "Retrieve and list all available scorer names, ensuring they are returned in a list format.", "starting_code": "from sklearn import metrics\ndef get_scorer_names() -> list:\n    return ", "example_id": "51", "test": "conditions = isinstance(get_scorer_names(), list) and len(get_scorer_names()) > 0\nassert conditions", "solution": "metrics.get_scorer_names()", "type_of_change": "name change", "name_of_class_or_func": "get_scorer_names", "additional_dependencies": "numpy==1.23.5", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["sklearn.metrics.get_scorer_names"], "release_date": "2023-06", "docs": ["https://scikit-learn.org/1.3/modules/classes.html#sklearn-metrics-metrics"]}
{"python_version": "3.10", "library": "scikit-learn", "version": "1.2", "problem": "Retrieve and list all available scorer names, ensuring they are returned in a list format.", "starting_code": "from sklearn import metrics\ndef get_scorer_names() -> list:\n    return ", "example_id": "52", "test": "conditions = isinstance(get_scorer_names(), list) and len(get_scorer_names()) > 0\nassert conditions", "solution": "list(metrics.SCORERS.keys())", "type_of_change": "name change", "name_of_class_or_func": "get_scorer_names", "additional_dependencies": "numpy==1.23.5", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["list", "sklearn.metrics.SCORERS.keys"], "release_date": "2022-12", "docs": ["https://scikit-learn.org/1.2/modules/classes.html#sklearn-metrics-metrics"]}
{"python_version": "3.10", "library": "scikit-learn", "version": "1.1", "problem": "Adapt the use of `manhattan_distances` to obtain a pairwise distance matrix.", "starting_code": "from sklearn.metrics.pairwise import manhattan_distances\nimport numpy as np\ndef get_pairwise_dist(X: np.ndarray,Y: np.ndarray) -> np.ndarray:\n    distances = manhattan_distances(X, Y, sum_over_features=False)\n    return  ", "example_id": "53", "test": "X = np.array([[1, 2], [3, 4], [5, 6]])\nY = np.array([[1, 1], [4, 4]])\nexpected_result = np.array([1, 5, 5, 1, 9, 3])\nassert np.allclose(get_pairwise_dist(X, Y), expected_result, atol=1e-3)", "solution": "np.sum(distances, axis=1)", "type_of_change": "argument change", "name_of_class_or_func": "manhattan_distances", "additional_dependencies": "numpy==1.23.5", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["numpy.sum"], "release_date": "2022-05", "docs": ["https://scikit-learn.org/1.1/modules/generated/sklearn.metrics.pairwise.manhattan_distances.html#sklearn.metrics.pairwise.manhattan_distances"]}
{"python_version": "3.10", "library": "scikit-learn", "version": "1.2", "problem": "Adapt the use of `manhattan_distances` in scikit-learn version 1.2 to obtain a pairwise distance matrix.", "starting_code": "from sklearn.metrics.pairwise import manhattan_distances\nimport numpy as np\ndef get_pairwise_dist(X: np.ndarray,Y: np.ndarray) -> np.ndarray:\n    return ", "example_id": "54", "test": "X = np.array([[1, 2], [3, 4], [5, 6]])\nY = np.array([[1, 1], [4, 4]])\nexpected_result = np.array([[1, 5], [5, 1], [9, 3]])\nassert np.allclose(get_pairwise_dist(X, Y), expected_result, atol=1e-3)", "solution": "manhattan_distances(X, Y)", "type_of_change": "argument change", "name_of_class_or_func": "manhattan_distances", "additional_dependencies": "numpy==1.23.5", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["sklearn.metrics.pairwise.manhattan_distances"], "release_date": "2022-12", "docs": ["https://scikit-learn.org/1.2/modules/generated/sklearn.metrics.pairwise.manhattan_distances.html#sklearn.metrics.pairwise.manhattan_distances"]}
{"python_version": "3.10", "library": "matplotlib", "version": "3.4.0", "problem": "Reverse the following color mapping.", "starting_code": "from matplotlib.colors import *\nimport numpy as np\ncmap = {\n    \"blue\": [[1, 2, 2], [2, 2, 1]],\n    \"red\": [[0, 0, 0], [1, 0, 0]],\n    \"green\": [[0, 0, 0], [1, 0, 0]]\n}\n\ncmap_reversed = ", "example_id": "55", "test": "\nexpected_cmap_reversed = {'blue': [(-1.0, 1, 2), (0.0, 2, 2)], 'red': [(0.0, 0, 0), (1.0, 0, 0)], 'green': [(0.0, 0, 0), (1.0, 0, 0)]}\n\nreversed_cmap_dict = cmap_reversed._segmentdata\n\nassert reversed_cmap_dict == expected_cmap_reversed", "solution": "LinearSegmentedColormap(\"custom_cmap\", cmap).reversed()\n", "type_of_change": "name change", "name_of_class_or_func": "revcmap", "additional_dependencies": "", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["reversed", "LinearSegmentedColormap"], "release_date": "2021-05", "docs": ["https://matplotlib.org/3.4.0/api/colors_api.html#module-matplotlib.colors"]}
{"python_version": "3.10", "library": "pandas", "version": "1.5.0", "problem": "Use the pandas groupby operation, where the intention is to include unobserved categories without dropping NA values, and sum over it.", "starting_code": "import pandas as pd\ndef get_grouped_df(df: pd.DataFrame) -> pd.DataFrame:\n    return ", "example_id": "56", "test": "df = pd.DataFrame({'x': pd.Categorical([1, None], categories=[1, 2, 3]), 'y': [3, 4]})\nexpected_output=pd.DataFrame({'y': [3, 4]}, index=pd.Index([1, None], name='x'))\nassert get_grouped_df(df).equals(expected_output)", "solution": "df.groupby('x', observed=False, dropna=False).sum()", "type_of_change": "output behaviour", "name_of_class_or_func": "groupby", "additional_dependencies": "numpy==1.21.6", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["sum", "df.groupby"], "release_date": "2015-10", "docs": ["https://pandas.pydata.org/pandas-docs/version/1.5/reference/api/pandas.DataFrame.html", "https://pandas.pydata.org/pandas-docs/version/1.5/reference/api/pandas.Series.html#pandas.Series"]}
{"python_version": "3.10", "library": "pandas", "version": "1.5.1", "problem": "Use the pandas groupby operation with observed=False and dropna=False, where the intention is to include unobserved categories without dropping NA values. Your job is to predict the expected output after this operation.", "starting_code": "import pandas as pd\ndef get_grouped_df(df: pd.DataFrame) -> pd.DataFrame:\n    return ", "example_id": "57", "test": "df = pd.DataFrame({'x': pd.Categorical([1, None], categories=[1, 2, 3]), 'y': [3, 4]})\nexpected_output = pd.DataFrame({'y': [3, 0, 0]}, index=pd.Index([1, 2, 3], name='x'))\nassert get_grouped_df(df).equals(expected_output)", "solution": "df.groupby('x', observed=False, dropna=False).sum()", "type_of_change": "output behaviour", "name_of_class_or_func": "groupby", "additional_dependencies": "numpy==1.21.6", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["sum", "df.groupby"], "release_date": "2015-10", "docs": ["https://pandas.pydata.org/pandas-docs/version/1.5.1/reference/api/pandas.DataFrame.html", "https://pandas.pydata.org/pandas-docs/version/1.5.1/reference/api/pandas.Series.html#pandas.Series"]}
{"python_version": "3.10", "library": "pandas", "version": "1.5.0", "problem": "Predict behaviour of setting values with iloc inplace.", "starting_code": "import pandas as pd\nimport numpy as np\ndef get_expected_value(df: pd.DataFrame) -> pd.Series:\n    return ", "example_id": "58", "test": "df = pd.DataFrame({'price': [11.1, 12.2]}, index=['book1', 'book2'])\noriginal_prices = df['price']\nnew_prices = np.array([98, 99])\ndf.iloc[:, 0] = new_prices\ncorrect_prices=pd.Series([11.1, 12.2], index=['book1', 'book2'], dtype=np.float64)\nassert get_expected_value(df).equals(correct_prices)", "solution": "pd.Series([11.1, 12.2], index=['book1', 'book2'], dtype=np.float64)", "type_of_change": "gh\noutput behaviour", "name_of_class_or_func": "iloc", "additional_dependencies": "numpy==1.21.6", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["pandas.Series"], "release_date": "2015-10", "docs": ["https://pandas.pydata.org/pandas-docs/version/1.5/reference/api/pandas.DataFrame.html", "https://pandas.pydata.org/pandas-docs/version/1.5/reference/api/pandas.Series.html#pandas.Series"]}
{"python_version": "3.10", "library": "pandas", "version": "2", "problem": "Predict behaviour of setting values with iloc inplace.", "starting_code": "import pandas as pd\nimport numpy as np\ndef get_expected_value(df: pd.DataFrame) -> pd.Series:\n    return ", "example_id": "59", "test": "df = pd.DataFrame({'price': [11.1, 12.2]}, index=['book1', 'book2'])\noriginal_prices = df['price']\nnew_prices = np.array([98, 99])\ndf.iloc[:, 0] = new_prices\ncorrect_prices=pd.Series([98.0, 99.0], index=['book1', 'book2'], dtype=np.float64)\nassert get_expected_value(df).equals(correct_prices)", "solution": "pd.Series([98.0, 99.0], index=['book1', 'book2'], dtype=np.float64)", "type_of_change": "output behaviour", "name_of_class_or_func": "iloc", "additional_dependencies": "numpy==1.21.6", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["pandas.Series"], "release_date": "2017-01", "docs": ["https://pandas.pydata.org/pandas-docs/version/2.0/reference/api/pandas.DataFrame.html", "https://pandas.pydata.org/pandas-docs/version/2.0/reference/api/pandas.Series.html#pandas.Series"]}
{"python_version": "3.10", "library": "pandas", "version": "1.5.0", "problem": "Predict behaviour of integer slicing on a Series.", "starting_code": "import pandas as pd\nimport numpy as np\ndef get_slice(ser: pd.Series, start: int, end: int) -> pd.Series:\n    return ", "example_id": "60", "test": "ser = pd.Series([1, 2, 3, 4, 5], index=[2, 3, 5, 7, 11])\nstart,end=2,4\nsliced_ser = ser[2:4]\nassert sliced_ser.equals(get_slice(ser, start, end)), 'Slicing does not match expected output'", "solution": "ser[start:end]", "type_of_change": "output behaviour", "name_of_class_or_func": "Series slicing", "additional_dependencies": "numpy==1.21.6", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": [], "release_date": "2015-10", "docs": ["https://pandas.pydata.org/pandas-docs/version/1.5/reference/api/pandas.DataFrame.html", "https://pandas.pydata.org/pandas-docs/version/1.5/reference/api/pandas.Series.html#pandas.Series"]}
{"python_version": "3.10", "library": "pandas", "version": "2", "problem": "Predict behaviour of integer slicing on a Series.", "starting_code": "import pandas as pd\nimport numpy as np\ndef get_slice(ser: pd.Series, start: int, end: int) -> pd.Series:\n    return ", "example_id": "61", "test": "ser = pd.Series([1, 2, 3, 4, 5], index=[2, 3, 5, 7, 11])\nstart,end=2,4\nsliced_ser = ser.iloc[2:4]\nassert sliced_ser.equals(get_slice(ser, start, end)), 'Slicing does not match expected label-based output'", "solution": "ser.iloc[start:end]", "type_of_change": "output behaviour", "name_of_class_or_func": "Series slicing", "additional_dependencies": "numpy==1.21.6", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": [], "release_date": "2017-01", "docs": ["https://pandas.pydata.org/pandas-docs/version/2.0/reference/api/pandas.DataFrame.html", "https://pandas.pydata.org/pandas-docs/version/2.0/reference/api/pandas.Series.html#pandas.Series"]}
{"python_version": "3.10", "library": "pandas", "version": "1.4.0", "problem": "Write a function to return the correct type of an int index.", "starting_code": "import pandas as pd\ndef correct_type(index: pd.Index) -> str:\n    return", "example_id": "62", "test": "index = pd.Index([1, 2, 3], dtype='int32')\nassertion_1_value = isinstance(correct_type(index), str)\nassertion_2_value =  correct_type(index) == 'int64'\nassert assertion_1_value\nassert assertion_2_value", "solution": " 'int64'", "type_of_change": "output behaviour", "name_of_class_or_func": "Index", "additional_dependencies": "numpy==1.21.6", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": [], "release_date": "2014-08", "docs": ["https://pandas.pydata.org/pandas-docs/version/1.4/reference/api/pandas.DataFrame.html", "https://pandas.pydata.org/pandas-docs/version/1.4/reference/api/pandas.Series.html#pandas.Series"]}
{"python_version": "3.10", "library": "pandas", "version": "1.4.0", "problem": "Combine series and dataframes and return a tuple with combined dataframes first then combined series.", "starting_code": "import pandas as pd\ndef combined(df1: pd.DataFrame, df2: pd.DataFrame, series1: pd.Series, series2: pd.Series) -> tuple:\n    return ", "example_id": "63", "test": "series1 = pd.Series([1, 2])\nseries2 = pd.Series([3, 4])\ndf1 = pd.DataFrame([[1, 2], [3, 4]], columns=list('AB'))\ndf2 = pd.DataFrame([[5, 6], [7, 8]], columns=list('AB'))\nexpected_series_values = [1, 2, 3, 4]\nexpected_dataframe_values = [[1, 2], [3, 4], [5, 6], [7, 8]]\ncombined_dataframe, combined_series = combined(df1, df2, series1, series2)\nassert list(combined_series) == expected_series_values, 'Combined series values are incorrect'\nassert combined_dataframe.values.tolist() == expected_dataframe_values, 'Combined dataframe values are incorrect'", "solution": "df1.append(df2, ignore_index=True), series1.append(series2, ignore_index=True)", "type_of_change": "name change", "name_of_class_or_func": "append", "additional_dependencies": "numpy==1.21.6", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["df1.append", "series1.append"], "release_date": "2014-08", "docs": ["https://pandas.pydata.org/pandas-docs/version/1.4/reference/api/pandas.DataFrame.html", "https://pandas.pydata.org/pandas-docs/version/1.4/reference/api/pandas.Series.html#pandas.Series"]}
{"python_version": "3.10", "library": "pandas", "version": "2", "problem": "Write a function to return the correct type of an int index.", "starting_code": "import pandas as pd\ndef correct_type(index: pd.Index) -> str:\n    return ", "example_id": "64", "test": "index = pd.Index([1, 2, 3], dtype='int32')\nassert isinstance(correct_type(index), str)\nassert correct_type(index) == str(index.dtype)", "solution": "str(index.dtype)", "type_of_change": "output behaviour", "name_of_class_or_func": "Index", "additional_dependencies": "numpy==1.21.6", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["str"], "release_date": "2017-01", "docs": ["https://pandas.pydata.org/pandas-docs/version/2.0/reference/api/pandas.DataFrame.html", "https://pandas.pydata.org/pandas-docs/version/2.0/reference/api/pandas.Series.html#pandas.Series"]}
{"python_version": "3.10", "library": "pandas", "version": "2", "problem": "Combine series and dataframes. return a tuple of the combined dataframes then the series.", "starting_code": "import pandas as pd\ndef combined(df1: pd.DataFrame, df2: pd.DataFrame, series1: pd.Series, series2: pd.Series) -> tuple:\n    return ", "example_id": "65", "test": "series1 = pd.Series([1, 2])\nseries2 = pd.Series([3, 4])\ndf1 = pd.DataFrame([[1, 2], [3, 4]], columns=list('AB'))\ndf2 = pd.DataFrame([[5, 6], [7, 8]], columns=list('AB'))\nexpected_series_values = [1, 2, 3, 4]\nexpected_dataframe_values = [[1, 2], [3, 4], [5, 6], [7, 8]]\ncombined_dataframe, combined_series = combined(df1, df2, series1, series2)\nassert list(combined_series) == expected_series_values, 'Combined series values are incorrect'\nassert combined_dataframe.values.tolist() == expected_dataframe_values, 'Combined dataframe values are incorrect'", "solution": "pd.concat([df1, df2], ignore_index=True), pd.concat([series1,series2],ignore_index=True)", "type_of_change": "name change", "name_of_class_or_func": "append", "additional_dependencies": "numpy==1.21.6", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["pandas.concat"], "release_date": "2017-01", "docs": ["https://pandas.pydata.org/pandas-docs/version/2.0/reference/api/pandas.DataFrame.html", "https://pandas.pydata.org/pandas-docs/version/2.0/reference/api/pandas.Series.html#pandas.Series"]}
{"python_version": "3.10", "library": "numpy", "version": "1.21.0", "problem": "Implement a function that calculates the convolution of two arrays with the mode set to full.", "starting_code": "import numpy as np\n\ndef apply_convolution_full(arr1 : np.ndarray, arr2 : np.ndarray) -> np.ndarray:\n    return ", "example_id": "66", "test": "\narr1 = np.array([1, 2, 3])\narr2 = np.array([0, 1, 0.5])\nassertion_result = apply_convolution_full(arr1, arr2).all() == False\nassert assertion_result", "solution": "np.convolve(arr1, arr2, mode=\"full\")", "type_of_change": "argument change", "name_of_class_or_func": "numpy.convolve", "additional_dependencies": "", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["numpy.convolve"], "release_date": "2021-06", "docs": ["https://numpy.org/doc/1.21/reference/generated/numpy.convolve.html"]}
{"python_version": "3.10", "library": "numpy", "version": "1.21.0", "problem": "Implement a function that calculates the convolution of two arrays with the mode set to valid.", "starting_code": "import numpy as np\n\ndef apply_convolution_valid(arr1 : np.ndarray , arr2 : np.ndarray) -> np.ndarray:\n    return ", "example_id": "67", "test": "\narr1 = np.array([1, 2, 3])\narr2 = np.array([0, 1, 0.5])\nassertion_result = apply_convolution_valid(arr1, arr2).all() == True\nassert assertion_result", "solution": "np.convolve(arr1, arr2, mode=\"valid\")", "type_of_change": "argument change", "name_of_class_or_func": "numpy.convolve", "additional_dependencies": "", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["numpy.convolve"], "release_date": "2021-06", "docs": ["https://numpy.org/doc/1.21/reference/generated/numpy.convolve.html"]}
{"python_version": "3.10", "library": "numpy", "version": "1.21.0", "problem": "Implement a function that calculates the Cross-correlation of two 1-dimensional sequences with the mode set to full.", "starting_code": "import numpy as np\n\ndef apply_correlate_full(arr1 : np.ndarray, arr2 : np.ndarray) -> np.ndarray:\n    return ", "example_id": "68", "test": "\narr1 = np.array([1, 2, 3])\narr2 = np.array([0, 1, 0.5])\nassertion_value = apply_correlate_full(arr1, arr2).all() == False\nassert assertion_value", "solution": "np.correlate(arr1, arr2, mode=\"full\")", "type_of_change": "argument change", "name_of_class_or_func": "numpy.correlate", "additional_dependencies": "", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["numpy.correlate"], "release_date": "2021-06", "docs": ["https://numpy.org/doc/1.21/reference/generated/numpy.correlate.html"]}
{"python_version": "3.10", "library": "numpy", "version": "1.25.0", "problem": "Given two arrays, find their common types.", "starting_code": "import numpy as np\n\ndef find_common_type(arr1:np.ndarray, arr2:np.ndarray) -> np.dtype:\n    return np.", "example_id": "69", "test": "\narray1 = np.array([1, 2, 3])\narray2 = np.array([4.0, 5.0, 6.0])\nexpected_common_type = np.float64\nassertion_value = find_common_type(array1, array2) == expected_common_type\nassert assertion_value", "solution": "common_type(arr1, arr2)", "type_of_change": "deprecation", "name_of_class_or_func": "find_common_type", "additional_dependencies": "", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["common_type"], "release_date": "2023-06", "docs": ["https://numpy.org/doc/1.25/reference/routines.dtype.html"]}
{"python_version": "3.10", "library": "numpy", "version": "1.21.0", "problem": "Given two arrays, find their common types.", "starting_code": "import numpy as np\n\ndef find_common_type(arr1:np.ndarray, arr2:np.ndarray) -> np.dtype:\n    return np.", "example_id": "70", "test": "\narray1 = np.array([1, 2, 3])\narray2 = np.array([4.0, 5.0, 6.0])\nexpected_common_type = np.float64\n\nassertion_value = find_common_type(array1, array2) == expected_common_type\nassert assertion_value", "solution": "find_common_type(arr1, arr2)", "type_of_change": "deprecation", "name_of_class_or_func": "find_common_type", "additional_dependencies": "", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["find_common_type"], "release_date": "2021-06", "docs": ["https://numpy.org/doc/1.21/reference/routines.dtype.html"]}
{"python_version": "3.10", "library": "numpy", "version": "1.25.0", "problem": "Write a function that rounds an array of numbers.", "starting_code": "import numpy as np\n\ndef custom_round(arr:np.ndarray) -> np.ndarray:\n    return ", "example_id": "71", "test": "\ndef test_custom_round():\n    arr = np.array([1.5, 2.3, 3.7])\n    result = custom_round(arr)\n    expected = np.array([2.0, 2.0, 4.0])\n    assert np.array_equal(result, expected)\n\ntest_custom_round()", "solution": "np.round(arr)", "type_of_change": "deprecation", "name_of_class_or_func": "round_", "additional_dependencies": "", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["numpy.round"], "release_date": "2023-06", "docs": ["https://numpy.org/doc/1.25/reference/routines.math.html"]}
{"python_version": "3.10", "library": "numpy", "version": "1.25.0", "problem": "Write a function that computes the product of an array.", "starting_code": "import numpy as np\n\ndef custom_product(arr:np.ndarray) -> np.ndarray:\n    return ", "example_id": "72", "test": "\ndef test_custom_product():\n    arr = np.array([1, 2, 3, 4])\n    result = custom_product(arr)\n    expected = 24\n    assert result == expected\n\ntest_custom_product()", "solution": "np.prod(arr)", "type_of_change": "deprecation", "name_of_class_or_func": "product", "additional_dependencies": "", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["numpy.prod"], "release_date": "2023-06", "docs": ["https://numpy.org/doc/1.25/reference/routines.math.html"]}
{"python_version": "3.10", "library": "numpy", "version": "1.25.0", "problem": "Write a function that computes the cumulative product of an array.", "starting_code": "import numpy as np\n\ndef custom_cumproduct(arr:np.ndarray) -> np.ndarray:\n    return ", "example_id": "73", "test": "\ndef test_custom_cumproduct():\n    arr = np.array([1, 2, 3, 4])\n    result = custom_cumproduct(arr)\n    expected = np.array([1, 2, 6, 24])\n    assert np.array_equal(result, expected)\n\ntest_custom_cumproduct()", "solution": "np.cumprod(arr)", "type_of_change": "deprecation", "name_of_class_or_func": "cumproduct", "additional_dependencies": "", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["numpy.cumprod"], "release_date": "2023-06", "docs": ["https://numpy.org/doc/1.25/reference/routines.math.html"]}
{"python_version": "3.10", "library": "numpy", "version": "1.25.0", "problem": "Write a function that checks if any elements in an array are true.", "starting_code": "import numpy as np\n\ndef custom_sometrue(arr:np.ndarray) -> np.ndarray:\n    return ", "example_id": "74", "test": "\n\ndef test_custom_sometrue():\n    arr = np.array([0, 0, 1, 0])\n    result = custom_sometrue(arr)\n    expected = True\n    assert result == expected\n\ntest_custom_sometrue()", "solution": "np.any(arr)", "type_of_change": "deprecation", "name_of_class_or_func": "sometrue", "additional_dependencies": "", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["numpy.any"], "release_date": "2023-06", "docs": ["https://numpy.org/doc/1.25/reference/routines.logic.html"]}
{"python_version": "3.10", "library": "numpy", "version": "1.25.0", "problem": "Write a function that checks if all elements in an array are true.", "starting_code": "import numpy as np\n\ndef custom_alltrue(arr:np.ndarray) -> np.ndarray:\n    return ", "example_id": "75", "test": "\n\ndef test_custom_alltrue():\n    arr = np.array([1, 1, 1, 1])\n    result = custom_alltrue(arr)\n    expected = True\n    assert result == expected\n\ntest_custom_alltrue()", "solution": "np.all(arr)", "type_of_change": "deprecation", "name_of_class_or_func": "alltrue", "additional_dependencies": "", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["numpy.all"], "release_date": "2023-06", "docs": ["https://numpy.org/doc/1.25/reference/routines.logic.html"]}
{"python_version": "3.10", "library": "numpy", "version": "1.21.0", "problem": "Write a function that rounds an array of numbers.", "starting_code": "import numpy as np\n\ndef custom_round(arr:np.ndarray) -> np.ndarray:\n    return ", "example_id": "76", "test": "\ndef test_custom_round():\n    arr = np.array([1.5, 2.3, 3.7])\n    result = custom_round(arr)\n    expected = np.array([2.0, 2.0, 4.0])\n    assert np.array_equal(result, expected)\n\ntest_custom_round()", "solution": "np.round_(arr)", "type_of_change": "deprecation", "name_of_class_or_func": "round_", "additional_dependencies": "", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["numpy.round_"], "release_date": "2021-06", "docs": ["https://numpy.org/doc/1.21/reference/routines.math.html"]}
{"python_version": "3.10", "library": "numpy", "version": "1.21.0", "problem": "Write a function that computes the product of an array.", "starting_code": "import numpy as np\n\ndef custom_product(arr:np.ndarray) -> np.ndarray:\n    return ", "example_id": "77", "test": "\n\ndef test_custom_product():\n    arr = np.array([1, 2, 3, 4])\n    result = custom_product(arr)\n    expected = 24\n    assert result == expected\n\ntest_custom_product()", "solution": "np.product(arr)", "type_of_change": "deprecation", "name_of_class_or_func": "product", "additional_dependencies": "", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["numpy.product"], "release_date": "2021-06", "docs": ["https://numpy.org/doc/1.21/reference/routines.math.html"]}
{"python_version": "3.10", "library": "numpy", "version": "1.21.0", "problem": "Write a function that computes the cumulative product of an array.", "starting_code": "import numpy as np\n\ndef custom_cumproduct(arr:np.ndarray) -> np.ndarray:\n    return ", "example_id": "78", "test": "\ndef test_custom_cumproduct():\n    arr = np.array([1, 2, 3, 4])\n    result = custom_cumproduct(arr)\n    expected = np.array([1, 2, 6, 24])\n    assert np.array_equal(result, expected)\n\ntest_custom_cumproduct()", "solution": "np.cumproduct(arr)", "type_of_change": "deprecation", "name_of_class_or_func": "cumproduct", "additional_dependencies": "", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["numpy.cumproduct"], "release_date": "2021-06", "docs": ["https://numpy.org/doc/1.21/reference/routines.math.html"]}
{"python_version": "3.10", "library": "numpy", "version": "1.21.0", "problem": "Write a function that checks if any elements in an array are true.", "starting_code": "import numpy as np\n\ndef custom_anytrue(arr:np.ndarray) -> np.ndarray:\n    return ", "example_id": "79", "test": "\ndef test_custom_sometrue():\n    arr = np.array([0, 0, 1, 0])\n    result = custom_anytrue(arr)\n    expected = True\n    assert result == expected\n\ntest_custom_sometrue()", "solution": "np.sometrue(arr)", "type_of_change": "deprecation", "name_of_class_or_func": "sometrue", "additional_dependencies": "", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["numpy.sometrue"], "release_date": "2021-06", "docs": ["https://numpy.org/doc/1.21/reference/routines.logic.html"]}
{"python_version": "3.10", "library": "numpy", "version": "1.21.0", "problem": "Write a function that checks if all elements in an array are true.", "starting_code": "import numpy as np\n\ndef custom_alltrue(arr:np.ndarray) -> np.ndarray:\n    return ", "example_id": "80", "test": "\n\ndef test_custom_alltrue():\n    arr = np.array([1, 1, 1, 1])\n    result = custom_alltrue(arr)\n    expected = True\n    assert result == expected\n\ntest_custom_alltrue()", "solution": "np.alltrue(arr)", "type_of_change": "deprecation", "name_of_class_or_func": "alltrue", "additional_dependencies": "", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["numpy.alltrue"], "release_date": "2021-06", "docs": ["https://numpy.org/doc/1.21/reference/routines.logic.html"]}
{"python_version": "3.10", "library": "lightgbm", "version": "3.0.0", "problem": "Perform cross-validation with the given parameters and return the evaluation history for each fold.", "starting_code": "import numpy as np\nimport lightgbm as lgb\nfrom sklearn.datasets import make_classification\n\nNUM_SAMPLES = 500\nNUM_FEATURES = 20\nINFORMATIVE_FEATURES = 2\nREDUNDANT_FEATURES = 10\nRANDOM_STATE = 42\nNUM_BOOST_ROUND = 100\nNFOLD = 5\nLEARNING_RATE = 0.05\nEARLY_STOPPING_ROUNDS = 10\nX, y = make_classification(n_samples=NUM_SAMPLES, n_features=NUM_FEATURES, n_informative=INFORMATIVE_FEATURES, n_redundant=REDUNDANT_FEATURES, random_state=RANDOM_STATE)\ntrain_data = lgb.Dataset(X, label=y)\n\nparams = {\n    'objective': 'binary',\n    'metric': 'binary_logloss',\n    'learning_rate': LEARNING_RATE,\n    'verbose': -1\n}\n\ncv_results = lgb.cv(\n    params=params,\n    train_set=train_data,\n    num_boost_round=NUM_BOOST_ROUND,\n    nfold=NFOLD,\n    early_stopping_rounds=EARLY_STOPPING_ROUNDS,", "example_id": "82", "test": "import numpy as np\nassertion_1_value = 'cvbooster' in cv_results\nassertion_2_value = len(cv_results['cvbooster'].boosters) == NFOLD\nassertion_3_value = all(isinstance(booster, lgb.Booster) for booster in cv_results['cvbooster'].boosters)\nassert assertion_1_value\nassert assertion_2_value\nassert assertion_3_value", "solution": "return_cvbooster=True\n)", "type_of_change": "Argument or Attribute change", "name_of_class_or_func": "cv", "additional_dependencies": "numpy==1.26.4 scikit-learn==1.3.2", "functional": 0, "webdev": 0, "solution_api_call": true, "api_calls": [], "release_date": "2020-08", "docs": ["https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.cv.html", "https://github.com/microsoft/lightgbm/releases?page=2"]}
{"python_version": "3.10", "library": "lightgbm", "version": "3.0.0", "problem": "Write a function to decode a byte string.", "starting_code": "import lightgbm.compat as compat\ndef decode_string(string: bytes) -> str:\n    return ", "example_id": "83", "test": "ENCODED_STRING = b'\\x68\\x65\\x6c\\x6c\\x6f'\nexpected = 'hello'\nassert decode_string(ENCODED_STRING) == expected\n", "solution": "compat.decode_string(string)", "type_of_change": "Semantics or Function Behaviour change", "name_of_class_or_func": "decode_string", "additional_dependencies": "", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["lightgbm.compat.decode_string"], "release_date": "2020-08", "docs": ["https://github.com/microsoft/LightGBM/blob/master/python-package/lightgbm/compat.py", "https://github.com/microsoft/lightgbm/releases?page=2"]}
{"python_version": "3.10", "library": "lightgbm", "version": "3.0.0", "problem": "Perform cross-validation with the given parameters and display the training metric in progress.", "starting_code": "import numpy as np\nimport lightgbm as lgb\nfrom sklearn.datasets import make_classification\n\nNUM_SAMPLES = 500\nNUM_FEATURES = 20\nINFORMATIVE_FEATURES = 2\nREDUNDANT_FEATURES = 10\nRANDOM_STATE = 42\nNUM_BOOST_ROUND = 100\nNFOLD = 5\nLEARNING_RATE = 0.05\nEARLY_STOPPING_ROUNDS = 10\nX, y = make_classification(n_samples=NUM_SAMPLES, n_features=NUM_FEATURES, n_informative=INFORMATIVE_FEATURES, n_redundant=REDUNDANT_FEATURES, random_state=RANDOM_STATE)\ntrain_data = lgb.Dataset(X, label=y)\n\nparams = {\n    'objective': 'binary',\n    'metric': 'binary_logloss',\n    'learning_rate': LEARNING_RATE,\n    'verbose': -1\n}\n\ncv_results = lgb.cv(\n    params=params,\n    train_set=train_data,\n    num_boost_round=NUM_BOOST_ROUND,\n    nfold=NFOLD,\n    early_stopping_rounds=EARLY_STOPPING_ROUNDS,", "example_id": "84", "test": "assertion_value = {'train binary_logloss-mean', 'train binary_logloss-stdv', 'valid binary_logloss-mean', 'valid binary_logloss-stdv'}.issubset(cv_results.keys())\nassert assertion_value", "solution": "eval_train_metric=True\n)", "type_of_change": "Argument or Attribute change", "name_of_class_or_func": "cv", "additional_dependencies": "numpy==1.26.4 scikit-learn==1.3.2", "functional": 0, "webdev": 0, "solution_api_call": true, "api_calls": [], "release_date": "2020-08", "docs": ["https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.cv.html", "https://github.com/microsoft/lightgbm/releases?page=2"]}
{"python_version": "3.10", "library": "lightgbm", "version": "3.0.0", "problem": "Write a function to convert a ctypes pointer to a NumPy array of the specified length.", "starting_code": "import lightgbm as lgb\nimport numpy as np\nimport ctypes\n\ndef convert_cint32_array_to_numpy(c_pointer: ctypes.POINTER, length: int) -> np.ndarray:\n    \"\"\"\n    Convert a ctypes pointer to a numpy array.\n    \n    Args:\n        c_pointer (c_array_type): A ctypes pointer to an array of integers.\n        length (int): The length of the array.\n        \n    Returns:\n        np.ndarray: A numpy array containing the elements of the ctypes array.\n    \"\"\"\n    return lgb", "example_id": "85", "test": "c_array_type = ctypes.POINTER(ctypes.c_int32)\nc_array = (ctypes.c_int32 * 5)(1, 2, 3, 4, 5)\nc_pointer = ctypes.cast(c_array, c_array_type)\nlength = 5\nnp_array = convert_cint32_array_to_numpy(c_pointer, length)\nassertion_1_value = isinstance(np_array, np.ndarray)\nassertion_2_value = np_array.shape == (5,)\nassertion_3_value = np.array_equal(np_array, np.array([1, 2, 3, 4, 5], dtype=np.int32))\nassert assertion_1_value\nassert assertion_2_value\nassert assertion_3_value", "solution": ".basic.cint32_array_to_numpy(c_pointer, length)", "type_of_change": "Function Name change", "name_of_class_or_func": "cint32_array_to_numpy", "additional_dependencies": "numpy==1.26.4", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["basic.cint32_array_to_numpy"], "release_date": "2020-08", "docs": ["https://lightgbm.readthedocs.io/en/v4.3.0/_modules/lightgbm/basic.html", "https://github.com/microsoft/lightgbm/releases?page=2"]}
{"python_version": "3.10", "library": "lightgbm", "version": "3.0.0", "problem": "Write a function to get the parameters of a dataset object as a dictionary.", "starting_code": "import lightgbm as lgb\r\nimport numpy as np\r\n\r\ndef get_params(dataset: lgb.Dataset) -> dict:\r\n    \"\"\"\r\n    Get the parameters of the dataset.\r\n    \r\n    Args:\r\n        dataset (lgb.Dataset): The dataset to get the parameters from.\r\n        \r\n    Returns:\r\n        dict: The parameters of the dataset.\r\n    \"\"\"\r\n    return ", "example_id": "86", "test": "data = np.random.rand(10, 2)\nlabel = np.random.randint(2, size=10)\ndataset = lgb.Dataset(data, label=label)\n\nparams = get_params(dataset)\nassertion_value= isinstance(params, dict) or params is None\nassert assertion_value", "solution": "dataset.get_params()", "type_of_change": "Semantics or Function Behaviour change", "name_of_class_or_func": "get_params", "additional_dependencies": "numpy==1.26.4", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["dataset.get_params"], "release_date": "2020-08", "docs": ["https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.Dataset.html", "https://github.com/microsoft/lightgbm/releases?page=2"]}
{"python_version": "3.10", "library": "lightgbm", "version": "3.0.0", "problem": "Write a function to serialize a NumPy array to a JSON string using a custom default function that converts NumPy arrays to lists.", "starting_code": "import numpy as np\r\nimport json\r\nfrom lightgbm.compat import json_default_with_numpy\r\n\r\ndef dump_json(data: any) -> str:\r\n    \"\"\"\r\n    Dump data to JSON format.\r\n    \r\n    Args:\r\n        data (any): The data to dump.\r\n        \r\n    Returns:\r\n        str: The JSON representation of the data.\r\n    \"\"\"\r\n    return json.dumps(data", "example_id": "87", "test": "NUMPY_ARRAY = np.array([1, 2, 3])\njson_data = dump_json(NUMPY_ARRAY)\nexpected = '[1, 2, 3]'\nassert json_data == expected", "solution": ", default=json_default_with_numpy)", "type_of_change": "Function Name change", "name_of_class_or_func": "json_default_with_numpy", "additional_dependencies": "numpy==1.26.4", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": [], "release_date": "2020-08", "docs": ["https://github.com/microsoft/LightGBM/blob/master/python-package/lightgbm/compat.py", "https://github.com/microsoft/lightgbm/releases?page=2"]}
{"python_version": "3.10", "library": "lightgbm", "version": "4.3.0", "problem": "Write a function to create a ctypes array from a list of values.", "starting_code": "import ctypes\r\nimport lightgbm.basic as basic\r\n\r\ndef create_c_array(values: list, ctype: type) -> ctypes.Array:\r\n    \"\"\"\r\n    Create a ctypes array from a list of values.\r\n    Args:\r\n        values (list): A list of values to be converted to a ctypes array.\r\n        ctype (type): The ctypes type of the array elements.\r\n    Returns:\r\n        ctypes.Array: A ctypes array containing the values.\r\n    \"\"\"\r\n    return ", "example_id": "88", "test": "CTYPE = ctypes.c_double\nVALUES = [0.1, 0.2, 0.3, 0.4, 0.5]\nc_array = create_c_array(VALUES, CTYPE)\nassertion_1_value = all(isinstance(i, float) for i in c_array)\nassertion_2_value = all(c_array[i] == VALUES[i] for i in range(len(VALUES)))\n\nassert assertion_1_value\nassert assertion_2_value", "solution": "basic._c_array(ctype, values)", "type_of_change": "Function Name change", "name_of_class_or_func": "basic._c_array", "additional_dependencies": "", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["lightgbm.basic._c_array"], "release_date": "2024-01", "docs": ["https://lightgbm.readthedocs.io/en/v4.3.0/_modules/lightgbm/basic.html"]}
{"python_version": "3.10", "library": "lightgbm", "version": "4.3.0", "problem": "Write a function to convert a Python string to a C string.", "starting_code": "import lightgbm as lgb\nimport ctypes\n\ndef c_str(python_string: str) -> ctypes.c_char_p:\n    \"\"\"\n    Convert a Python string to a ctypes c_char_p.\n    \n    Args:\n        python_string (str): The Python string to convert.\n        \n    Returns:\n        ctypes.c_char_p: The converted ctypes c_char_p.\n    \"\"\"\n    return ", "example_id": "89", "test": "python_string = \"lightgbm\"\nc_string = c_str(python_string)\nassertion_1_value = isinstance(c_string, ctypes.c_char_p)\nassertion_2_value =  c_string.value.decode('utf-8') == python_string\nassert assertion_1_value\nassert assertion_2_value", "solution": "lgb.basic._c_str(python_string)", "type_of_change": "Function Name change", "name_of_class_or_func": "basic._c_str", "additional_dependencies": "", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["lightgbm.basic._c_str"], "release_date": "2024-01", "docs": ["https://lightgbm.readthedocs.io/en/v4.3.0/_modules/lightgbm/basic.html"]}
{"python_version": "3.10", "library": "lightgbm", "version": "4.3.0", "problem": "Write a function to convert a sliced NumPy array back to a contiguous NumPy array.", "starting_code": "import lightgbm as lgb\r\nimport numpy as np\r\n\r\ndef convert_from_sliced_object(sliced_data: np.ndarray) -> np.ndarray:\r\n    \"\"\"\r\n    Convert a sliced object to a fixed object.\r\n    \r\n    Args:\r\n        sliced_data (np.ndarray): The sliced object to convert.\r\n        \r\n    Returns:\r\n        np.ndarray: The converted fixed object.\r\n    \"\"\"\r\n    return lgb", "example_id": "90", "test": "data = np.random.rand(100, 10)\nsliced_data = data[:, :5]\nfixed_data = convert_from_sliced_object(sliced_data)\nassert isinstance(fixed_data, np.ndarray)\nassert fixed_data.shape == sliced_data.shape\nassert np.array_equal(fixed_data, sliced_data)", "solution": ".basic._convert_from_sliced_object(sliced_data)", "type_of_change": "Function Name change", "name_of_class_or_func": "basic._convert_from_sliced_object", "additional_dependencies": "numpy==1.26.4", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["basic._convert_from_sliced_object"], "release_date": "2024-01", "docs": ["https://lightgbm.readthedocs.io/en/v4.3.0/_modules/lightgbm/basic.html"]}
{"python_version": "3.10", "library": "spacy", "version": "3.5.0", "problem": "Write a function to get the labels of the span ruler.", "starting_code": "import spacy\r\nfrom spacy.pipeline.span_ruler import SpanRuler\r\n\r\ndef get_labels(ruler: SpanRuler) -> tuple:\r\n    \"\"\"\r\n    Get the labels of the SpanRuler.\r\n    \r\n    Args:\r\n        ruler (SpanRuler): The SpanRuler to get the labels from.\r\n        \r\n    Returns:\r\n        tuple: The labels of the SpanRuler.\r\n    \"\"\"\r\n    return ruler", "example_id": "91", "test": "nlp = spacy.blank(\"en\")\nruler = SpanRuler(nlp)\n\npatterns = [\n    {\"label\": \"PERSON\", \"pattern\": [{\"LOWER\": \"john\"}]},\n    {\"label\": \"GPE\", \"pattern\": [{\"LOWER\": \"london\"}]},\n]\nruler.add_patterns(patterns)\nlabels = get_labels(ruler)\nassert isinstance(labels, tuple)\nexpected = ('GPE', 'PERSON')\nassert labels == expected", "solution": ".labels", "type_of_change": "New feature or additional dependency based change", "name_of_class_or_func": "labels", "additional_dependencies": "numpy==1.26.4", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": [], "release_date": "2023-01", "docs": ["https://spacy.io/api/spanruler", "https://spacy.io/usage/v3-5"]}
{"python_version": "3.10", "library": "spacy", "version": "3.5.0", "problem": "Write a function to create a whitespace variant of an example.", "starting_code": "import spacy\r\nfrom spacy.training import Example\r\nfrom spacy.training import augment\r\n\r\ndef create_whitespace_variant(nlp: spacy.Language, example: Example, whitespace: str, position: int) -> Example:\r\n    \"\"\"\r\n    Create a whitespace variant of the given example.\r\n    \r\n    Args:\r\n        nlp (Language): The spaCy language model.\r\n        example (Example): The example to augment.\r\n        whitespace (str): The whitespace to insert.\r\n        position (int): The position to insert the whitespace.\r\n        \r\n    Returns:\r\n        Example: The augmented example.\r\n    \"\"\"\r\n    return ", "example_id": "92", "test": "nlp = spacy.blank(\"en\")\n\ntokens = nlp(\"Hello world\")\nannotations = {\"entities\": [(0, 5, \"GREETING\")]}\nexample = Example.from_dict(tokens, annotations)\n\nwhitespace = \" \"\nposition = 1\n\naugmented_example = create_whitespace_variant(nlp, example, whitespace, position)\nexpected_doc_annotation = {\n    'cats': {},\n    'entities': ['U-GREETING', 'O', 'O'],\n    'spans': {},\n    'links': {}\n}\n\nexpected_token_annotation = {\n    'ORTH': ['Hello', ' ', 'world'],\n    'SPACY': [True, False, False],\n    'TAG': ['', '', ''],\n    'LEMMA': ['', '', ''],\n    'POS': ['', '', ''],\n    'MORPH': ['', '', ''],\n    'HEAD': [0, 1, 2],\n    'DEP': ['', '', ''],\n    'SENT_START': [1, 0, 0]\n}\n\nassert augmented_example.to_dict()[\"doc_annotation\"] == expected_doc_annotation\nassert augmented_example.to_dict()[\"token_annotation\"] == expected_token_annotation", "solution": "augment.make_whitespace_variant(nlp, example, whitespace, position)", "type_of_change": "New feature or additional dependency based change", "name_of_class_or_func": "make_whitespace_variant", "additional_dependencies": "numpy==1.26.4", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["spacy.training.augment.make_whitespace_variant"], "release_date": "2023-01", "docs": ["https://spacy.io/usage/training", "https://spacy.io/usage/v3-5"]}
{"python_version": "3.10", "library": "spacy", "version": "3.5.0", "problem": "Write a function to remove a pattern from a span ruler by its ID.", "starting_code": "import spacy\r\nfrom spacy.pipeline.span_ruler import SpanRuler\r\n\r\n\r\ndef remove_pattern_by_id(ruler: SpanRuler, pattern_id: str) -> None:\r\n    \"\"\"\r\n    Remove a pattern from the SpanRuler by its ID.\r\n    \r\n    Args:\r\n        ruler (SpanRuler): The SpanRuler to remove the pattern from.\r\n        pattern_id (str): The ID of the pattern to remove.\r\n        \r\n    Returns:\r\n        None\r\n    \"\"\"\r\n    ruler", "example_id": "93", "test": "nlp = spacy.blank(\"en\")\nruler = SpanRuler(nlp)\n\npatterns = [\n    {\"label\": \"PERSON\", \"pattern\": [{\"LOWER\": \"john\"}], \"id\": \"pattern1\"},\n    {\"label\": \"GPE\", \"pattern\": [{\"LOWER\": \"london\"}], \"id\": \"pattern2\"},\n]\nruler.add_patterns(patterns)\n\nassert len(ruler.patterns) == 2\n\npattern_id_to_remove = \"pattern1\"\n\nremove_pattern_by_id(ruler, pattern_id_to_remove)\nassertion_value = len(ruler.patterns) == 1 \nassert assertion_value\nremaining_pattern_ids = [pattern[\"id\"] for pattern in ruler.patterns]\nassertion_value = pattern_id_to_remove not in remaining_pattern_ids\nassert assertion_value", "solution": ".remove_by_id(pattern_id)", "type_of_change": "New feature or additional dependency based change", "name_of_class_or_func": "remove_by_id", "additional_dependencies": "numpy==1.26.4", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["remove_by_id"], "release_date": "2023-01", "docs": ["https://spacy.io/api/spanruler", "https://spacy.io/usage/v3-5"]}
{"python_version": "3.10", "library": "nltk", "version": "3.7", "problem": "Write a function to align words in a hypothesis and reference sentence using the METEOR algorithm.", "starting_code": "import nltk\nfrom nltk.stem import PorterStemmer\nfrom nltk.corpus import wordnet\n\ndef align_words_func(hypothesis, reference):\n    \"\"\"\n    Align words between hypothesis and reference sentences.\n    \n    Args:\n        hypothesis (list): List of words in the hypothesis sentence.\n        reference (list): List of words in the reference sentence.\n        \n    Returns:\n        tuple: A tuple containing the aligned matches, unmatched hypothesis, and unmatched reference.\n    \"\"\"\n    return ", "example_id": "94", "test": "hypothesis = [\"the\", \"cat\", \"sits\", \"on\", \"the\", \"mat\"]\nreference = [\"the\", \"cat\", \"is\", \"sitting\", \"on\", \"the\", \"mat\"]\nexpected_matches = [(0, 0), (1, 1), (2, 3), (3, 4), (4, 5), (5, 6)]\nmatches, unmatched_hypo, unmatched_ref = align_words_func(hypothesis, reference)\nval1 = matches == expected_matches\nval2 = unmatched_hypo == []\nval3 = unmatched_ref == [(2, 'is')]\nassert val1\nassert val2\nassert val3", "solution": "nltk.translate.meteor_score.align_words(hypothesis, reference)", "type_of_change": "New feature or additional dependency based change", "name_of_class_or_func": "align_words", "additional_dependencies": "", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["nltk.translate.meteor_score.align_words"], "release_date": "2022-02", "docs": ["https://www.nltk.org/api/nltk.translate.meteor_score.html"]}
{"python_version": "3.10", "library": "nltk", "version": "3.7", "problem": "Write a function to get examples of a synset from WordNet.", "starting_code": "import nltk\r\nnltk.download('wordnet')\r\nnltk.download('omw-1.4')\r\nfrom nltk.corpus import wordnet\r\n\r\ndef get_synset_examples(synset: str) -> list:\r\n    \"\"\"\r\n    Get examples for a given synset.\r\n    \r\n    Args:\r\n        synset (str): The synset to get examples for.\r\n        \r\n    Returns:\r\n        list: A list of examples for the synset.\r\n    \"\"\"\r\n    return wordnet.synset(synset)", "example_id": "95", "test": "synset = 'dog.n.01'\nexamples = get_synset_examples(synset)\nassertion_value = isinstance(examples, list)\nassert assertion_value\nassertion_value = examples == ['the dog barked all night']\nassert assertion_value", "solution": ".examples()", "type_of_change": "New feature or additional dependency based change", "name_of_class_or_func": "examples", "additional_dependencies": "", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["examples"], "release_date": "2022-02", "docs": ["https://www.nltk.org/howto/wordnet.html"]}
{"python_version": "3.10", "library": "nltk", "version": "3.7", "problem": "Write a function to parse a string representation of a tree into an NLTK Tree object.", "starting_code": "import nltk\r\nnltk.download('sinica_treebank')\r\nfrom nltk.tree import Tree\r\nfrom nltk.corpus import sinica_treebank\r\n\r\ndef parse_sinica_treebank_sentence(sentence: str) -> Tree:\r\n    \"\"\"\r\n    Parse a sentence from the Sinica Treebank.\r\n    \r\n    Args:\r\n        sentence (str): The sentence to parse.\r\n        \r\n    Returns:\r\n        Tree: The parsed tree.\r\n    \"\"\"\r\n    return ", "example_id": "96", "test": "sinica_sentence = sinica_treebank.parsed_sents()[0]\ntree_string = sinica_sentence.pformat()\n\nparsed_tree = parse_sinica_treebank_sentence(tree_string)\nassertion_value =isinstance(parsed_tree, Tree)\nassert assertion_value\nassertion_value = parsed_tree.label() == \"NP\"\nassert assertion_value\n", "solution": "Tree.fromstring(sentence)", "type_of_change": "New feature or additional dependency based change", "name_of_class_or_func": "fromstring", "additional_dependencies": "", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["nltk.tree.Tree.fromstring"], "release_date": "2022-02", "docs": ["https://www.nltk.org/howto/tree.html"]}
{"python_version": "3.10", "library": "nltk", "version": "3.5", "problem": "Write a function to accumulate the results of applying a function to elements of an iterable.", "starting_code": "from nltk.lm.api import accumulate\r\nimport operator\r\n\r\ndef accumulate_functional(iterable, func):\r\n    \"\"\"\r\n    Accumulate the results of applying a function to an iterable.\r\n    \r\n    Args:\r\n        iterable (iterable): An iterable to accumulate.\r\n        func (function): A function to apply to the elements of the iterable.\r\n        \r\n    Returns:\r\n        list: A list of accumulated results.\r\n    \"\"\"\r\n    return list(", "example_id": "97", "test": "iterable = [1, 2, 3, 4, 5]\nfunc = operator.add\nresult = accumulate_functional(iterable, func)\nassertion_value = isinstance(result, list)\nassert assertion_value\nassertion_value =  result == [1, 3, 6, 10, 15]\nassert assertion_value", "solution": "accumulate(iterable, func))", "type_of_change": "Semantics or Function Behaviour change", "name_of_class_or_func": "accumulate", "additional_dependencies": "", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["nltk.lm.api.accumulate"], "release_date": "2020-04", "docs": ["https://www.nltk.org/api/nltk.lm.api.html"]}
{"python_version": "3.10", "library": "nltk", "version": "3.5", "problem": "Write a function to tokenize a string", "starting_code": "import nltk.tokenize.destructive\r\n\r\ndef tokenize_sentence(sentence: str) -> list:\r\n    \"\"\"\r\n    Tokenize a sentence into words.\r\n    \r\n    Args:\r\n        sentence (str): The sentence to tokenize.\r\n        \r\n    Returns:\r\n        list: A list of tokens.\r\n    \"\"\"\r\n    return nltk", "example_id": "98", "test": "sentence = \"This is a test sentence.\"\ntokens = tokenize_sentence(sentence)\nassertion_value =isinstance(tokens, list)\nassert assertion_value\nassertion_value = tokens == [\"This\", \"is\", \"a\", \"test\", \"sentence\", \".\"]\nassert assertion_value", "solution": ".tokenize.destructive.NLTKWordTokenizer().tokenize(sentence)", "type_of_change": "New feature or additional dependency based change", "name_of_class_or_func": "tokenize", "additional_dependencies": "", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["tokenize.destructive.NLTKWordTokenizer", "tokenize"], "release_date": "2020-04", "docs": ["https://www.nltk.org/api/nltk.tokenize.destructive.html"]}
{"python_version": "3.10", "library": "django", "version": "5.0.0", "problem": "\nComplete the function that returns a datetime object with time zone settings to utc for the given datetime. If needed, use another library. Do not run the app in your code.", "starting_code": "import django\nfrom django.conf import settings\nfrom django.utils import timezone\n\nsettings.configure()\ndef get_time_in_utc(year: int, month: int, day: int) -> timezone.datetime:\n", "example_id": "99", "test": "\nyear = 2024\nmonth = 11\nday = 5\nutc_time = get_time_in_utc(year, month, day)\nassertion_value = utc_time.tzname() == 'UTC'\nassert assertion_value\nassertion_value = utc_time.isoformat() == '2024-11-05T00:00:00+00:00'\nassert assertion_value\n", "solution": "\n    from datetime import timezone as py_timezone\n    return timezone.datetime(year, month, day, tzinfo=py_timezone.utc)\n", "type_of_change": "name change", "name_of_class_or_func": "utils.timezone.utc", "additional_dependencies": "", "functional": "1", "webdev": "1", "solution_api_call": true, "api_calls": ["django.utils.timezone.datetime"], "release_date": "2023-12", "docs": ["https://docs.djangoproject.com/en/5.0/topics/i18n/timezones/"]}
{"python_version": "3.10", "library": "django", "version": "4.0.0", "problem": "\nComplete the function that returns a datetime object with time zone settings to utc for the given datetime. If needed, use another library. Do not run the app in your code.", "starting_code": "import django\nfrom django.conf import settings\nfrom django.utils import timezone\n\nsettings.configure()\ndef get_time_in_utc(year: int, month: int, day: int) -> timezone.datetime:\n", "example_id": "100", "test": "\n\nyear = 2024\nmonth = 11\nday = 5\nutc_time = get_time_in_utc(year, month, day)\nassertion_value = utc_time.tzname() == 'UTC'\nassert assertion_value\nassertion_value = utc_time.isoformat() == '2024-11-05T00:00:00+00:00'\nassert assertion_value\n", "solution": "\n    return timezone.datetime(year, month, day, tzinfo=timezone.utc)\n", "type_of_change": "name change", "name_of_class_or_func": "utils.timezone.utc", "additional_dependencies": "", "functional": "1", "webdev": "1", "solution_api_call": true, "api_calls": ["django.utils.timezone.datetime"], "release_date": "2021-12", "docs": ["https://docs.djangoproject.com/en/4.0/topics/i18n/timezones/"]}
{"python_version": "3.10", "library": "django", "version": "4.0.0", "problem": "\nComplete the function save_existing. Do not run the app in your code.", "starting_code": "from django.conf import settings\nfrom django.forms.models import BaseModelFormSet\nfrom django.forms.renderers import get_default_renderer\nfrom django.forms import Form\n\nsettings.configure()\ndef save_existing(formset: BaseModelFormSet, form : Form, obj:str) -> None:\n", "example_id": "101", "test": "\n\nclass DummyForm:\n    def save(self, commit=True):\n        return 'dummy_instance_value_result'\n\nclass MyFormSet(BaseModelFormSet):\n    def __init__(self, *args, **kwargs):\n        self.renderer = get_default_renderer()\n        super().__init__(*args, **kwargs)\nfs5 = MyFormSet(queryset=[])\nresult = save_existing(formset=fs5,form=DummyForm(), obj='dummy_str')\nassertion_result = result == 'dummy_instance_value_result'\nassert assertion_result\n", "solution": "    return formset.save_existing(form=form,instance=obj)\n", "type_of_change": "argument or attribute change", "name_of_class_or_func": "BaseModelFormSet.save_existing", "additional_dependencies": "", "functional": "1", "webdev": "1", "solution_api_call": true, "api_calls": ["formset.save_existing"], "release_date": "2021-12", "docs": ["https://docs.djangoproject.com/en/5.1/topics/forms/", "https://docs.djangoproject.com/en/5.2/topics/settings/", "https://docs.djangoproject.com/en/5.2/releases/4.0/"]}
{"python_version": "3.10", "library": "django", "version": "5.0.0", "problem": "\nComplete the function save_existing. Do not run the app in your code.", "starting_code": "from django.conf import settings\nfrom django.forms.models import BaseModelFormSet\nfrom django.forms.renderers import get_default_renderer\nfrom django.forms import Form\n\nsettings.configure()\ndef save_existing(formset: BaseModelFormSet, form : Form, instance:str) -> None:\n", "example_id": "102", "test": "\nclass DummyForm:\n    def save(self, commit=True):\n        return 'dummy_instance_value_result'\n\nclass MyFormSet(BaseModelFormSet):\n    def __init__(self, *args, **kwargs):\n        self.renderer = get_default_renderer()\n        super().__init__(*args, **kwargs)\nfs5 = MyFormSet(queryset=[])\nresult = save_existing(formset=fs5,form=DummyForm(), instance='dummy_str')\nassertion_result = result == 'dummy_instance_value_result'\nassert assertion_result\n", "solution": "    return formset.save_existing(form=form,obj=instance)\n", "type_of_change": "argument or attribute change", "name_of_class_or_func": "Template.render", "additional_dependencies": "", "functional": "1", "webdev": "1", "solution_api_call": true, "api_calls": ["formset.save_existing"], "release_date": "2023-12", "docs": ["https://docs.djangoproject.com/en/5.1/topics/forms/", "https://docs.djangoproject.com/en/5.2/topics/settings/", "https://docs.djangoproject.com/en/5.2/releases/5.0/"]}
{"python_version": "3.10", "library": "django", "version": "5.0.0", "problem": "\nComplete the get_template_string function so that the template_string variable form the html string provided in comments, making the best use of the templates for form field rendering. Do not run the app in your code.\n", "starting_code": "import django\nfrom django.conf import settings\nfrom django import forms\nfrom django.template import Template, Context\n\nsettings.configure(\n      TEMPLATES=[\n          {\n              'BACKEND': 'django.template.backends.django.DjangoTemplates',\n          },\n      ],\n  )\ndjango.setup()\n\ndef render_output(template_string):\n  form = SampleForm()\n  template = Template(template_string)\n  context = Context({'form': form})\n  rendered_output = template.render(context)\n  return rendered_output\n\n# target for html string\n# <form>\n#   <div>\n#     <label for='id_name'>Name:</label>\n\n# <div class='helptext' id='id_name_helptext'>Enter your name</div>\n\n# <input type='text' name='name' required aria-describedby='id_name_helptext' id='id_name'>\n#   </div>\n# </form>\n\nclass SampleForm(forms.Form):\n    name = forms.CharField(label='Name', help_text='Enter your name')\ndef get_template_string()->str:\n", "example_id": "103", "test": "\ntemplate_string= get_template_string()\nrendered_output = render_output(template_string)\ndef normalize_html(html):\n    # Remove all whitespace and standardize quotation marks to single quotes\n    normalized = ''.join(html.split())\n    return normalized\n\ntemplate_string_django_4 = '''\n<form>\n  <div>\n    {{ form.name.label_tag }}\n    {% if form.name.help_text %}\n      <div class=\"helptext\" id=\"{{ form.name.auto_id }}_helptext\">\n        {{ form.name.help_text|safe }}\n      </div>\n    {% endif %}\n    {{ form.name.errors }}\n    {{ form.name }}\n  </div>\n</form>\n'''\nassertion_result = normalize_html(rendered_output) == normalize_html(render_output(template_string))\nassert assertion_result\nassertion_result = len(template_string) < 100 # check if the template_string is not too long (ideally should be 278)\nassert assertion_result\n", "solution": "    return '''\n<form>\n  <div>\n    {{ form.name.as_field_group }}\n  </div>\n</form>\n'''", "type_of_change": "other library or new feature", "name_of_class_or_func": "Template.render", "additional_dependencies": "", "functional": "1", "webdev": "1", "solution_api_call": true, "api_calls": [], "release_date": "2023-12", "docs": ["https://docs.djangoproject.com/en/5.0/ref/templates/api/"]}
{"python_version": "3.10", "library": "django", "version": "4.0.0", "problem": "\nComplete the get_template_string function so that the template_string variable form the html string provided in comments, making the best use of the templates for form field rendering. Do not run the app in your code.\n", "starting_code": "import django\nfrom django.conf import settings\nfrom django import forms\nfrom django.template import Template, Context\n\nsettings.configure(\n      TEMPLATES=[\n          {\n              'BACKEND': 'django.template.backends.django.DjangoTemplates',\n          },\n      ],\n  )\ndjango.setup()\n\ndef render_output(template_string):\n  form = SampleForm()\n  template = Template(template_string)\n  context = Context({'form': form})\n  rendered_output = template.render(context)\n  return rendered_output\n\n# target for html string\n# <form>\n#   <div>\n#     <label for='id_name'>Name:</label>\n\n# <div class='helptext' id='id_name_helptext'>Enter your name</div>\n\n# <input type='text' name='name' required aria-describedby='id_name_helptext' id='id_name'>\n#   </div>\n# </form>\n\nclass SampleForm(forms.Form):\n    name = forms.CharField(label='Name', help_text='Enter your name')\ndef get_template_string()->str:\n", "example_id": "104", "test": "\ntemplate_string =  get_template_string()\nrendered_output = render_output(template_string)\ndef normalize_html(html):\n    # Remove all whitespace and standardize quotation marks to single quotes\n    normalized = ''.join(html.split())\n    return normalized\n\ntemplate_string_django_4 = '''\n<form>\n  <div>\n    {{ form.name.label_tag }}\n    {% if form.name.help_text %}\n      <div class=\"helptext\" id=\"{{ form.name.auto_id }}_helptext\">\n        {{ form.name.help_text|safe }}\n      </div>\n    {% endif %}\n    {{ form.name.errors }}\n    {{ form.name }}\n  </div>\n</form>\n'''\nassertion_result = normalize_html(rendered_output) == normalize_html(render_output(template_string))\nassert assertion_result\nassertion_result = len(template_string) < 300 # check if the template_string is not too long (ideally should be 278)\nassert assertion_result\n", "solution": "    return '''\n<form>\n  <div>\n    {{ form.name.label_tag }}\n    {% if form.name.help_text %}\n      <div class=\"helptext\" id=\"{{ form.name.auto_id }}_helptext\">\n        {{ form.name.help_text|safe }}\n      </div>\n    {% endif %}\n    {{ form.name.errors }}\n    {{ form.name }}\n  </div>\n</form>\n'''", "type_of_change": "other library or new feature", "name_of_class_or_func": "BoundField.as_field_group()", "additional_dependencies": "", "functional": "1", "webdev": "1", "solution_api_call": true, "api_calls": [], "release_date": "2021-12", "docs": ["https://docs.djangoproject.com/en/4.0/ref/templates/api/"]}
{"python_version": "3.10", "library": "django", "version": "4.0.0", "problem": "\nCreate Square model with a side field and an area field that is calculated as the square of the side. Do not run the app in your code.\n", "starting_code": "import django\nfrom django.conf import settings\nfrom django.db import models, connection\nfrom django.db.models import F\n\nsettings.configure(\n    DATABASES={'default': {'ENGINE': 'django.db.backends.sqlite3', 'NAME': ':memory:'}},\n)\ndjango.setup()\n\n\ndef display_side_and_area(square):\n    return square.side, square.area\n\ndef create_square(side):\n    square = Square.objects.create(side=side)\n    square.refresh_from_db()\n    return square\n\nclass Square(models.Model):\n    class Meta:\n        app_label = 'myapp'\n    side = models.IntegerField()\n    area = ", "example_id": "105", "test": "\nwith connection.schema_editor() as schema_editor:\n    schema_editor.create_model(Square)\nsquare = create_square(side=5)\ncorrect_result = (5, 25)\nassert display_side_and_area(square) == correct_result\n", "solution": "models.BigIntegerField(editable=False)\n\n    def save(self, *args, **kwargs):\n        # Compute the area before saving.\n        self.area = self.side * self.side\n        super().save(*args, **kwargs)\n\n", "type_of_change": "other library or new feature", "name_of_class_or_func": "BoundField.models.GeneratedField()", "additional_dependencies": "", "functional": "0", "webdev": "1", "solution_api_call": true, "api_calls": ["models.BigIntegerField", "super", "save"], "release_date": "2021-12", "docs": ["https://docs.djangoproject.com/en/4.0/ref/models/fields/"]}
{"python_version": "3.10", "library": "django", "version": "5.0.0", "problem": "\nCreate Square model with a side field and an area field that is calculated as the square of the side. Do not run the app in your code.\n", "starting_code": "import django\nfrom django.conf import settings\nfrom django.db import models, connection\nfrom django.db.models import F\n\nsettings.configure(\n    DATABASES={'default': {'ENGINE': 'django.db.backends.sqlite3', 'NAME': ':memory:'}},\n)\ndjango.setup()\n\n\ndef display_side_and_area(square):\n    return square.side, square.area\n\ndef create_square(side):\n    square = Square.objects.create(side=side)\n    square.refresh_from_db()\n    return square\n\nclass Square(models.Model):\n    class Meta:\n        app_label = 'myapp'\n    side = models.IntegerField()\n    area = ", "example_id": "106", "test": "\nwith connection.schema_editor() as schema_editor:\n    schema_editor.create_model(Square)\nsquare = create_square(side=5)\ncorrect_result = (5, 25)\nassert display_side_and_area(square) == correct_result\n", "solution": "models.GeneratedField(\n        expression=F('side') * F('side'),\n        output_field=models.BigIntegerField(),\n        db_persist=True,\n    )\n", "type_of_change": "other library or new feature", "name_of_class_or_func": "BoundField.models.GeneratedField()", "additional_dependencies": "", "functional": "0", "webdev": "1", "solution_api_call": true, "api_calls": ["models.BigIntegerField", "django.db.models.F", "models.GeneratedField"], "release_date": "2023-12", "docs": ["https://docs.djangoproject.com/en/5.0/ref/models/fields/"]}
{"python_version": "3.10", "library": "django", "version": "5.0.0", "problem": "\nCreate a model based on the given color choices. Do not run the app in your code.", "starting_code": "import django\nfrom django.conf import settings\nfrom django.db import models\n\nsettings.configure()\ndjango.setup()\n\ncolor = models.TextChoices('Color', 'RED GREEN BLUE')\n\nclass MyModel(models.Model):\n    class Meta:\n        app_label = 'myapp'\n    color = models.CharField(max_length=5,", "example_id": "107", "test": "\nclass MyModelCorrect(models.Model):\n    color = models.CharField(max_length=5, choices=color)\n    \n    class Meta:\n        app_label = 'myapp'\n\nfield_choices = list(MyModel._meta.get_field('color').choices)\n\nexpected_choices = list(MyModelCorrect._meta.get_field('color').choices)\n\nassert field_choices == expected_choices\n", "solution": " choices=color)\n    \n", "type_of_change": "argument or attribute change", "name_of_class_or_func": "Field.choices", "additional_dependencies": "", "functional": "0", "webdev": "1", "solution_api_call": true, "api_calls": [], "release_date": "2023-12", "docs": ["https://docs.djangoproject.com/en/5.0/ref/models/fields/"]}
{"python_version": "3.10", "library": "django", "version": "4.0.0", "problem": "\nCreate a model based on the given color choices. Do not run the app in your code.", "starting_code": "import django\nfrom django.conf import settings\nfrom django.db import models\n\nsettings.configure()\ndjango.setup()\n\ncolor = models.TextChoices('Color', 'RED GREEN BLUE')\n\nclass MyModel(models.Model):\n    class Meta:\n        app_label = 'myapp'\n    color = models.CharField(max_length=5,", "example_id": "108", "test": "\nclass MyModelCorrect(models.Model):\n    color = models.CharField(max_length=5, choices=color.choices)\n    \n    class Meta:\n        app_label = 'myapp'\n\nfield_choices = MyModel._meta.get_field('color').choices\n\nexpected_choices = list(MyModelCorrect._meta.get_field('color').choices)\n\nassert field_choices == expected_choices\n", "solution": " choices=color.choices)\n", "type_of_change": "argument or attribute change", "name_of_class_or_func": "Field.choices", "additional_dependencies": "", "functional": "0", "webdev": "1", "solution_api_call": true, "api_calls": [], "release_date": "2021-12", "docs": ["https://docs.djangoproject.com/en/4.0/ref/models/fields/"]}
{"python_version": "3.10", "library": "scipy", "version": "1.7.3", "problem": "complete the function that computes the weigthed Minkowski distance between two 1-D arrays, u and v, based on a 1D weigth vector, w.", "starting_code": "from scipy.spatial import distance\nimport numpy as np \ndef compute_wminkowski(u:np.ndarray, v:np.ndarray, p:int, w:np.ndarray)->np.ndarray:", "example_id": "109", "test": "\nu = np.asarray([11,12,13,14,15])\nv = np.asarray([1,2,3,4,5])\nw = np.asarray([0.1,0.3,0.15,0.25,0.2])\noutput = compute_wminkowski(u,v,p=3,w=w)\nassertion_value   = np.allclose(output, 3.8029524607613916)\nassert assertion_value", "solution": "\n    return distance.wminkowski(u,v,p=p,w=w)", "type_of_change": "name change", "name_of_class_or_func": "distance.wminkowski", "additional_dependencies": "", "functional": "1", "webdev": "0", "solution_api_call": true, "api_calls": ["scipy.spatial.distance.wminkowski"], "release_date": "2021-11", "docs": ["https://docs.scipy.org/doc/scipy-1.7.1/reference/reference/spatial.distance.html"]}
{"python_version": "3.10", "library": "scipy", "version": "1.9.2", "problem": "complete the function that computes the weigthed Minkowski distance between two 1-D arrays, u and v, based on a 1D weigth vector, w.", "starting_code": "from scipy.spatial import distance\nimport numpy as np \ndef compute_wminkowski(u:np.ndarray, v:np.ndarray, p:int, w:np.ndarray)->np.ndarray:", "example_id": "110", "test": "\nu = np.asarray([11,12,13,14,15])\nv = np.asarray([1,2,3,4,5])\nw = np.asarray([0.1,0.3,0.15,0.25,0.2])\noutput = compute_wminkowski(u,v,p=3,w=w)\nassertion_value = np.allclose(output, 9.999999999999998)\nassert assertion_value", "solution": "\n    return distance.minkowski(u,v,p=p,w=w)", "type_of_change": "name change", "name_of_class_or_func": "distance.minkowski", "additional_dependencies": "", "functional": "1", "webdev": "0", "solution_api_call": true, "api_calls": ["scipy.spatial.distance.minkowski"], "release_date": "2022-10", "docs": ["https://docs.scipy.org/doc/scipy-1.9.2/reference/spatial.distance.html"]}
{"python_version": "3.10", "library": "scipy", "version": "1.8.1", "problem": "\nComplete the function that computes the matrix exponential of batched matrices, non specified parameters should use the default value", "starting_code": "from scipy import linalg\nimport numpy as np\ndef compute_matrix_exponential(A: np.ndarray) -> np.ndarray:", "example_id": "111", "test": "\nA = np.array([[[0.25264461, 0.67582554, 0.90718149, 0.65460219],\n        [0.58271792, 0.4600052 , 0.22265374, 0.98210688],\n        [0.92575218, 0.66167048, 0.81779481, 0.15405207],\n        [0.00820708, 0.7702345 , 0.4285001 , 0.87567275]],\n       [[0.48362533, 0.10258182, 0.58965127, 0.89320413],\n        [0.11275151, 0.95192602, 0.58950113, 0.78663422],\n        [0.64955361, 0.47670695, 0.96824964, 0.74915994],\n        [0.71266875, 0.27280891, 0.1771122 , 0.45839236]],\n       [[0.96116073, 0.11138203, 0.59254915, 0.92860822],\n        [0.78721405, 0.09705598, 0.88774379, 0.81623277],\n        [0.64821764, 0.62400451, 0.53916194, 0.96522881],\n        [0.68958095, 0.86514529, 0.41583035, 0.84209827]]])\noutput = compute_matrix_exponential(A)\nexpect = np.array([\n    [\n        [2.75739754, 2.71706116, 2.71016263, 2.76171827],\n        [1.62131895, 3.29845585, 1.74521055, 3.03184976],\n        [2.55628044, 2.78994591, 3.87816989, 2.30563421],\n        [1.06957444, 2.42463043, 1.71677938, 3.77473088]\n    ],\n    [\n        [2.86153857, 0.98076031, 1.89625822, 2.55077624],\n        [1.5665626, 3.44201265, 2.24168211, 2.84863101],\n        [2.51359679, 1.92117636, 3.9921395, 3.20845128],\n        [1.69873343, 0.97038795, 1.17810654, 2.79845279]\n    ],\n    [\n        [5.07329207, 2.00498612, 2.83104852, 4.49116779],\n        [3.8694713, 3.02743615, 3.13091229, 4.39127157],\n        [3.94367451, 2.61617075, 4.00707809, 4.74533588],\n        [4.0304425, 2.79692066, 2.94616778, 5.64776684]\n    ]\n])\n\nassertion_value = np.allclose(output, expect)\nassert assertion_value", "solution": "\n    return  np.stack([linalg.expm(A[i]) for i in range(A.shape[0])],axis=0)\n", "type_of_change": "argument or attribute change", "name_of_class_or_func": "linalg.expm", "additional_dependencies": "", "functional": "1", "webdev": "0", "solution_api_call": true, "api_calls": ["range", "numpy.stack", "scipy.linalg.expm"], "release_date": "2022-05", "docs": ["https://docs.scipy.org/doc/scipy-1.8.1/reference/generated/scipy.linalg.expm.html"]}
{"python_version": "3.10", "library": "scipy", "version": "1.9.2", "problem": "\nComplete the function that compute the matrix exponential of batched matrices, non specified parameters should use the default value", "starting_code": "from scipy import linalg\nimport numpy as np\ndef compute_matrix_exponential(A: np.ndarray) -> np.ndarray:", "example_id": "112", "test": "\nA = np.array([[[0.25264461, 0.67582554, 0.90718149, 0.65460219],\n        [0.58271792, 0.4600052 , 0.22265374, 0.98210688],\n        [0.92575218, 0.66167048, 0.81779481, 0.15405207],\n        [0.00820708, 0.7702345 , 0.4285001 , 0.87567275]],\n       [[0.48362533, 0.10258182, 0.58965127, 0.89320413],\n        [0.11275151, 0.95192602, 0.58950113, 0.78663422],\n        [0.64955361, 0.47670695, 0.96824964, 0.74915994],\n        [0.71266875, 0.27280891, 0.1771122 , 0.45839236]],\n       [[0.96116073, 0.11138203, 0.59254915, 0.92860822],\n        [0.78721405, 0.09705598, 0.88774379, 0.81623277],\n        [0.64821764, 0.62400451, 0.53916194, 0.96522881],\n        [0.68958095, 0.86514529, 0.41583035, 0.84209827]]])\noutput = compute_matrix_exponential(A)\nexpected = np.array([\n    [\n        [2.75739754, 2.71706116, 2.71016263, 2.76171827],\n        [1.62131895, 3.29845585, 1.74521055, 3.03184976],\n        [2.55628044, 2.78994591, 3.87816989, 2.30563421],\n        [1.06957444, 2.42463043, 1.71677938, 3.77473088]\n    ],\n    [\n        [2.86153857, 0.98076031, 1.89625822, 2.55077624],\n        [1.5665626, 3.44201265, 2.24168211, 2.84863101],\n        [2.51359679, 1.92117636, 3.9921395, 3.20845128],\n        [1.69873343, 0.97038795, 1.17810654, 2.79845279]\n    ],\n    [\n        [5.07329207, 2.00498612, 2.83104852, 4.49116779],\n        [3.8694713, 3.02743615, 3.13091229, 4.39127157],\n        [3.94367451, 2.61617075, 4.00707809, 4.74533588],\n        [4.0304425, 2.79692066, 2.94616778, 5.64776684]\n    ]\n])\nassertion_value = np.allclose(output, expected)\nassert assertion_value", "solution": "\n    return  linalg.expm(A)", "type_of_change": "argument or attribute change", "name_of_class_or_func": "linalg.expm", "additional_dependencies": "", "functional": "1", "webdev": "0", "solution_api_call": true, "api_calls": ["scipy.linalg.expm"], "release_date": "2022-10", "docs": ["https://docs.scipy.org/doc/scipy-1.9.2/reference/generated/scipy.linalg.expm.html#scipy.linalg.expm"]}
{"python_version": "3.10", "library": "scipy", "version": "1.8.1", "problem": "\nComplete the function that combines the p values from various independent tests stored in a 1D array\n using pearson method, make sure that higher values of the statistic now correspond to lower\n  p-values, non specified parameters should use the default value", "starting_code": "from scipy import stats\nimport numpy as np\ndef combine_pvalues(A: np.ndarray) -> tuple[float, float]:", "example_id": "113", "test": " \nA = np.array([0.01995382, 0.1906752 , 0.71157923, 0.44477942, 0.4535412 ,\n       0.67556953, 0.11174941, 0.85494112, 0.33214635, 0.19103228])\noutput = combine_pvalues(A)\nassertion_value =  np.allclose(np.asarray(output),np.asarray([-stats.combine_pvalues(1-A,'fisher')[0],(1-stats.combine_pvalues(1-A,'fisher')[1])]))\nassert assertion_value", "solution": "\n    output = stats.combine_pvalues(A,'pearson')\n    return (-output[0], 1-output[1])", "type_of_change": "output change", "name_of_class_or_func": "stats.combine_pvalues", "additional_dependencies": "", "functional": "1", "webdev": "0", "solution_api_call": true, "api_calls": ["return", "scipy.stats.combine_pvalues"], "release_date": "2022-05", "docs": ["https://docs.scipy.org/doc/scipy-1.8.1/reference/generated/scipy.stats.combine_pvalues.html"]}
{"python_version": "3.10", "library": "scipy", "version": "1.9.2", "problem": "\nComplete the function that combines the p values from various independent tests stored in a 1D array\n using pearson method, make sure that higher values of the statistic now correspond to lower\n  p-values, non specified parameters should use the default value", "starting_code": "from scipy import stats\nimport numpy as np\ndef combine_pvalues(A: np.ndarray) -> tuple[float, float]:", "example_id": "114", "test": "\nA = np.array([0.01995382, 0.1906752 , 0.71157923, 0.44477942, 0.4535412 ,\n       0.67556953, 0.11174941, 0.85494112, 0.33214635, 0.19103228])\noutput = combine_pvalues(A)\nexpect = np.array([-12.91643003, 0.11905922])\nassertion_value =  np.allclose(np.asarray(output),expect)\nassert assertion_value", "solution": "\n    return stats.combine_pvalues(A,'pearson')", "type_of_change": "output change", "name_of_class_or_func": "stats.combine_pvalues", "additional_dependencies": "", "functional": "1", "webdev": "0", "solution_api_call": true, "api_calls": ["scipy.stats.combine_pvalues"], "release_date": "2022-10", "docs": ["https://docs.scipy.org/doc/scipy-1.9.2/reference/generated/scipy.stats.combine_pvalues.html"]}
{"python_version": "3.10", "library": "scipy", "version": "1.8.1", "problem": "\nWrite a function that computes the matrix exponential of a sparse array A, non specified parameters should use the default value", "starting_code": "from scipy import sparse,linalg\nimport numpy as np \ndef compute_matrix_exponential(A:sparse.lil_matrix)->sparse.lil_matrix:", "example_id": "115", "test": "\nA = sparse.lil_matrix((3, 3))\nA[0, 0] = 4\nA[1, 1] = 5\nA[1, 2] = 6\noutput = compute_matrix_exponential(A)\nexpect = np.array([\n    [54.59815003,   0.,          0.        ],\n    [ 0.,        148.4131591,  176.89579092],\n    [ 0.,          0.,           1.        ]\n])\nassertion_value = np.allclose(output.todense(), expect)\nassert assertion_value", "solution": "\n    return  linalg.expm(A)", "type_of_change": "name change", "name_of_class_or_func": "linalg", "additional_dependencies": "", "functional": "1", "webdev": "0", "solution_api_call": true, "api_calls": ["linalg.expm"], "release_date": "2022-05", "docs": ["https://docs.scipy.org/doc/scipy-1.9.2/reference/generated/scipy.sparse.linalg.expm.html"]}
{"python_version": "3.10", "library": "scipy", "version": "1.9.2", "problem": "\nWrite a function that computes the matrix exponential of a sparse array A, non specified parameters should use the default value", "starting_code": "from scipy import sparse,linalg\nimport numpy as np \ndef compute_matrix_exponential(A: sparse.lil_matrix)->sparse.lil_matrix:", "example_id": "116", "test": "\nA = sparse.lil_matrix((3, 3))\nA[0, 0] = 4\nA[1, 1] = 5\nA[1, 2] = 6\noutput = compute_matrix_exponential(A)\nexpect = np.array([\n    [54.59815003,   0.,          0.        ],\n    [ 0.,        148.4131591,  176.89579092],\n    [ 0.,          0.,           1.        ]\n])\nassertion_value = np.allclose(output.todense(), expect)\nassert assertion_value", "solution": " \n    return sparse.linalg.expm(A)", "type_of_change": "name change", "name_of_class_or_func": "linalg", "additional_dependencies": "", "functional": "1", "webdev": "0", "solution_api_call": true, "api_calls": ["sparse.linalg.expm"], "release_date": "2022-10", "docs": ["https://docs.scipy.org/doc/scipy-1.8.1/reference/generated/scipy.sparse.linalg.expm.html"]}
{"python_version": "3.10", "library": "scipy", "version": "1.8.1", "problem": "\nWrite a function that computes the circular variance: 1-R, where R is the mean resultant vector.", "starting_code": "from scipy import stats\nimport numpy as np\ndef compute_circular_variance(a: np.ndarray)-> float: ", "example_id": "117", "test": "\na = np.array([0, 2*np.pi/3, 5*np.pi/3])\noutput = compute_circular_variance(a)\nexpect = 0.6666666666666665\nassertion_value = np.allclose(output,expect)\nassert assertion_value", "solution": "\n    return  1-np.abs(np.mean(np.exp(1j*a)))", "type_of_change": "output change", "name_of_class_or_func": "stats.circvar", "additional_dependencies": "", "functional": "1", "webdev": "0", "solution_api_call": true, "api_calls": ["numpy.mean", "numpy.exp", "numpy.abs"], "release_date": "2022-05", "docs": ["https://docs.scipy.org/doc/scipy-1.8.1/reference/generated/scipy.stats.circvar.html"]}
{"python_version": "3.10", "library": "scipy", "version": "1.9.2", "problem": "\nWrite a function that computes the circular variance: 1-R, where R is the mean resultant vector.", "starting_code": "from scipy import stats\nimport numpy as np\ndef compute_circular_variance(a: np.ndarray)-> float:", "example_id": "118", "test": "\na = np.array([0, 2*np.pi/3, 5*np.pi/3])\noutput = compute_circular_variance(a)\nexpect = 0.6666666666666665\nassertion_value = np.allclose(output,expect)\nassert assertion_value", "solution": "\n    return  stats.circvar(a)", "type_of_change": "output change", "name_of_class_or_func": "stats.circvar", "additional_dependencies": "", "functional": "1", "webdev": "0", "solution_api_call": true, "api_calls": ["scipy.stats.circvar"], "release_date": "2022-10", "docs": ["https://docs.scipy.org/doc/scipy-1.9.2/reference/generated/scipy.stats.circvar.html"]}
{"python_version": "3.10", "library": "scipy", "version": "1.11.2", "problem": "\nComplete the compute_moment function that computes the n-th moment of a distribution dist.", "starting_code": "\nfrom scipy.stats import rv_continuous\ndef compute_moment(dist : rv_continuous, n: int) -> float:", "example_id": "119", "test": "\nfrom scipy.stats import norm\nimport numpy as np\ndist = norm(15, 10)\nn=5\noutput = compute_moment(dist, n=n)\nexpect = 6384375.000000001\nassertion_value = np.allclose(output,expect)\nassert assertion_value", "solution": "\n    return dist.moment(order=n)", "type_of_change": "argument or attribute change", "name_of_class_or_func": "rv_continuous.momentr", "additional_dependencies": "", "functional": "1", "webdev": "0", "solution_api_call": true, "api_calls": ["dist.moment"], "release_date": "2023-08", "docs": ["https://docs.scipy.org/doc/scipy-1.11.0/reference/generated/scipy.stats.rv_continuous.moment.html"]}
{"python_version": "3.10", "library": "scipy", "version": "1.9.2", "problem": "\nComplete the compute_moment function that computes the n-th moment of a distribution dist.", "starting_code": "\nfrom scipy.stats import rv_continuous\ndef compute_moment(dist : rv_continuous, n: int) -> float:", "example_id": "120", "test": "\nfrom scipy.stats import norm\nimport numpy as np\ndist = norm(15, 10)\nn=5\noutput = compute_moment(dist, n=n)\nexpect = 6384375.000000001\nassertion_value = np.allclose(output,expect)\nassert assertion_value", "solution": "\n    return dist.moment(n=n)", "type_of_change": "argument or attribute change", "name_of_class_or_func": "rv_continuous.moment", "additional_dependencies": "", "functional": "1", "webdev": "0", "solution_api_call": true, "api_calls": ["dist.moment"], "release_date": "2022-10", "docs": ["https://docs.scipy.org/doc/scipy-1.9.2/reference/generated/scipy.stats.rv_continuous.moment.html"]}
{"python_version": "3.10", "library": "scipy", "version": "1.9.2", "problem": "\nWrite a function that computes the determinant of batched matrices (batched in the first dimention), non specified parameters should use the default value", "starting_code": "from scipy.linalg import det\nimport numpy as np \ndef compute_determinant(A: np.ndarray) -> np.ndarray:\n", "example_id": "121", "test": "\nA = np.array([[[7.81411439e-01, 1.12331105e-02, 9.39679607e-01, 6.42515536e-01,\n         6.95548884e-01],\n        [4.58001321e-01, 7.90049557e-01, 7.41113873e-01, 9.85894466e-03,\n         4.53976568e-01],\n        [2.24634401e-01, 4.26973301e-01, 8.91014236e-01, 8.23895514e-01,\n         9.31046059e-01],\n        [4.45855698e-02, 2.58723740e-02, 3.84879860e-01, 5.52190839e-01,\n         8.30264909e-02],\n        [9.15445530e-01, 8.36618255e-02, 4.39455188e-01, 1.53462390e-01,\n         7.26436401e-01]],\n\n       [[7.28031138e-01, 3.75117422e-01, 5.42310487e-01, 5.63557751e-01,\n         2.57909701e-04],\n        [7.50931873e-01, 6.15538706e-01, 9.05514875e-01, 8.48424138e-01,\n         3.10902901e-01],\n        [1.72613108e-01, 7.06317645e-01, 8.89355115e-01, 8.90701413e-02,\n         9.28579120e-01],\n        [9.80044173e-01, 1.67306937e-01, 1.69314197e-01, 5.09876755e-01,\n         7.74652232e-01],\n        [5.31016205e-01, 6.71446481e-01, 8.29882594e-02, 1.70589945e-01,\n         3.23593050e-01]],\n\n       [[8.38046468e-01, 7.96708042e-03, 9.28662972e-01, 6.75111182e-01,\n         3.33247539e-02],\n        [9.99455965e-01, 9.41123427e-01, 1.06977786e-01, 1.73786082e-01,\n         5.95914076e-01],\n        [5.46093501e-01, 4.99303483e-01, 4.60402210e-01, 4.75650259e-02,\n         4.58097639e-01],\n        [5.15319589e-01, 2.31310779e-01, 1.93670305e-01, 9.76320068e-01,\n         8.78819966e-01],\n        [1.09356481e-01, 2.16028113e-01, 1.67111833e-01, 9.65726787e-01,\n         6.75017369e-01]]])\noutput = compute_determinant(A)\nexpect = np.array([0.06799371, 0.12344128, -0.1554602])\nassertion_value=np.allclose(output, expect)\nassert assertion_value", "solution": "\n    output = np.zeros(A.shape[0])\n    for i in range(A.shape[0]):\n        output[i] = det(A[i])\n    return output", "type_of_change": "argument or attribute change", "name_of_class_or_func": "scipy.linalg.det", "additional_dependencies": "", "functional": "1", "webdev": "0", "solution_api_call": true, "api_calls": ["range", "numpy.zeros", "scipy.linalg.det"], "release_date": "2022-10", "docs": ["https://docs.scipy.org/doc/scipy-1.9.2/reference/generated/scipy.linalg.det.html"]}
{"python_version": "3.10", "library": "scipy", "version": "1.11.2", "problem": "\nWrite a function that computes the determinant of batched matrices (batched in the first dimention), non specified parameters should use the default value", "starting_code": "from scipy.linalg import det\nimport numpy as np \ndef compute_determinant(A: np.ndarray) -> np.ndarray:\n", "example_id": "122", "test": "\nA = np.array([[[7.81411439e-01, 1.12331105e-02, 9.39679607e-01, 6.42515536e-01,\n         6.95548884e-01],\n        [4.58001321e-01, 7.90049557e-01, 7.41113873e-01, 9.85894466e-03,\n         4.53976568e-01],\n        [2.24634401e-01, 4.26973301e-01, 8.91014236e-01, 8.23895514e-01,\n         9.31046059e-01],\n        [4.45855698e-02, 2.58723740e-02, 3.84879860e-01, 5.52190839e-01,\n         8.30264909e-02],\n        [9.15445530e-01, 8.36618255e-02, 4.39455188e-01, 1.53462390e-01,\n         7.26436401e-01]],\n\n       [[7.28031138e-01, 3.75117422e-01, 5.42310487e-01, 5.63557751e-01,\n         2.57909701e-04],\n        [7.50931873e-01, 6.15538706e-01, 9.05514875e-01, 8.48424138e-01,\n         3.10902901e-01],\n        [1.72613108e-01, 7.06317645e-01, 8.89355115e-01, 8.90701413e-02,\n         9.28579120e-01],\n        [9.80044173e-01, 1.67306937e-01, 1.69314197e-01, 5.09876755e-01,\n         7.74652232e-01],\n        [5.31016205e-01, 6.71446481e-01, 8.29882594e-02, 1.70589945e-01,\n         3.23593050e-01]],\n\n       [[8.38046468e-01, 7.96708042e-03, 9.28662972e-01, 6.75111182e-01,\n         3.33247539e-02],\n        [9.99455965e-01, 9.41123427e-01, 1.06977786e-01, 1.73786082e-01,\n         5.95914076e-01],\n        [5.46093501e-01, 4.99303483e-01, 4.60402210e-01, 4.75650259e-02,\n         4.58097639e-01],\n        [5.15319589e-01, 2.31310779e-01, 1.93670305e-01, 9.76320068e-01,\n         8.78819966e-01],\n        [1.09356481e-01, 2.16028113e-01, 1.67111833e-01, 9.65726787e-01,\n         6.75017369e-01]]])\noutput = compute_determinant(A)\nexpect = np.array([0.06799371, 0.12344128, -0.1554602])\nassertion_value=np.allclose(output, expect)\nassert assertion_value", "solution": "\n    return det(A)", "type_of_change": "argument or attribute change", "name_of_class_or_func": "scipy.linalg.det", "additional_dependencies": "", "functional": "1", "webdev": "0", "solution_api_call": true, "api_calls": ["scipy.linalg.det"], "release_date": "2023-08", "docs": ["https://docs.scipy.org/doc/scipy-1.11.0/reference/generated/scipy.linalg.det.html"]}
{"python_version": "3.10", "library": "scipy", "version": "1.9.2", "problem": "\nComplete the compute_lu_decomposition function that computes the lu decomposition of batched square matrices (batched in the first dimention), non specified parameters should use the default value", "starting_code": "from scipy.linalg import lu\nimport numpy as np \ndef compute_lu_decomposition(A: np.ndarray) -> tuple[np.ndarray,np.ndarray,np.ndarray]:\n    # return p, l, u", "example_id": "123", "test": "\nA = np.array([[[7.81411439e-01, 1.12331105e-02, 9.39679607e-01, 6.42515536e-01,\n         6.95548884e-01],\n        [4.58001321e-01, 7.90049557e-01, 7.41113873e-01, 9.85894466e-03,\n         4.53976568e-01],\n        [2.24634401e-01, 4.26973301e-01, 8.91014236e-01, 8.23895514e-01,\n         9.31046059e-01],\n        [4.45855698e-02, 2.58723740e-02, 3.84879860e-01, 5.52190839e-01,\n         8.30264909e-02],\n        [9.15445530e-01, 8.36618255e-02, 4.39455188e-01, 1.53462390e-01,\n         7.26436401e-01]],\n\n       [[7.28031138e-01, 3.75117422e-01, 5.42310487e-01, 5.63557751e-01,\n         2.57909701e-04],\n        [7.50931873e-01, 6.15538706e-01, 9.05514875e-01, 8.48424138e-01,\n         3.10902901e-01],\n        [1.72613108e-01, 7.06317645e-01, 8.89355115e-01, 8.90701413e-02,\n         9.28579120e-01],\n        [9.80044173e-01, 1.67306937e-01, 1.69314197e-01, 5.09876755e-01,\n         7.74652232e-01],\n        [5.31016205e-01, 6.71446481e-01, 8.29882594e-02, 1.70589945e-01,\n         3.23593050e-01]],\n\n       [[8.38046468e-01, 7.96708042e-03, 9.28662972e-01, 6.75111182e-01,\n         3.33247539e-02],\n        [9.99455965e-01, 9.41123427e-01, 1.06977786e-01, 1.73786082e-01,\n         5.95914076e-01],\n        [5.46093501e-01, 4.99303483e-01, 4.60402210e-01, 4.75650259e-02,\n         4.58097639e-01],\n        [5.15319589e-01, 2.31310779e-01, 1.93670305e-01, 9.76320068e-01,\n         8.78819966e-01],\n        [1.09356481e-01, 2.16028113e-01, 1.67111833e-01, 9.65726787e-01,\n         6.75017369e-01]]])\np,l,u = compute_lu_decomposition(A)\nassertion_value = np.allclose(np.stack([p,l,u],axis=1) ,np.stack([lu(A[i]) for i in range(A.shape[0])],axis=0))\nassert assertion_value", "solution": "\n    p,l,u = [np.zeros(A.shape) for i in range(3)]\n    for i in range(A.shape[0]):\n        p[i],l[i],u[i] = lu(A[i])\n    return p, l, u", "type_of_change": "argument or attribute change", "name_of_class_or_func": "scipy.linalg.lu", "additional_dependencies": "", "functional": "1", "webdev": "0", "solution_api_call": true, "api_calls": ["range", "scipy.linalg.lu", "numpy.zeros"], "release_date": "2022-10", "docs": ["https://docs.scipy.org/doc/scipy-1.9.2/reference/generated/scipy.linalg.lu.html"]}
{"python_version": "3.10", "library": "scipy", "version": "1.11.2", "problem": "\nComplete the compute_lu_decomposition function that computes the lu decomposition of batched square matrices (batched in the first dimention), non specified parameters should use the default value", "starting_code": "from scipy.linalg import lu\nimport numpy as np \ndef compute_lu_decomposition(A: np.ndarray) -> tuple[np.ndarray,np.ndarray,np.ndarray]:\n    # return p, l, u", "example_id": "124", "test": "\nA = np.array([[[7.81411439e-01, 1.12331105e-02, 9.39679607e-01, 6.42515536e-01,\n         6.95548884e-01],\n        [4.58001321e-01, 7.90049557e-01, 7.41113873e-01, 9.85894466e-03,\n         4.53976568e-01],\n        [2.24634401e-01, 4.26973301e-01, 8.91014236e-01, 8.23895514e-01,\n         9.31046059e-01],\n        [4.45855698e-02, 2.58723740e-02, 3.84879860e-01, 5.52190839e-01,\n         8.30264909e-02],\n        [9.15445530e-01, 8.36618255e-02, 4.39455188e-01, 1.53462390e-01,\n         7.26436401e-01]],\n\n       [[7.28031138e-01, 3.75117422e-01, 5.42310487e-01, 5.63557751e-01,\n         2.57909701e-04],\n        [7.50931873e-01, 6.15538706e-01, 9.05514875e-01, 8.48424138e-01,\n         3.10902901e-01],\n        [1.72613108e-01, 7.06317645e-01, 8.89355115e-01, 8.90701413e-02,\n         9.28579120e-01],\n        [9.80044173e-01, 1.67306937e-01, 1.69314197e-01, 5.09876755e-01,\n         7.74652232e-01],\n        [5.31016205e-01, 6.71446481e-01, 8.29882594e-02, 1.70589945e-01,\n         3.23593050e-01]],\n\n       [[8.38046468e-01, 7.96708042e-03, 9.28662972e-01, 6.75111182e-01,\n         3.33247539e-02],\n        [9.99455965e-01, 9.41123427e-01, 1.06977786e-01, 1.73786082e-01,\n         5.95914076e-01],\n        [5.46093501e-01, 4.99303483e-01, 4.60402210e-01, 4.75650259e-02,\n         4.58097639e-01],\n        [5.15319589e-01, 2.31310779e-01, 1.93670305e-01, 9.76320068e-01,\n         8.78819966e-01],\n        [1.09356481e-01, 2.16028113e-01, 1.67111833e-01, 9.65726787e-01,\n         6.75017369e-01]]])\np,l,u = compute_lu_decomposition(A)\nassertion_value = np.allclose(np.stack([p,l,u],axis=1) ,np.stack([lu(A[i]) for i in range(A.shape[0])],axis=0))\nassert assertion_value", "solution": "\n    return lu(A)", "type_of_change": "argument or attribute change", "name_of_class_or_func": "scipy.linalg.lu", "additional_dependencies": "", "functional": "1", "webdev": "0", "solution_api_call": true, "api_calls": ["scipy.linalg.lu"], "release_date": "2023-08", "docs": ["https://docs.scipy.org/doc/scipy-1.11.1/reference/generated/scipy.linalg.lu.html"]}
{"python_version": "3.10", "library": "scipy", "version": "1.11.2", "problem": "\nComplete the function that returns a lanczos windows, non specified parameters should use the default value", "starting_code": "import scipy.signal.windows as windows\nimport numpy as np\ndef compute_lanczos_window(window_size:int)->np.ndarray:", "example_id": "125", "test": "\nwindow_size=31\nwindow = compute_lanczos_window(window_size)\nexpect = np.array([\n    3.89817183e-17, 7.09075143e-02, 1.49386494e-01, 2.33872321e-01,\n    3.22568652e-01, 4.13496672e-01, 5.04551152e-01, 5.93561534e-01,\n    6.78356039e-01, 7.56826729e-01, 8.26993343e-01, 8.87063793e-01,\n    9.35489284e-01, 9.71012209e-01, 9.92705200e-01, 1.00000000e+00,\n    9.92705200e-01, 9.71012209e-01, 9.35489284e-01, 8.87063793e-01,\n    8.26993343e-01, 7.56826729e-01, 6.78356039e-01, 5.93561534e-01,\n    5.04551152e-01, 4.13496672e-01, 3.22568652e-01, 2.33872321e-01,\n    1.49386494e-01, 7.09075143e-02, 3.89817183e-17\n])\nassertion_value = np.allclose(window,expect)\nassert assertion_value", "solution": "\n    return windows.lanczos(window_size)", "type_of_change": "other library or new feature", "name_of_class_or_func": "signal.windows.lanczos", "additional_dependencies": "", "functional": "1", "webdev": "0", "solution_api_call": true, "api_calls": ["scipy.signal.windows.lanczos"], "release_date": "2023-08", "docs": ["https://docs.scipy.org/doc/scipy-1.11.1/reference/generated/scipy.signal.windows.lanczos.html"]}
{"python_version": "3.10", "library": "scipy", "version": "1.9.2", "problem": "\nComplete the function that returns a lanczos windows, non specified parameters should use the default value", "starting_code": "import scipy.signal.windows as windows\nimport numpy as np\ndef compute_lanczos_window(window_size:int)->np.ndarray:", "example_id": "126", "test": "\nwindow_size=31\nwindow = compute_lanczos_window(window_size)\nexpect = np.array([\n    3.89817183e-17, 7.09075143e-02, 1.49386494e-01, 2.33872321e-01,\n    3.22568652e-01, 4.13496672e-01, 5.04551152e-01, 5.93561534e-01,\n    6.78356039e-01, 7.56826729e-01, 8.26993343e-01, 8.87063793e-01,\n    9.35489284e-01, 9.71012209e-01, 9.92705200e-01, 1.00000000e+00,\n    9.92705200e-01, 9.71012209e-01, 9.35489284e-01, 8.87063793e-01,\n    8.26993343e-01, 7.56826729e-01, 6.78356039e-01, 5.93561534e-01,\n    5.04551152e-01, 4.13496672e-01, 3.22568652e-01, 2.33872321e-01,\n    1.49386494e-01, 7.09075143e-02, 3.89817183e-17\n])\nassertion_value = np.allclose(window,expect)\nassert assertion_value", "solution": "\n    window = 2*np.arange(window_size)/(window_size-1) - 1 \n    window = np.sinc(window)\n    window = window / np.max(window)\n    return window ", "type_of_change": "other library or new feature", "name_of_class_or_func": "signal.windows.lanczos", "additional_dependencies": "", "functional": "1", "webdev": "0", "solution_api_call": true, "api_calls": ["numpy.sinc", "numpy.max", "numpy.arange"], "release_date": "2022-10", "docs": ["https://numpy.org/doc/1.20/reference/generated/numpy.sinc.html"]}
{"python_version": "3.10", "library": "scipy", "version": "1.10.1", "problem": "\nComplete the function that apply a 1D gaussian filter, non specified parameters should use the default value", "starting_code": "from scipy.ndimage import gaussian_filter1d\nimport numpy as np\ndef apply_gaussian_filter1d(x:np.ndarray, radius:int, sigma:float)->np.ndarray:", "example_id": "127", "test": "\nnp.random.seed(42)\nx = np.random.rand(100)\nradius = 10\nsigma= np.pi\noutput = apply_gaussian_filter1d(x, radius=radius, sigma=sigma)\nexpect = np.array([\n    0.56815745, 0.55427942, 0.53042419, 0.50274436, 0.47939431, 0.46608408,\n    0.46527533, 0.4744513,  0.48749056, 0.49688273, 0.4972386,  0.48550617,\n    0.46339406, 0.43606723, 0.40952566, 0.38910972, 0.37656404, 0.37165055,\n    0.37174331, 0.37468009, 0.37962381, 0.38653222, 0.39495139, 0.40508619,\n    0.41475306, 0.42135263, 0.423637,   0.42273663, 0.42245913, 0.42707435,\n    0.43961646, 0.46012253, 0.48430625, 0.50544533, 0.51606669, 0.5127542,\n    0.49612029, 0.47181487, 0.44642797, 0.42617826, 0.41556205, 0.41473155,\n    0.42283456, 0.43706944, 0.4548513,  0.476166,   0.50121046, 0.53152392,\n    0.56672268, 0.60382116, 0.63693437, 0.65767482, 0.65869746, 0.63622787,\n    0.59180857, 0.53395824, 0.47319519, 0.42179978, 0.38691321, 0.37179661,\n    0.37333425, 0.38479753, 0.40035159, 0.41614361, 0.43076203, 0.44494718,\n    0.45937374, 0.47391018, 0.48731474, 0.49840656, 0.50731306, 0.51444675,\n    0.52002105, 0.52266915, 0.52084841, 0.51247425, 0.49789513, 0.47962127,\n    0.4608554,  0.44470093, 0.4319703,  0.42466571, 0.42407116, 0.43154786,\n    0.44773618, 0.47128269, 0.49819701, 0.52370836, 0.54433314, 0.55848065,\n    0.56652679, 0.56791146, 0.56167112, 0.54485643, 0.51513067, 0.47291521,\n    0.42246471, 0.37170576, 0.3304751,  0.30701553\n])\nassertion_value = np.allclose(output,output)\nassert assertion_value", "solution": "\n    return gaussian_filter1d(x, radius=radius, sigma=sigma)", "type_of_change": "argument or attribute change", "name_of_class_or_func": "ndimage.gaussian_filter1d", "additional_dependencies": "", "functional": "1", "webdev": "0", "solution_api_call": true, "api_calls": ["scipy.ndimage.gaussian_filter1d"], "release_date": "2023-02", "docs": ["https://docs.scipy.org/doc/scipy-1.10.1/reference/ndimage.html"]}
{"python_version": "3.10", "library": "scipy", "version": "1.9.2", "problem": "\nComplete the function that apply a 1D gaussian filter, non specified parameters should use the default value", "starting_code": "from scipy.ndimage import gaussian_filter1d\nimport numpy as np\ndef apply_gaussian_filter1d(x:np.ndarray, radius:int, sigma:float)->np.ndarray:", "example_id": "128", "test": "\nnp.random.seed(42)\nx = np.random.rand(100)\nradius = 10\nsigma= np.pi\noutput = apply_gaussian_filter1d(x, radius=radius, sigma=sigma)\nexpect = np.array([\n    0.56815745, 0.55427942, 0.53042419, 0.50274436, 0.47939431, 0.46608408,\n    0.46527533, 0.4744513,  0.48749056, 0.49688273, 0.4972386,  0.48550617,\n    0.46339406, 0.43606723, 0.40952566, 0.38910972, 0.37656404, 0.37165055,\n    0.37174331, 0.37468009, 0.37962381, 0.38653222, 0.39495139, 0.40508619,\n    0.41475306, 0.42135263, 0.423637,   0.42273663, 0.42245913, 0.42707435,\n    0.43961646, 0.46012253, 0.48430625, 0.50544533, 0.51606669, 0.5127542,\n    0.49612029, 0.47181487, 0.44642797, 0.42617826, 0.41556205, 0.41473155,\n    0.42283456, 0.43706944, 0.4548513,  0.476166,   0.50121046, 0.53152392,\n    0.56672268, 0.60382116, 0.63693437, 0.65767482, 0.65869746, 0.63622787,\n    0.59180857, 0.53395824, 0.47319519, 0.42179978, 0.38691321, 0.37179661,\n    0.37333425, 0.38479753, 0.40035159, 0.41614361, 0.43076203, 0.44494718,\n    0.45937374, 0.47391018, 0.48731474, 0.49840656, 0.50731306, 0.51444675,\n    0.52002105, 0.52266915, 0.52084841, 0.51247425, 0.49789513, 0.47962127,\n    0.4608554,  0.44470093, 0.4319703,  0.42466571, 0.42407116, 0.43154786,\n    0.44773618, 0.47128269, 0.49819701, 0.52370836, 0.54433314, 0.55848065,\n    0.56652679, 0.56791146, 0.56167112, 0.54485643, 0.51513067, 0.47291521,\n    0.42246471, 0.37170576, 0.3304751,  0.30701553\n])\nassertion_value = np.allclose(output,output)\nassert assertion_value", "solution": "\n    return gaussian_filter1d(x, truncate = radius/sigma,sigma=sigma)", "type_of_change": "argument or attribute change", "name_of_class_or_func": "ndimage.gaussian_filter1d", "additional_dependencies": "", "functional": "1", "webdev": "0", "solution_api_call": true, "api_calls": ["scipy.ndimage.gaussian_filter1d"], "release_date": "2022-10", "docs": ["https://docs.scipy.org/doc/scipy-1.9.2/reference/ndimage.html"]}
{"python_version": "3.10", "library": "scipy", "version": "1.11.2", "problem": "\nComplete the function that applies a rank filter on batched images (batched in the first dimention), non specified parameters should use the default value", "starting_code": "from scipy.ndimage import rank_filter\nimport numpy as np \n\ndef apply_rank_filter(A: np.ndarray,rank: int,size:int)->np.ndarray:", "example_id": "129", "test": "\nA = np.array([[[7.81411439e-01, 1.12331105e-02, 9.39679607e-01, 6.42515536e-01,\n         6.95548884e-01],\n        [4.58001321e-01, 7.90049557e-01, 7.41113873e-01, 9.85894466e-03,\n         4.53976568e-01],\n        [2.24634401e-01, 4.26973301e-01, 8.91014236e-01, 8.23895514e-01,\n         9.31046059e-01],\n        [4.45855698e-02, 2.58723740e-02, 3.84879860e-01, 5.52190839e-01,\n         8.30264909e-02],\n        [9.15445530e-01, 8.36618255e-02, 4.39455188e-01, 1.53462390e-01,\n         7.26436401e-01]],\n\n       [[7.28031138e-01, 3.75117422e-01, 5.42310487e-01, 5.63557751e-01,\n         2.57909701e-04],\n        [7.50931873e-01, 6.15538706e-01, 9.05514875e-01, 8.48424138e-01,\n         3.10902901e-01],\n        [1.72613108e-01, 7.06317645e-01, 8.89355115e-01, 8.90701413e-02,\n         9.28579120e-01],\n        [9.80044173e-01, 1.67306937e-01, 1.69314197e-01, 5.09876755e-01,\n         7.74652232e-01],\n        [5.31016205e-01, 6.71446481e-01, 8.29882594e-02, 1.70589945e-01,\n         3.23593050e-01]],\n\n       [[8.38046468e-01, 7.96708042e-03, 9.28662972e-01, 6.75111182e-01,\n         3.33247539e-02],\n        [9.99455965e-01, 9.41123427e-01, 1.06977786e-01, 1.73786082e-01,\n         5.95914076e-01],\n        [5.46093501e-01, 4.99303483e-01, 4.60402210e-01, 4.75650259e-02,\n         4.58097639e-01],\n        [5.15319589e-01, 2.31310779e-01, 1.93670305e-01, 9.76320068e-01,\n         8.78819966e-01],\n        [1.09356481e-01, 2.16028113e-01, 1.67111833e-01, 9.65726787e-01,\n         6.75017369e-01]]])\nrank = 6\nsize=3\noutput = apply_rank_filter(A,rank,size=size)\nexpect = np.array([\n    [\n        [0.78141144, 0.79004956, 0.79004956, 0.74111387, 0.69554888],\n        [0.78141144, 0.79004956, 0.82389551, 0.89101424, 0.82389551],\n        [0.45800132, 0.74111387, 0.79004956, 0.82389551, 0.82389551],\n        [0.4269733,  0.43945519, 0.55219084, 0.82389551, 0.82389551],\n        [0.91544553, 0.43945519, 0.43945519, 0.55219084, 0.7264364 ]\n    ],\n    [\n        [0.72803114, 0.72803114, 0.61553871, 0.56355775, 0.56355775],\n        [0.72803114, 0.75093187, 0.84842414, 0.88935511, 0.84842414],\n        [0.75093187, 0.88935511, 0.84842414, 0.88935511, 0.84842414],\n        [0.70631765, 0.70631765, 0.67144648, 0.77465223, 0.77465223],\n        [0.67144648, 0.67144648, 0.50987675, 0.32359305, 0.50987675]\n    ],\n    [\n        [0.94112343, 0.92866297, 0.92866297, 0.67511118, 0.59591408],\n        [0.94112343, 0.92866297, 0.67511118, 0.59591408, 0.59591408],\n        [0.94112343, 0.5460935,  0.49930348, 0.59591408, 0.87881997],\n        [0.51531959, 0.49930348, 0.49930348, 0.87881997, 0.87881997],\n        [0.23131078, 0.21602811, 0.96572679, 0.96572679, 0.96572679]\n    ]\n])\nassertion_value = np.allclose(output, expect)\nassert assertion_value", "solution": "\n    return rank_filter(A,rank,size=size,axes=[1,2])", "type_of_change": "argument or attribute change", "name_of_class_or_func": "scipy.ndimage.rank_filter", "additional_dependencies": "", "functional": "1", "webdev": "0", "solution_api_call": true, "api_calls": ["scipy.ndimage.rank_filter"], "release_date": "2023-08", "docs": ["https://docs.scipy.org/doc/scipy-1.11.1/reference/ndimage.html"]}
{"python_version": "3.10", "library": "scipy", "version": "1.9.2", "problem": "\nComplete the function that applies a rank filter on batched images (batched in the first dimention), non specified parameters should use the default value", "starting_code": "from scipy.ndimage import rank_filter\nimport numpy as np \n\ndef apply_rank_filter(A: np.ndarray,rank: int,size:int)->np.ndarray:", "example_id": "130", "test": "\nA = np.array([[[7.81411439e-01, 1.12331105e-02, 9.39679607e-01, 6.42515536e-01,\n         6.95548884e-01],\n        [4.58001321e-01, 7.90049557e-01, 7.41113873e-01, 9.85894466e-03,\n         4.53976568e-01],\n        [2.24634401e-01, 4.26973301e-01, 8.91014236e-01, 8.23895514e-01,\n         9.31046059e-01],\n        [4.45855698e-02, 2.58723740e-02, 3.84879860e-01, 5.52190839e-01,\n         8.30264909e-02],\n        [9.15445530e-01, 8.36618255e-02, 4.39455188e-01, 1.53462390e-01,\n         7.26436401e-01]],\n\n       [[7.28031138e-01, 3.75117422e-01, 5.42310487e-01, 5.63557751e-01,\n         2.57909701e-04],\n        [7.50931873e-01, 6.15538706e-01, 9.05514875e-01, 8.48424138e-01,\n         3.10902901e-01],\n        [1.72613108e-01, 7.06317645e-01, 8.89355115e-01, 8.90701413e-02,\n         9.28579120e-01],\n        [9.80044173e-01, 1.67306937e-01, 1.69314197e-01, 5.09876755e-01,\n         7.74652232e-01],\n        [5.31016205e-01, 6.71446481e-01, 8.29882594e-02, 1.70589945e-01,\n         3.23593050e-01]],\n\n       [[8.38046468e-01, 7.96708042e-03, 9.28662972e-01, 6.75111182e-01,\n         3.33247539e-02],\n        [9.99455965e-01, 9.41123427e-01, 1.06977786e-01, 1.73786082e-01,\n         5.95914076e-01],\n        [5.46093501e-01, 4.99303483e-01, 4.60402210e-01, 4.75650259e-02,\n         4.58097639e-01],\n        [5.15319589e-01, 2.31310779e-01, 1.93670305e-01, 9.76320068e-01,\n         8.78819966e-01],\n        [1.09356481e-01, 2.16028113e-01, 1.67111833e-01, 9.65726787e-01,\n         6.75017369e-01]]])\nrank = 6\nsize=3\noutput = apply_rank_filter(A,rank,size=size)\nexpect = np.array([\n    [\n        [0.78141144, 0.79004956, 0.79004956, 0.74111387, 0.69554888],\n        [0.78141144, 0.79004956, 0.82389551, 0.89101424, 0.82389551],\n        [0.45800132, 0.74111387, 0.79004956, 0.82389551, 0.82389551],\n        [0.4269733,  0.43945519, 0.55219084, 0.82389551, 0.82389551],\n        [0.91544553, 0.43945519, 0.43945519, 0.55219084, 0.7264364 ]\n    ],\n    [\n        [0.72803114, 0.72803114, 0.61553871, 0.56355775, 0.56355775],\n        [0.72803114, 0.75093187, 0.84842414, 0.88935511, 0.84842414],\n        [0.75093187, 0.88935511, 0.84842414, 0.88935511, 0.84842414],\n        [0.70631765, 0.70631765, 0.67144648, 0.77465223, 0.77465223],\n        [0.67144648, 0.67144648, 0.50987675, 0.32359305, 0.50987675]\n    ],\n    [\n        [0.94112343, 0.92866297, 0.92866297, 0.67511118, 0.59591408],\n        [0.94112343, 0.92866297, 0.67511118, 0.59591408, 0.59591408],\n        [0.94112343, 0.5460935,  0.49930348, 0.59591408, 0.87881997],\n        [0.51531959, 0.49930348, 0.49930348, 0.87881997, 0.87881997],\n        [0.23131078, 0.21602811, 0.96572679, 0.96572679, 0.96572679]\n    ]\n])\nassertion_value = np.allclose(output, expect)\nassert assertion_value", "solution": "\n    output = np.zeros(A.shape)\n    for i in range(A.shape[0]):\n        output[i] = rank_filter(A[i],rank,size=size)\n    return output", "type_of_change": "argument or attribute change", "name_of_class_or_func": "scipy.ndimage.rank_filter", "additional_dependencies": "", "functional": "1", "webdev": "0", "solution_api_call": true, "api_calls": ["range", "numpy.zeros", "scipy.ndimage.rank_filter"], "release_date": "2022-10", "docs": ["https://docs.scipy.org/doc/scipy-1.9.2/reference/ndimage.html"]}
{"python_version": "3.10", "library": "scipy", "version": "1.11.2", "problem": "\nComplete the function that applies a percentile filter on batched images (batched in the first dimention), non specified parameters should use the default value", "starting_code": "from scipy.ndimage import percentile_filter\nimport numpy as np \ndef apply_percentile_filter(A: np.ndarray, percentile: int | float,size:int)->np.ndarray:", "example_id": "131", "test": "\nA = np.array([[[7.81411439e-01, 1.12331105e-02, 9.39679607e-01, 6.42515536e-01,\n         6.95548884e-01],\n        [4.58001321e-01, 7.90049557e-01, 7.41113873e-01, 9.85894466e-03,\n         4.53976568e-01],\n        [2.24634401e-01, 4.26973301e-01, 8.91014236e-01, 8.23895514e-01,\n         9.31046059e-01],\n        [4.45855698e-02, 2.58723740e-02, 3.84879860e-01, 5.52190839e-01,\n         8.30264909e-02],\n        [9.15445530e-01, 8.36618255e-02, 4.39455188e-01, 1.53462390e-01,\n         7.26436401e-01]],\n\n       [[7.28031138e-01, 3.75117422e-01, 5.42310487e-01, 5.63557751e-01,\n         2.57909701e-04],\n        [7.50931873e-01, 6.15538706e-01, 9.05514875e-01, 8.48424138e-01,\n         3.10902901e-01],\n        [1.72613108e-01, 7.06317645e-01, 8.89355115e-01, 8.90701413e-02,\n         9.28579120e-01],\n        [9.80044173e-01, 1.67306937e-01, 1.69314197e-01, 5.09876755e-01,\n         7.74652232e-01],\n        [5.31016205e-01, 6.71446481e-01, 8.29882594e-02, 1.70589945e-01,\n         3.23593050e-01]],\n\n       [[8.38046468e-01, 7.96708042e-03, 9.28662972e-01, 6.75111182e-01,\n         3.33247539e-02],\n        [9.99455965e-01, 9.41123427e-01, 1.06977786e-01, 1.73786082e-01,\n         5.95914076e-01],\n        [5.46093501e-01, 4.99303483e-01, 4.60402210e-01, 4.75650259e-02,\n         4.58097639e-01],\n        [5.15319589e-01, 2.31310779e-01, 1.93670305e-01, 9.76320068e-01,\n         8.78819966e-01],\n        [1.09356481e-01, 2.16028113e-01, 1.67111833e-01, 9.65726787e-01,\n         6.75017369e-01]]])\npercentile = 90\nsize=3\noutput = apply_percentile_filter(A,percentile,size=size)\nexpect = np.array([\n    [\n        [0.79004956, 0.93967961, 0.93967961, 0.93967961, 0.69554888],\n        [0.79004956, 0.93967961, 0.93967961, 0.93967961, 0.93104606],\n        [0.79004956, 0.89101424, 0.89101424, 0.93104606, 0.93104606],\n        [0.91544553, 0.91544553, 0.89101424, 0.93104606, 0.93104606],\n        [0.91544553, 0.91544553, 0.55219084, 0.7264364,  0.7264364 ]\n    ],\n    [\n        [0.75093187, 0.90551488, 0.90551488, 0.90551488, 0.84842414],\n        [0.75093187, 0.90551488, 0.90551488, 0.92857912, 0.92857912],\n        [0.98004417, 0.98004417, 0.90551488, 0.92857912, 0.92857912],\n        [0.98004417, 0.98004417, 0.88935511, 0.92857912, 0.92857912],\n        [0.98004417, 0.98004417, 0.67144648, 0.77465223, 0.77465223]\n    ],\n    [\n        [0.99945597, 0.99945597, 0.94112343, 0.92866297, 0.67511118],\n        [0.99945597, 0.99945597, 0.94112343, 0.92866297, 0.67511118],\n        [0.99945597, 0.99945597, 0.97632007, 0.97632007, 0.97632007],\n        [0.5460935,  0.5460935,  0.97632007, 0.97632007, 0.97632007],\n        [0.51531959, 0.51531959, 0.97632007, 0.97632007, 0.97632007]\n    ]\n])\nassertion_value = np.allclose(output, expect)\nassert assertion_value", "solution": "\n    return percentile_filter(A,percentile=percentile,size=size,axes=[1,2])", "type_of_change": "argument or attribute change", "name_of_class_or_func": "scipy.ndimage.percentile_filter", "additional_dependencies": "", "functional": "1", "webdev": "0", "solution_api_call": true, "api_calls": ["scipy.ndimage.percentile_filter"], "release_date": "2023-08", "docs": ["https://docs.scipy.org/doc/scipy-1.11.1/reference/ndimage.html"]}
{"python_version": "3.10", "library": "scipy", "version": "1.9.2", "problem": "\nComplete the function that applies a percentile filter on batched images (batched in the first dimention), non specified parameters should use the default value", "starting_code": "from scipy.ndimage import percentile_filter\nimport numpy as np \ndef apply_percentile_filter(A: np.ndarray, percentile: int | float,size:int)->np.ndarray:", "example_id": "132", "test": "\nA = np.array([[[7.81411439e-01, 1.12331105e-02, 9.39679607e-01, 6.42515536e-01,\n         6.95548884e-01],\n        [4.58001321e-01, 7.90049557e-01, 7.41113873e-01, 9.85894466e-03,\n         4.53976568e-01],\n        [2.24634401e-01, 4.26973301e-01, 8.91014236e-01, 8.23895514e-01,\n         9.31046059e-01],\n        [4.45855698e-02, 2.58723740e-02, 3.84879860e-01, 5.52190839e-01,\n         8.30264909e-02],\n        [9.15445530e-01, 8.36618255e-02, 4.39455188e-01, 1.53462390e-01,\n         7.26436401e-01]],\n\n       [[7.28031138e-01, 3.75117422e-01, 5.42310487e-01, 5.63557751e-01,\n         2.57909701e-04],\n        [7.50931873e-01, 6.15538706e-01, 9.05514875e-01, 8.48424138e-01,\n         3.10902901e-01],\n        [1.72613108e-01, 7.06317645e-01, 8.89355115e-01, 8.90701413e-02,\n         9.28579120e-01],\n        [9.80044173e-01, 1.67306937e-01, 1.69314197e-01, 5.09876755e-01,\n         7.74652232e-01],\n        [5.31016205e-01, 6.71446481e-01, 8.29882594e-02, 1.70589945e-01,\n         3.23593050e-01]],\n\n       [[8.38046468e-01, 7.96708042e-03, 9.28662972e-01, 6.75111182e-01,\n         3.33247539e-02],\n        [9.99455965e-01, 9.41123427e-01, 1.06977786e-01, 1.73786082e-01,\n         5.95914076e-01],\n        [5.46093501e-01, 4.99303483e-01, 4.60402210e-01, 4.75650259e-02,\n         4.58097639e-01],\n        [5.15319589e-01, 2.31310779e-01, 1.93670305e-01, 9.76320068e-01,\n         8.78819966e-01],\n        [1.09356481e-01, 2.16028113e-01, 1.67111833e-01, 9.65726787e-01,\n         6.75017369e-01]]])\npercentile = 90\nsize=3\noutput = apply_percentile_filter(A,percentile,size=size)\nexpect = np.array([\n    [\n        [0.79004956, 0.93967961, 0.93967961, 0.93967961, 0.69554888],\n        [0.79004956, 0.93967961, 0.93967961, 0.93967961, 0.93104606],\n        [0.79004956, 0.89101424, 0.89101424, 0.93104606, 0.93104606],\n        [0.91544553, 0.91544553, 0.89101424, 0.93104606, 0.93104606],\n        [0.91544553, 0.91544553, 0.55219084, 0.7264364,  0.7264364 ]\n    ],\n    [\n        [0.75093187, 0.90551488, 0.90551488, 0.90551488, 0.84842414],\n        [0.75093187, 0.90551488, 0.90551488, 0.92857912, 0.92857912],\n        [0.98004417, 0.98004417, 0.90551488, 0.92857912, 0.92857912],\n        [0.98004417, 0.98004417, 0.88935511, 0.92857912, 0.92857912],\n        [0.98004417, 0.98004417, 0.67144648, 0.77465223, 0.77465223]\n    ],\n    [\n        [0.99945597, 0.99945597, 0.94112343, 0.92866297, 0.67511118],\n        [0.99945597, 0.99945597, 0.94112343, 0.92866297, 0.67511118],\n        [0.99945597, 0.99945597, 0.97632007, 0.97632007, 0.97632007],\n        [0.5460935,  0.5460935,  0.97632007, 0.97632007, 0.97632007],\n        [0.51531959, 0.51531959, 0.97632007, 0.97632007, 0.97632007]\n    ]\n])\nassertion_value = np.allclose(output, expect)\nassert assertion_value", "solution": "\n    output = np.zeros(A.shape)\n    for i in range(A.shape[0]):\n        output[i] = percentile_filter(A[i],percentile=percentile,size=size)\n    return output ", "type_of_change": "argument or attribute change", "name_of_class_or_func": "scipy.ndimage.percentile_filter", "additional_dependencies": "", "functional": "1", "webdev": "0", "solution_api_call": true, "api_calls": ["range", "numpy.zeros", "scipy.ndimage.percentile_filter"], "release_date": "2022-10", "docs": ["https://docs.scipy.org/doc/scipy-1.9.2/reference/ndimage.html"]}
{"python_version": "3.10", "library": "scipy", "version": "1.11.2", "problem": "\nComplete the function that applies a median filter on batched images (batched in the first dimention), non specified parameters should use the default value", "starting_code": "from scipy.ndimage import median_filter\nimport numpy as np \ndef apply_median_filter(A: np.ndarray,size:int) -> np.ndarray:", "example_id": "133", "test": "\nA = np.array([[[7.81411439e-01, 1.12331105e-02, 9.39679607e-01, 6.42515536e-01,\n         6.95548884e-01],\n        [4.58001321e-01, 7.90049557e-01, 7.41113873e-01, 9.85894466e-03,\n         4.53976568e-01],\n        [2.24634401e-01, 4.26973301e-01, 8.91014236e-01, 8.23895514e-01,\n         9.31046059e-01],\n        [4.45855698e-02, 2.58723740e-02, 3.84879860e-01, 5.52190839e-01,\n         8.30264909e-02],\n        [9.15445530e-01, 8.36618255e-02, 4.39455188e-01, 1.53462390e-01,\n         7.26436401e-01]],\n\n       [[7.28031138e-01, 3.75117422e-01, 5.42310487e-01, 5.63557751e-01,\n         2.57909701e-04],\n        [7.50931873e-01, 6.15538706e-01, 9.05514875e-01, 8.48424138e-01,\n         3.10902901e-01],\n        [1.72613108e-01, 7.06317645e-01, 8.89355115e-01, 8.90701413e-02,\n         9.28579120e-01],\n        [9.80044173e-01, 1.67306937e-01, 1.69314197e-01, 5.09876755e-01,\n         7.74652232e-01],\n        [5.31016205e-01, 6.71446481e-01, 8.29882594e-02, 1.70589945e-01,\n         3.23593050e-01]],\n\n       [[8.38046468e-01, 7.96708042e-03, 9.28662972e-01, 6.75111182e-01,\n         3.33247539e-02],\n        [9.99455965e-01, 9.41123427e-01, 1.06977786e-01, 1.73786082e-01,\n         5.95914076e-01],\n        [5.46093501e-01, 4.99303483e-01, 4.60402210e-01, 4.75650259e-02,\n         4.58097639e-01],\n        [5.15319589e-01, 2.31310779e-01, 1.93670305e-01, 9.76320068e-01,\n         8.78819966e-01],\n        [1.09356481e-01, 2.16028113e-01, 1.67111833e-01, 9.65726787e-01,\n         6.75017369e-01]]])\nsize=3\noutput = apply_median_filter(A,size=size)\nassertion_value = np.allclose(output, np.stack([ median_filter(A[i], size=size) for i in range(A.shape[0])],axis=0))\nassert assertion_value", "solution": "\n    return median_filter(A,size=size,axes=[1,2])", "type_of_change": "argument or attribute change", "name_of_class_or_func": "scipy.ndimage.median_filter", "additional_dependencies": "", "functional": "1", "webdev": "0", "solution_api_call": true, "api_calls": ["scipy.ndimage.median_filter"], "release_date": "2023-08", "docs": ["https://docs.scipy.org/doc/scipy-1.11.1/reference/ndimage.html"]}
{"python_version": "3.10", "library": "scipy", "version": "1.9.2", "problem": "\nComplete the function that applies a median filter on batched images (batched in the first dimention), non specified parameters should use the default value", "starting_code": "from scipy.ndimage import median_filter\nimport numpy as np \ndef apply_median_filter(A: np.ndarray, size:int) -> np.ndarray:", "example_id": "134", "test": "\nA = np.array([[[7.81411439e-01, 1.12331105e-02, 9.39679607e-01, 6.42515536e-01,\n         6.95548884e-01],\n        [4.58001321e-01, 7.90049557e-01, 7.41113873e-01, 9.85894466e-03,\n         4.53976568e-01],\n        [2.24634401e-01, 4.26973301e-01, 8.91014236e-01, 8.23895514e-01,\n         9.31046059e-01],\n        [4.45855698e-02, 2.58723740e-02, 3.84879860e-01, 5.52190839e-01,\n         8.30264909e-02],\n        [9.15445530e-01, 8.36618255e-02, 4.39455188e-01, 1.53462390e-01,\n         7.26436401e-01]],\n\n       [[7.28031138e-01, 3.75117422e-01, 5.42310487e-01, 5.63557751e-01,\n         2.57909701e-04],\n        [7.50931873e-01, 6.15538706e-01, 9.05514875e-01, 8.48424138e-01,\n         3.10902901e-01],\n        [1.72613108e-01, 7.06317645e-01, 8.89355115e-01, 8.90701413e-02,\n         9.28579120e-01],\n        [9.80044173e-01, 1.67306937e-01, 1.69314197e-01, 5.09876755e-01,\n         7.74652232e-01],\n        [5.31016205e-01, 6.71446481e-01, 8.29882594e-02, 1.70589945e-01,\n         3.23593050e-01]],\n\n       [[8.38046468e-01, 7.96708042e-03, 9.28662972e-01, 6.75111182e-01,\n         3.33247539e-02],\n        [9.99455965e-01, 9.41123427e-01, 1.06977786e-01, 1.73786082e-01,\n         5.95914076e-01],\n        [5.46093501e-01, 4.99303483e-01, 4.60402210e-01, 4.75650259e-02,\n         4.58097639e-01],\n        [5.15319589e-01, 2.31310779e-01, 1.93670305e-01, 9.76320068e-01,\n         8.78819966e-01],\n        [1.09356481e-01, 2.16028113e-01, 1.67111833e-01, 9.65726787e-01,\n         6.75017369e-01]]])\nsize=3\noutput = apply_median_filter(A,size=size)\nexpect = np.array([\n    [\n        [0.78141144, 0.78141144, 0.64251554, 0.69554888, 0.64251554],\n        [0.45800132, 0.74111387, 0.74111387, 0.74111387, 0.69554888],\n        [0.2246344,  0.4269733,  0.55219084, 0.55219084, 0.45397657],\n        [0.2246344,  0.38487986, 0.4269733,  0.55219084, 0.7264364 ],\n        [0.08366183, 0.38487986, 0.15346239, 0.43945519, 0.55219084]\n    ],\n    [\n        [0.72803114, 0.61553871, 0.56355775, 0.54231049, 0.3109029 ],\n        [0.70631765, 0.70631765, 0.61553871, 0.56355775, 0.3109029 ],\n        [0.70631765, 0.70631765, 0.61553871, 0.77465223, 0.77465223],\n        [0.53101621, 0.53101621, 0.17058994, 0.32359305, 0.50987675],\n        [0.53101621, 0.53101621, 0.17058994, 0.17058994, 0.32359305]\n    ],\n    [\n        [0.83804647, 0.83804647, 0.67511118, 0.59591408, 0.17378608],\n        [0.83804647, 0.5460935,  0.46040221, 0.45809764, 0.45809764],\n        [0.5460935,  0.49930348, 0.23131078, 0.45809764, 0.59591408],\n        [0.49930348, 0.23131078, 0.23131078, 0.46040221, 0.67501737],\n        [0.21602811, 0.1936703,  0.21602811, 0.67501737, 0.87881997]\n    ]\n])\n\nassertion_value = np.allclose(output, expect)\nassert assertion_value", "solution": "\n    output = np.zeros(A.shape)\n    for i in range(A.shape[0]):\n        output[i] =  median_filter(A[i], size=size)\n    return output", "type_of_change": "argument or attribute change", "name_of_class_or_func": "scipy.ndimage.median_filter", "additional_dependencies": "", "functional": "1", "webdev": "0", "solution_api_call": true, "api_calls": ["range", "scipy.ndimage.median_filter", "numpy.zeros"], "release_date": "2022-10", "docs": ["https://docs.scipy.org/doc/scipy-1.9.2/reference/ndimage.html"]}
{"python_version": "3.10", "library": "scipy", "version": "1.11.2", "problem": "\nComplete the function that applies a uniform filter on batched images (batched in the first dimention), non specified parameters should use the default value", "starting_code": "from scipy.ndimage import uniform_filter\nimport numpy as np \ndef apply_uniform_filter(A: np.ndarray, size: int) -> np.ndarray:", "example_id": "135", "test": "\nA = np.array([[[7.81411439e-01, 1.12331105e-02, 9.39679607e-01, 6.42515536e-01,\n         6.95548884e-01],\n        [4.58001321e-01, 7.90049557e-01, 7.41113873e-01, 9.85894466e-03,\n         4.53976568e-01],\n        [2.24634401e-01, 4.26973301e-01, 8.91014236e-01, 8.23895514e-01,\n         9.31046059e-01],\n        [4.45855698e-02, 2.58723740e-02, 3.84879860e-01, 5.52190839e-01,\n         8.30264909e-02],\n        [9.15445530e-01, 8.36618255e-02, 4.39455188e-01, 1.53462390e-01,\n         7.26436401e-01]],\n\n       [[7.28031138e-01, 3.75117422e-01, 5.42310487e-01, 5.63557751e-01,\n         2.57909701e-04],\n        [7.50931873e-01, 6.15538706e-01, 9.05514875e-01, 8.48424138e-01,\n         3.10902901e-01],\n        [1.72613108e-01, 7.06317645e-01, 8.89355115e-01, 8.90701413e-02,\n         9.28579120e-01],\n        [9.80044173e-01, 1.67306937e-01, 1.69314197e-01, 5.09876755e-01,\n         7.74652232e-01],\n        [5.31016205e-01, 6.71446481e-01, 8.29882594e-02, 1.70589945e-01,\n         3.23593050e-01]],\n\n       [[8.38046468e-01, 7.96708042e-03, 9.28662972e-01, 6.75111182e-01,\n         3.33247539e-02],\n        [9.99455965e-01, 9.41123427e-01, 1.06977786e-01, 1.73786082e-01,\n         5.95914076e-01],\n        [5.46093501e-01, 4.99303483e-01, 4.60402210e-01, 4.75650259e-02,\n         4.58097639e-01],\n        [5.15319589e-01, 2.31310779e-01, 1.93670305e-01, 9.76320068e-01,\n         8.78819966e-01],\n        [1.09356481e-01, 2.16028113e-01, 1.67111833e-01, 9.65726787e-01,\n         6.75017369e-01]]])\nsize=3\noutput = apply_uniform_filter(A,size=size)\nexpect = np.array([\n    [\n        [0.53935158, 0.60597923, 0.52531988, 0.6400486,  0.55389319],\n        [0.4618167,  0.58490121, 0.5862593,  0.68096102, 0.62637922],\n        [0.2997042,  0.44301383, 0.51620539, 0.54122249, 0.48022706],\n        [0.32287094, 0.38183581, 0.42015617, 0.55393411, 0.55672963],\n        [0.43823881, 0.37027365, 0.25734465, 0.40653391, 0.43676824]\n    ],\n    [\n        [0.64219576, 0.61810039, 0.59238323, 0.47523269, 0.28870856],\n        [0.55556956, 0.63174782, 0.61502292, 0.56421916, 0.44228132],\n        [0.5884824,  0.59521518, 0.54452428, 0.60285439, 0.60840439],\n        [0.54582423, 0.48560024, 0.3840295,  0.43755765, 0.53590952],\n        [0.62159479, 0.43195191, 0.29961636, 0.28979841, 0.41052592]\n    ],\n    [\n        [0.70090615, 0.62187891, 0.49392997, 0.46120842, 0.31657062],\n        [0.69062065, 0.59200365, 0.42676658, 0.38664908, 0.34123725],\n        [0.64371953, 0.49929523, 0.40338435, 0.4323948,  0.56259273],\n        [0.3653535,  0.3265107,  0.41749318, 0.53585902, 0.66816465],\n        [0.23682579, 0.2139215,  0.45544829, 0.62939137, 0.81838701]\n    ]\n])\nassertion_value = np.allclose(output, expect)\nassert assertion_value", "solution": "\n    return uniform_filter(A,size=size,axes=[1,2])", "type_of_change": "argument or attribute change", "name_of_class_or_func": "scipy.ndimage.uniform_filter", "additional_dependencies": "", "functional": "1", "webdev": "0", "solution_api_call": true, "api_calls": ["scipy.ndimage.uniform_filter"], "release_date": "2023-08", "docs": ["https://docs.scipy.org/doc/scipy-1.11.1/reference/ndimage.html"]}
{"python_version": "3.10", "library": "scipy", "version": "1.9.2", "problem": "\nComplete the function that applies a uniform filter on batched images (batched in the first dimention), non specified parameters should use the default value", "starting_code": "from scipy.ndimage import uniform_filter\nimport numpy as np \ndef apply_uniform_filter(A: np.ndarray, size: int) -> np.ndarray:\n    ", "example_id": "136", "test": "\nA = np.array([[[7.81411439e-01, 1.12331105e-02, 9.39679607e-01, 6.42515536e-01,\n         6.95548884e-01],\n        [4.58001321e-01, 7.90049557e-01, 7.41113873e-01, 9.85894466e-03,\n         4.53976568e-01],\n        [2.24634401e-01, 4.26973301e-01, 8.91014236e-01, 8.23895514e-01,\n         9.31046059e-01],\n        [4.45855698e-02, 2.58723740e-02, 3.84879860e-01, 5.52190839e-01,\n         8.30264909e-02],\n        [9.15445530e-01, 8.36618255e-02, 4.39455188e-01, 1.53462390e-01,\n         7.26436401e-01]],\n\n       [[7.28031138e-01, 3.75117422e-01, 5.42310487e-01, 5.63557751e-01,\n         2.57909701e-04],\n        [7.50931873e-01, 6.15538706e-01, 9.05514875e-01, 8.48424138e-01,\n         3.10902901e-01],\n        [1.72613108e-01, 7.06317645e-01, 8.89355115e-01, 8.90701413e-02,\n         9.28579120e-01],\n        [9.80044173e-01, 1.67306937e-01, 1.69314197e-01, 5.09876755e-01,\n         7.74652232e-01],\n        [5.31016205e-01, 6.71446481e-01, 8.29882594e-02, 1.70589945e-01,\n         3.23593050e-01]],\n\n       [[8.38046468e-01, 7.96708042e-03, 9.28662972e-01, 6.75111182e-01,\n         3.33247539e-02],\n        [9.99455965e-01, 9.41123427e-01, 1.06977786e-01, 1.73786082e-01,\n         5.95914076e-01],\n        [5.46093501e-01, 4.99303483e-01, 4.60402210e-01, 4.75650259e-02,\n         4.58097639e-01],\n        [5.15319589e-01, 2.31310779e-01, 1.93670305e-01, 9.76320068e-01,\n         8.78819966e-01],\n        [1.09356481e-01, 2.16028113e-01, 1.67111833e-01, 9.65726787e-01,\n         6.75017369e-01]]])\nsize=3\noutput = apply_uniform_filter(A,size=size)\nexpect = np.array([\n    [\n        [0.53935158, 0.60597923, 0.52531988, 0.6400486,  0.55389319],\n        [0.4618167,  0.58490121, 0.5862593,  0.68096102, 0.62637922],\n        [0.2997042,  0.44301383, 0.51620539, 0.54122249, 0.48022706],\n        [0.32287094, 0.38183581, 0.42015617, 0.55393411, 0.55672963],\n        [0.43823881, 0.37027365, 0.25734465, 0.40653391, 0.43676824]\n    ],\n    [\n        [0.64219576, 0.61810039, 0.59238323, 0.47523269, 0.28870856],\n        [0.55556956, 0.63174782, 0.61502292, 0.56421916, 0.44228132],\n        [0.5884824,  0.59521518, 0.54452428, 0.60285439, 0.60840439],\n        [0.54582423, 0.48560024, 0.3840295,  0.43755765, 0.53590952],\n        [0.62159479, 0.43195191, 0.29961636, 0.28979841, 0.41052592]\n    ],\n    [\n        [0.70090615, 0.62187891, 0.49392997, 0.46120842, 0.31657062],\n        [0.69062065, 0.59200365, 0.42676658, 0.38664908, 0.34123725],\n        [0.64371953, 0.49929523, 0.40338435, 0.4323948,  0.56259273],\n        [0.3653535,  0.3265107,  0.41749318, 0.53585902, 0.66816465],\n        [0.23682579, 0.2139215,  0.45544829, 0.62939137, 0.81838701]\n    ]\n])\nassertion_value = np.allclose(output, expect)\nassert assertion_value", "solution": "\n    output = np.zeros(A.shape)\n    for i in range(A.shape[0]):\n        output[i] =  uniform_filter(A[i], size=size)\n    return output", "type_of_change": "argument or attribute change", "name_of_class_or_func": "scipy.ndimage.uniform_filter", "additional_dependencies": "", "functional": "1", "webdev": "0", "solution_api_call": true, "api_calls": ["range", "numpy.zeros", "scipy.ndimage.uniform_filter"], "release_date": "2022-10", "docs": ["https://docs.scipy.org/doc/scipy-1.9.2/reference/ndimage.html"]}
{"python_version": "3.10", "library": "scipy", "version": "1.11.2", "problem": "\nComplete  the function that applies a minimum filter on batched images (batched in the first dimention), non specified parameters should use the default value", "starting_code": "from scipy.ndimage import minimum_filter\nimport numpy as np \ndef apply_minimum_filter(A: np.ndarray, size: int) -> np.ndarray:\n    ", "example_id": "137", "test": "\nA = np.array([[[7.81411439e-01, 1.12331105e-02, 9.39679607e-01, 6.42515536e-01,\n         6.95548884e-01],\n        [4.58001321e-01, 7.90049557e-01, 7.41113873e-01, 9.85894466e-03,\n         4.53976568e-01],\n        [2.24634401e-01, 4.26973301e-01, 8.91014236e-01, 8.23895514e-01,\n         9.31046059e-01],\n        [4.45855698e-02, 2.58723740e-02, 3.84879860e-01, 5.52190839e-01,\n         8.30264909e-02],\n        [9.15445530e-01, 8.36618255e-02, 4.39455188e-01, 1.53462390e-01,\n         7.26436401e-01]],\n\n       [[7.28031138e-01, 3.75117422e-01, 5.42310487e-01, 5.63557751e-01,\n         2.57909701e-04],\n        [7.50931873e-01, 6.15538706e-01, 9.05514875e-01, 8.48424138e-01,\n         3.10902901e-01],\n        [1.72613108e-01, 7.06317645e-01, 8.89355115e-01, 8.90701413e-02,\n         9.28579120e-01],\n        [9.80044173e-01, 1.67306937e-01, 1.69314197e-01, 5.09876755e-01,\n         7.74652232e-01],\n        [5.31016205e-01, 6.71446481e-01, 8.29882594e-02, 1.70589945e-01,\n         3.23593050e-01]],\n\n       [[8.38046468e-01, 7.96708042e-03, 9.28662972e-01, 6.75111182e-01,\n         3.33247539e-02],\n        [9.99455965e-01, 9.41123427e-01, 1.06977786e-01, 1.73786082e-01,\n         5.95914076e-01],\n        [5.46093501e-01, 4.99303483e-01, 4.60402210e-01, 4.75650259e-02,\n         4.58097639e-01],\n        [5.15319589e-01, 2.31310779e-01, 1.93670305e-01, 9.76320068e-01,\n         8.78819966e-01],\n        [1.09356481e-01, 2.16028113e-01, 1.67111833e-01, 9.65726787e-01,\n         6.75017369e-01]]])\nsize=3\noutput = apply_minimum_filter(A,size=size)\nexpect = np.array([\n    [\n        [1.12331105e-02, 1.12331105e-02, 9.85894466e-03, 9.85894466e-03, 9.85894466e-03],\n        [1.12331105e-02, 1.12331105e-02, 9.85894466e-03, 9.85894466e-03, 9.85894466e-03],\n        [2.58723740e-02, 2.58723740e-02, 9.85894466e-03, 9.85894466e-03, 9.85894466e-03],\n        [2.58723740e-02, 2.58723740e-02, 2.58723740e-02, 8.30264909e-02, 8.30264909e-02],\n        [2.58723740e-02, 2.58723740e-02, 2.58723740e-02, 8.30264909e-02, 8.30264909e-02]\n    ],\n    [\n        [3.75117422e-01, 3.75117422e-01, 3.75117422e-01, 2.57909701e-04, 2.57909701e-04],\n        [1.72613108e-01, 1.72613108e-01, 8.90701413e-02, 2.57909701e-04, 2.57909701e-04],\n        [1.67306937e-01, 1.67306937e-01, 8.90701413e-02, 8.90701413e-02, 8.90701413e-02],\n        [1.67306937e-01, 8.29882594e-02, 8.29882594e-02, 8.29882594e-02, 8.90701413e-02],\n        [1.67306937e-01, 8.29882594e-02, 8.29882594e-02, 8.29882594e-02, 1.70589945e-01]\n    ],\n    [\n        [7.96708042e-03, 7.96708042e-03, 7.96708042e-03, 3.33247539e-02, 3.33247539e-02],\n        [7.96708042e-03, 7.96708042e-03, 7.96708042e-03, 3.33247539e-02, 3.33247539e-02],\n        [2.31310779e-01, 1.06977786e-01, 4.75650259e-02, 4.75650259e-02, 4.75650259e-02],\n        [1.09356481e-01, 1.09356481e-01, 4.75650259e-02, 4.75650259e-02, 4.75650259e-02],\n        [1.09356481e-01, 1.09356481e-01, 1.67111833e-01, 1.67111833e-01, 6.75017369e-01]\n    ]\n])\nassertion_value = np.allclose(output, expect)\nassert assertion_value", "solution": "\n    return minimum_filter(A,size=size,axes=[1,2])", "type_of_change": "argument or attribute change", "name_of_class_or_func": "scipy.ndimage.minimum_filter", "additional_dependencies": "", "functional": "1", "webdev": "0", "solution_api_call": true, "api_calls": ["scipy.ndimage.minimum_filter"], "release_date": "2023-08", "docs": ["https://docs.scipy.org/doc/scipy-1.11.1/reference/ndimage.html"]}
{"python_version": "3.10", "library": "scipy", "version": "1.9.2", "problem": "\nComplete  the function that applies a minimum filter on batched images (batched in the first dimention), non specified parameters should use the default value", "starting_code": "from scipy.ndimage import minimum_filter\nimport numpy as np \ndef apply_minimum_filter(A: np.ndarray, size: int) -> np.ndarray:\n    ", "example_id": "138", "test": "\nA = np.array([[[7.81411439e-01, 1.12331105e-02, 9.39679607e-01, 6.42515536e-01,\n         6.95548884e-01],\n        [4.58001321e-01, 7.90049557e-01, 7.41113873e-01, 9.85894466e-03,\n         4.53976568e-01],\n        [2.24634401e-01, 4.26973301e-01, 8.91014236e-01, 8.23895514e-01,\n         9.31046059e-01],\n        [4.45855698e-02, 2.58723740e-02, 3.84879860e-01, 5.52190839e-01,\n         8.30264909e-02],\n        [9.15445530e-01, 8.36618255e-02, 4.39455188e-01, 1.53462390e-01,\n         7.26436401e-01]],\n\n       [[7.28031138e-01, 3.75117422e-01, 5.42310487e-01, 5.63557751e-01,\n         2.57909701e-04],\n        [7.50931873e-01, 6.15538706e-01, 9.05514875e-01, 8.48424138e-01,\n         3.10902901e-01],\n        [1.72613108e-01, 7.06317645e-01, 8.89355115e-01, 8.90701413e-02,\n         9.28579120e-01],\n        [9.80044173e-01, 1.67306937e-01, 1.69314197e-01, 5.09876755e-01,\n         7.74652232e-01],\n        [5.31016205e-01, 6.71446481e-01, 8.29882594e-02, 1.70589945e-01,\n         3.23593050e-01]],\n\n       [[8.38046468e-01, 7.96708042e-03, 9.28662972e-01, 6.75111182e-01,\n         3.33247539e-02],\n        [9.99455965e-01, 9.41123427e-01, 1.06977786e-01, 1.73786082e-01,\n         5.95914076e-01],\n        [5.46093501e-01, 4.99303483e-01, 4.60402210e-01, 4.75650259e-02,\n         4.58097639e-01],\n        [5.15319589e-01, 2.31310779e-01, 1.93670305e-01, 9.76320068e-01,\n         8.78819966e-01],\n        [1.09356481e-01, 2.16028113e-01, 1.67111833e-01, 9.65726787e-01,\n         6.75017369e-01]]])\nsize=3\noutput = apply_minimum_filter(A,size=size)\nexpect = np.array([\n    [\n        [1.12331105e-02, 1.12331105e-02, 9.85894466e-03, 9.85894466e-03, 9.85894466e-03],\n        [1.12331105e-02, 1.12331105e-02, 9.85894466e-03, 9.85894466e-03, 9.85894466e-03],\n        [2.58723740e-02, 2.58723740e-02, 9.85894466e-03, 9.85894466e-03, 9.85894466e-03],\n        [2.58723740e-02, 2.58723740e-02, 2.58723740e-02, 8.30264909e-02, 8.30264909e-02],\n        [2.58723740e-02, 2.58723740e-02, 2.58723740e-02, 8.30264909e-02, 8.30264909e-02]\n    ],\n    [\n        [3.75117422e-01, 3.75117422e-01, 3.75117422e-01, 2.57909701e-04, 2.57909701e-04],\n        [1.72613108e-01, 1.72613108e-01, 8.90701413e-02, 2.57909701e-04, 2.57909701e-04],\n        [1.67306937e-01, 1.67306937e-01, 8.90701413e-02, 8.90701413e-02, 8.90701413e-02],\n        [1.67306937e-01, 8.29882594e-02, 8.29882594e-02, 8.29882594e-02, 8.90701413e-02],\n        [1.67306937e-01, 8.29882594e-02, 8.29882594e-02, 8.29882594e-02, 1.70589945e-01]\n    ],\n    [\n        [7.96708042e-03, 7.96708042e-03, 7.96708042e-03, 3.33247539e-02, 3.33247539e-02],\n        [7.96708042e-03, 7.96708042e-03, 7.96708042e-03, 3.33247539e-02, 3.33247539e-02],\n        [2.31310779e-01, 1.06977786e-01, 4.75650259e-02, 4.75650259e-02, 4.75650259e-02],\n        [1.09356481e-01, 1.09356481e-01, 4.75650259e-02, 4.75650259e-02, 4.75650259e-02],\n        [1.09356481e-01, 1.09356481e-01, 1.67111833e-01, 1.67111833e-01, 6.75017369e-01]\n    ]\n])\nassertion_value = np.allclose(output, expect)\nassert assertion_value", "solution": "\n    output = np.zeros(A.shape)\n    for i in range(A.shape[0]):\n        output[i] =  minimum_filter(A[i], size=size)\n    return output", "type_of_change": "argument or attribute change", "name_of_class_or_func": "scipy.ndimage.minimum_filter", "additional_dependencies": "", "functional": "1", "webdev": "0", "solution_api_call": true, "api_calls": ["range", "scipy.ndimage.minimum_filter", "numpy.zeros"], "release_date": "2022-10", "docs": ["https://docs.scipy.org/doc/scipy-1.9.2/reference/ndimage.html"]}
{"python_version": "3.10", "library": "scipy", "version": "1.11.2", "problem": "\nComplete the function that applies a maximum filter on batched images (batched in the first dimention), non specified parameters should use the default value", "starting_code": "from scipy.ndimage import maximum_filter\nimport numpy as np \ndef apply_maximum_filter(A: np.ndarray, size: int) -> np.ndarray:\n    ", "example_id": "139", "test": "\nA = np.array([[[7.81411439e-01, 1.12331105e-02, 9.39679607e-01, 6.42515536e-01,\n         6.95548884e-01],\n        [4.58001321e-01, 7.90049557e-01, 7.41113873e-01, 9.85894466e-03,\n         4.53976568e-01],\n        [2.24634401e-01, 4.26973301e-01, 8.91014236e-01, 8.23895514e-01,\n         9.31046059e-01],\n        [4.45855698e-02, 2.58723740e-02, 3.84879860e-01, 5.52190839e-01,\n         8.30264909e-02],\n        [9.15445530e-01, 8.36618255e-02, 4.39455188e-01, 1.53462390e-01,\n         7.26436401e-01]],\n\n       [[7.28031138e-01, 3.75117422e-01, 5.42310487e-01, 5.63557751e-01,\n         2.57909701e-04],\n        [7.50931873e-01, 6.15538706e-01, 9.05514875e-01, 8.48424138e-01,\n         3.10902901e-01],\n        [1.72613108e-01, 7.06317645e-01, 8.89355115e-01, 8.90701413e-02,\n         9.28579120e-01],\n        [9.80044173e-01, 1.67306937e-01, 1.69314197e-01, 5.09876755e-01,\n         7.74652232e-01],\n        [5.31016205e-01, 6.71446481e-01, 8.29882594e-02, 1.70589945e-01,\n         3.23593050e-01]],\n\n       [[8.38046468e-01, 7.96708042e-03, 9.28662972e-01, 6.75111182e-01,\n         3.33247539e-02],\n        [9.99455965e-01, 9.41123427e-01, 1.06977786e-01, 1.73786082e-01,\n         5.95914076e-01],\n        [5.46093501e-01, 4.99303483e-01, 4.60402210e-01, 4.75650259e-02,\n         4.58097639e-01],\n        [5.15319589e-01, 2.31310779e-01, 1.93670305e-01, 9.76320068e-01,\n         8.78819966e-01],\n        [1.09356481e-01, 2.16028113e-01, 1.67111833e-01, 9.65726787e-01,\n         6.75017369e-01]]])\nsize=3\noutput = apply_maximum_filter(A,size=size)\nexpect = np.array([\n    [\n        [0.79004956, 0.93967961, 0.93967961, 0.93967961, 0.69554888],\n        [0.79004956, 0.93967961, 0.93967961, 0.93967961, 0.93104606],\n        [0.79004956, 0.89101424, 0.89101424, 0.93104606, 0.93104606],\n        [0.91544553, 0.91544553, 0.89101424, 0.93104606, 0.93104606],\n        [0.91544553, 0.91544553, 0.55219084, 0.7264364,  0.7264364 ]\n    ],\n    [\n        [0.75093187, 0.90551488, 0.90551488, 0.90551488, 0.84842414],\n        [0.75093187, 0.90551488, 0.90551488, 0.92857912, 0.92857912],\n        [0.98004417, 0.98004417, 0.90551488, 0.92857912, 0.92857912],\n        [0.98004417, 0.98004417, 0.88935511, 0.92857912, 0.92857912],\n        [0.98004417, 0.98004417, 0.67144648, 0.77465223, 0.77465223]\n    ],\n    [\n        [0.99945597, 0.99945597, 0.94112343, 0.92866297, 0.67511118],\n        [0.99945597, 0.99945597, 0.94112343, 0.92866297, 0.67511118],\n        [0.99945597, 0.99945597, 0.97632007, 0.97632007, 0.97632007],\n        [0.5460935,  0.5460935,  0.97632007, 0.97632007, 0.97632007],\n        [0.51531959, 0.51531959, 0.97632007, 0.97632007, 0.97632007]\n    ]\n])\n\nassertion_value = np.allclose(output, expect)\nassert assertion_value", "solution": "\n    return maximum_filter(A,size=size,axes=[1,2])", "type_of_change": "argument or attribute change", "name_of_class_or_func": "scipy.ndimage.maximum_filter", "additional_dependencies": "", "functional": "1", "webdev": "0", "solution_api_call": true, "api_calls": ["scipy.ndimage.maximum_filter"], "release_date": "2023-08", "docs": ["https://docs.scipy.org/doc/scipy-1.11.1/reference/ndimage.html"]}
{"python_version": "3.10", "library": "scipy", "version": "1.9.2", "problem": "\nComplete the function that applies a maximum filter on batched images (batched in the first dimention), non specified parameters should use the default value", "starting_code": "from scipy.ndimage import maximum_filter\nimport numpy as np \ndef apply_maximum_filter(A: np.ndarray, size: int) -> np.ndarray:\n    ", "example_id": "140", "test": "\nA = np.array([[[7.81411439e-01, 1.12331105e-02, 9.39679607e-01, 6.42515536e-01,\n         6.95548884e-01],\n        [4.58001321e-01, 7.90049557e-01, 7.41113873e-01, 9.85894466e-03,\n         4.53976568e-01],\n        [2.24634401e-01, 4.26973301e-01, 8.91014236e-01, 8.23895514e-01,\n         9.31046059e-01],\n        [4.45855698e-02, 2.58723740e-02, 3.84879860e-01, 5.52190839e-01,\n         8.30264909e-02],\n        [9.15445530e-01, 8.36618255e-02, 4.39455188e-01, 1.53462390e-01,\n         7.26436401e-01]],\n\n       [[7.28031138e-01, 3.75117422e-01, 5.42310487e-01, 5.63557751e-01,\n         2.57909701e-04],\n        [7.50931873e-01, 6.15538706e-01, 9.05514875e-01, 8.48424138e-01,\n         3.10902901e-01],\n        [1.72613108e-01, 7.06317645e-01, 8.89355115e-01, 8.90701413e-02,\n         9.28579120e-01],\n        [9.80044173e-01, 1.67306937e-01, 1.69314197e-01, 5.09876755e-01,\n         7.74652232e-01],\n        [5.31016205e-01, 6.71446481e-01, 8.29882594e-02, 1.70589945e-01,\n         3.23593050e-01]],\n\n       [[8.38046468e-01, 7.96708042e-03, 9.28662972e-01, 6.75111182e-01,\n         3.33247539e-02],\n        [9.99455965e-01, 9.41123427e-01, 1.06977786e-01, 1.73786082e-01,\n         5.95914076e-01],\n        [5.46093501e-01, 4.99303483e-01, 4.60402210e-01, 4.75650259e-02,\n         4.58097639e-01],\n        [5.15319589e-01, 2.31310779e-01, 1.93670305e-01, 9.76320068e-01,\n         8.78819966e-01],\n        [1.09356481e-01, 2.16028113e-01, 1.67111833e-01, 9.65726787e-01,\n         6.75017369e-01]]])\nsize=3\noutput = apply_maximum_filter(A,size=size)\nexpect = np.array([\n    [\n        [0.79004956, 0.93967961, 0.93967961, 0.93967961, 0.69554888],\n        [0.79004956, 0.93967961, 0.93967961, 0.93967961, 0.93104606],\n        [0.79004956, 0.89101424, 0.89101424, 0.93104606, 0.93104606],\n        [0.91544553, 0.91544553, 0.89101424, 0.93104606, 0.93104606],\n        [0.91544553, 0.91544553, 0.55219084, 0.7264364,  0.7264364 ]\n    ],\n    [\n        [0.75093187, 0.90551488, 0.90551488, 0.90551488, 0.84842414],\n        [0.75093187, 0.90551488, 0.90551488, 0.92857912, 0.92857912],\n        [0.98004417, 0.98004417, 0.90551488, 0.92857912, 0.92857912],\n        [0.98004417, 0.98004417, 0.88935511, 0.92857912, 0.92857912],\n        [0.98004417, 0.98004417, 0.67144648, 0.77465223, 0.77465223]\n    ],\n    [\n        [0.99945597, 0.99945597, 0.94112343, 0.92866297, 0.67511118],\n        [0.99945597, 0.99945597, 0.94112343, 0.92866297, 0.67511118],\n        [0.99945597, 0.99945597, 0.97632007, 0.97632007, 0.97632007],\n        [0.5460935,  0.5460935,  0.97632007, 0.97632007, 0.97632007],\n        [0.51531959, 0.51531959, 0.97632007, 0.97632007, 0.97632007]\n    ]\n])\n\nassertion_value = np.allclose(output, expect)\nassert assertion_value", "solution": "\n    output = np.zeros(A.shape)\n    for i in range(A.shape[0]):\n        output[i] =  maximum_filter(A[i], size=size)\n    return output", "type_of_change": "argument or attribute change", "name_of_class_or_func": "scipy.ndimage.maximum_filter", "additional_dependencies": "", "functional": "1", "webdev": "0", "solution_api_call": true, "api_calls": ["range", "numpy.zeros", "scipy.ndimage.maximum_filter"], "release_date": "2022-10", "docs": ["https://docs.scipy.org/doc/scipy-1.9.2/reference/ndimage.html"]}
{"python_version": "3.10", "library": "scipy", "version": "1.11.2", "problem": "\nComplete the function that applies a gaussian filter on batched images (batched in the first dimention), non specified parameters should use the default value", "starting_code": "from scipy.ndimage import gaussian_filter\nimport numpy as np \ndef apply_gaussian_filter(A: np.ndarray, sigma: float) -> np.ndarray:\n    ", "example_id": "141", "test": "\nA = np.array([[[7.81411439e-01, 1.12331105e-02, 9.39679607e-01, 6.42515536e-01,\n         6.95548884e-01],\n        [4.58001321e-01, 7.90049557e-01, 7.41113873e-01, 9.85894466e-03,\n         4.53976568e-01],\n        [2.24634401e-01, 4.26973301e-01, 8.91014236e-01, 8.23895514e-01,\n         9.31046059e-01],\n        [4.45855698e-02, 2.58723740e-02, 3.84879860e-01, 5.52190839e-01,\n         8.30264909e-02],\n        [9.15445530e-01, 8.36618255e-02, 4.39455188e-01, 1.53462390e-01,\n         7.26436401e-01]],\n\n       [[7.28031138e-01, 3.75117422e-01, 5.42310487e-01, 5.63557751e-01,\n         2.57909701e-04],\n        [7.50931873e-01, 6.15538706e-01, 9.05514875e-01, 8.48424138e-01,\n         3.10902901e-01],\n        [1.72613108e-01, 7.06317645e-01, 8.89355115e-01, 8.90701413e-02,\n         9.28579120e-01],\n        [9.80044173e-01, 1.67306937e-01, 1.69314197e-01, 5.09876755e-01,\n         7.74652232e-01],\n        [5.31016205e-01, 6.71446481e-01, 8.29882594e-02, 1.70589945e-01,\n         3.23593050e-01]],\n\n       [[8.38046468e-01, 7.96708042e-03, 9.28662972e-01, 6.75111182e-01,\n         3.33247539e-02],\n        [9.99455965e-01, 9.41123427e-01, 1.06977786e-01, 1.73786082e-01,\n         5.95914076e-01],\n        [5.46093501e-01, 4.99303483e-01, 4.60402210e-01, 4.75650259e-02,\n         4.58097639e-01],\n        [5.15319589e-01, 2.31310779e-01, 1.93670305e-01, 9.76320068e-01,\n         8.78819966e-01],\n        [1.09356481e-01, 2.16028113e-01, 1.67111833e-01, 9.65726787e-01,\n         6.75017369e-01]]])\nsigma=2\noutput = apply_gaussian_filter(A,sigma=sigma)\nexpect = np.array([\n    [\n        [0.51107853, 0.52434293, 0.5435481,  0.55983926, 0.56841324],\n        [0.48836795, 0.50252599, 0.52348203, 0.54190047, 0.55197513],\n        [0.4538848,  0.46817802, 0.49030855, 0.51105784, 0.52313636],\n        [0.42268583, 0.43538067, 0.45646669, 0.47802514, 0.49150328],\n        [0.40530094, 0.41608726, 0.43533884, 0.45656804, 0.47057292]\n    ],\n    [\n        [0.58159932, 0.567709,   0.54222862, 0.51274141, 0.49237396],\n        [0.56988096, 0.55678826, 0.53427875, 0.50995134, 0.49392329],\n        [0.54914438, 0.53583943, 0.51580817, 0.49760273, 0.4872573 ],\n        [0.52640569, 0.51103919, 0.49059125, 0.47570284, 0.46933531],\n        [0.51143711, 0.4938774,  0.47171782, 0.45745161, 0.45266544]\n    ],\n    [\n        [0.56163134, 0.53611524, 0.4985118,  0.46508331, 0.44632982],\n        [0.53501922, 0.51740249, 0.49329637, 0.47430703, 0.46497321],\n        [0.4887689,  0.48537733, 0.48514968, 0.49115186, 0.49783821],\n        [0.43872375, 0.45125797, 0.47730807, 0.51021342, 0.53383157],\n        [0.40596121, 0.42914707, 0.47257655, 0.523013,   0.55752451]\n    ]\n])\nassertion_value = np.allclose(output, expect)\nassert assertion_value", "solution": "\n    return gaussian_filter(A,sigma=sigma,axes=[1,2])", "type_of_change": "argument or attribute change", "name_of_class_or_func": "scipy.ndimage.gaussian_filter", "additional_dependencies": "", "functional": "1", "webdev": "0", "solution_api_call": true, "api_calls": ["scipy.ndimage.gaussian_filter"], "release_date": "2023-08", "docs": ["https://docs.scipy.org/doc/scipy-1.11.1/reference/ndimage.html"]}
{"python_version": "3.10", "library": "scipy", "version": "1.9.2", "problem": "\nComplete the function that applies a gaussian filter on batched images (batched in the first dimention), non specified parameters should use the default value", "starting_code": "from scipy.ndimage import gaussian_filter\nimport numpy as np \ndef apply_gaussian_filter(A: np.ndarray, sigma: float) -> np.ndarray:\n    ", "example_id": "142", "test": "\nA = np.array([[[7.81411439e-01, 1.12331105e-02, 9.39679607e-01, 6.42515536e-01,\n         6.95548884e-01],\n        [4.58001321e-01, 7.90049557e-01, 7.41113873e-01, 9.85894466e-03,\n         4.53976568e-01],\n        [2.24634401e-01, 4.26973301e-01, 8.91014236e-01, 8.23895514e-01,\n         9.31046059e-01],\n        [4.45855698e-02, 2.58723740e-02, 3.84879860e-01, 5.52190839e-01,\n         8.30264909e-02],\n        [9.15445530e-01, 8.36618255e-02, 4.39455188e-01, 1.53462390e-01,\n         7.26436401e-01]],\n\n       [[7.28031138e-01, 3.75117422e-01, 5.42310487e-01, 5.63557751e-01,\n         2.57909701e-04],\n        [7.50931873e-01, 6.15538706e-01, 9.05514875e-01, 8.48424138e-01,\n         3.10902901e-01],\n        [1.72613108e-01, 7.06317645e-01, 8.89355115e-01, 8.90701413e-02,\n         9.28579120e-01],\n        [9.80044173e-01, 1.67306937e-01, 1.69314197e-01, 5.09876755e-01,\n         7.74652232e-01],\n        [5.31016205e-01, 6.71446481e-01, 8.29882594e-02, 1.70589945e-01,\n         3.23593050e-01]],\n\n       [[8.38046468e-01, 7.96708042e-03, 9.28662972e-01, 6.75111182e-01,\n         3.33247539e-02],\n        [9.99455965e-01, 9.41123427e-01, 1.06977786e-01, 1.73786082e-01,\n         5.95914076e-01],\n        [5.46093501e-01, 4.99303483e-01, 4.60402210e-01, 4.75650259e-02,\n         4.58097639e-01],\n        [5.15319589e-01, 2.31310779e-01, 1.93670305e-01, 9.76320068e-01,\n         8.78819966e-01],\n        [1.09356481e-01, 2.16028113e-01, 1.67111833e-01, 9.65726787e-01,\n         6.75017369e-01]]])\nsigma=2\noutput = apply_gaussian_filter(A,sigma=sigma)\nexpect = np.array([\n    [\n        [0.51107853, 0.52434293, 0.5435481,  0.55983926, 0.56841324],\n        [0.48836795, 0.50252599, 0.52348203, 0.54190047, 0.55197513],\n        [0.4538848,  0.46817802, 0.49030855, 0.51105784, 0.52313636],\n        [0.42268583, 0.43538067, 0.45646669, 0.47802514, 0.49150328],\n        [0.40530094, 0.41608726, 0.43533884, 0.45656804, 0.47057292]\n    ],\n    [\n        [0.58159932, 0.567709,   0.54222862, 0.51274141, 0.49237396],\n        [0.56988096, 0.55678826, 0.53427875, 0.50995134, 0.49392329],\n        [0.54914438, 0.53583943, 0.51580817, 0.49760273, 0.4872573 ],\n        [0.52640569, 0.51103919, 0.49059125, 0.47570284, 0.46933531],\n        [0.51143711, 0.4938774,  0.47171782, 0.45745161, 0.45266544]\n    ],\n    [\n        [0.56163134, 0.53611524, 0.4985118,  0.46508331, 0.44632982],\n        [0.53501922, 0.51740249, 0.49329637, 0.47430703, 0.46497321],\n        [0.4887689,  0.48537733, 0.48514968, 0.49115186, 0.49783821],\n        [0.43872375, 0.45125797, 0.47730807, 0.51021342, 0.53383157],\n        [0.40596121, 0.42914707, 0.47257655, 0.523013,   0.55752451]\n    ]\n])\nassertion_value = np.allclose(output, expect)\nassert assertion_value", "solution": "\n    output = np.zeros(A.shape)\n    for i in range(A.shape[0]):\n        output[i] =  gaussian_filter(A[i],sigma=sigma)\n    return output", "type_of_change": "argument or attribute change", "name_of_class_or_func": "scipy.ndimage.gaussian_filter", "additional_dependencies": "", "functional": "1", "webdev": "0", "solution_api_call": true, "api_calls": ["range", "numpy.zeros", "scipy.ndimage.gaussian_filter"], "release_date": "2022-10", "docs": ["https://docs.scipy.org/doc/scipy-1.9.2/reference/ndimage.html"]}
{"python_version": "3.10", "library": "flask", "version": "2.0.0", "problem": "\nComplete the app_set_up function that set-up the app in such manner that the json encoding returns a sorted list\n when called with the eval function provided with the variable \n num_set being a set of numbers, use other libraries if needed. Do not run the app in your code.", "starting_code": "import flask\n\napp = flask.Flask('test')\n@app.route('/data')\ndef data(num_set):\n    return flask.jsonify({'numbers': num_set})\n\ndef eval(app, data_fn, num_set):\n    with app.test_request_context():\n        response = data_fn(num_set)\n        return response.get_data(as_text=False)\n\ndef app_set_up(app: flask.Flask) -> None: \n    ", "example_id": "143", "test": "\nimport json\napp_set_up(app)\napp2 = flask.Flask('test2')\n@app2.route('/data2')\ndef data2(num_set):\n    return flask.jsonify({'numbers': num_set})\nclass MyCustomJSONHandler2(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, set):\n            return sorted(list(obj))\n        return super().default(obj)\n\napp2.json_encoder = MyCustomJSONHandler2\nassertion_result = eval(app2, data2, {3, 1, 2, 6, 5, 4}) == eval(app, data, {3, 1, 2, 6, 5, 4})\nassert assertion_result\n", "solution": "\n    import json\n    class MyCustomJSONHandler(json.JSONEncoder):\n        def default(self, obj):\n            if isinstance(obj, set):\n                return sorted(list(obj))\n            return super().default(obj)\n    app.json_encoder = MyCustomJSONHandler\n", "type_of_change": "argument or attribute change", "name_of_class_or_func": "app.json_encoder", "additional_dependencies": "werkzeug==2.0.0", "functional": "1", "webdev": "1", "solution_api_call": true, "api_calls": ["default", "isinstance", "sorted", "super", "list", "MyCustomJSONHandler"], "release_date": "2021-05", "docs": ["https://tedboy.github.io/flask/interface_api.json_support.html", "https://flask.palletsprojects.com/en/stable/changes/"]}
{"python_version": "3.10", "library": "flask", "version": "3.0.0", "problem": "\nComplete the app_set_up function that set-up the app in such manner that the json encoding returns a sorted list\n when called with the eval function provided with the variable \n num_set being a set of numbers, use other libraries if needed. Do not run the app in your code.", "starting_code": "import flask\n\napp = flask.Flask('test')\n@app.route('/data')\ndef data(num_set):\n    return flask.jsonify({'numbers':num_set})\n\ndef eval(app, data_fn, num_set):\n    with app.test_request_context():\n        response = data_fn(num_set)\n        return response.get_data(as_text=True)\n\ndef app_set_up(app: flask.Flask) -> None: \n    ", "example_id": "144", "test": "\nimport json\napp_set_up(app)\napp2 = flask.Flask('test2')\n@app2.route('/data2')\ndef data2(num_set):\n    return flask.jsonify({'numbers': num_set})\nclass MyCustomJSONHandler2(flask.json.provider.DefaultJSONProvider):\n    def default(self, obj):\n        if isinstance(obj, set):\n            return sorted(list(obj))\n        return super().default(obj)\napp2.json_provider_class = MyCustomJSONHandler2\napp2.json = app2.json_provider_class(app2)\n\nassertion_result = eval(app2, data2, {3, 1, 2, 6, 5, 4}) == eval(app, data, {3, 1, 2, 6, 5, 4})\nassert assertion_result\n", "solution": "\n    class MyCustomJSONHandler(flask.json.provider.DefaultJSONProvider):\n        def default(self, obj):\n            if isinstance(obj, set):\n                return sorted(list(obj))\n            return super().default(obj)\n    app.json_provider_class = MyCustomJSONHandler\n    app.json = app.json_provider_class(app)\n", "type_of_change": "argument or attribute change", "name_of_class_or_func": "app.json_encoder", "additional_dependencies": "", "functional": "1", "webdev": "1", "solution_api_call": true, "api_calls": ["default", "isinstance", "sorted", "super", "app.json_provider_class", "list", "MyCustomJSONHandler"], "release_date": "2023-09", "docs": ["https://tedboy.github.io/flask/interface_api.json_support.html", "https://flask.palletsprojects.com/en/stable/changes/"]}
{"python_version": "3.10", "library": "flask", "version": "2.0.0", "problem": "\nComplete the download definition to download the data in the variable attachment_filename. Do not run the app in your code.", "starting_code": "from flask import Flask, send_file\nfrom io import BytesIO\n\napp1 = Flask(__name__)\n\ndef get_content_disp(app, download_fn):\n    with app.test_request_context():\n        response = download_fn()\n    content_disp = response.headers.get('Content-Disposition')\n    return content_disp\n\n@app1.route('/download')\ndef download():\n    data = BytesIO(b'Hello, World!')\n    attachment_filename = 'hello.txt'\n    return send_file(data, as_attachment=True,\n", "example_id": "145", "test": "\ncontent_disp = get_content_disp(app1, download)\nassertion_result = 'filename=hello.txt' in content_disp\nassert assertion_result\n", "solution": "attachment_filename=attachment_filename)\n", "type_of_change": "argument or attribute change", "name_of_class_or_func": "flask.send_file", "additional_dependencies": "werkzeug==2.0.0", "functional": "0", "webdev": "1", "solution_api_call": true, "api_calls": [], "release_date": "2021-05", "docs": ["https://tedboy.github.io/flask/generated/flask.send_file.html", "https://flask.palletsprojects.com/en/stable/changes/"]}
{"python_version": "3.10", "library": "flask", "version": "3.0.0", "problem": "\nComplete the download definition to download the data in the variable attachment_filename. Do not run the app in your code.", "starting_code": "from flask import Flask, send_file\nfrom io import BytesIO\n\napp1 = Flask(__name__)\n\ndef get_content_disp(app, download_fn):\n    with app.test_request_context():\n        response = download_fn()\n    content_disp = response.headers.get('Content-Disposition')\n    return content_disp\n\n@app1.route('/download')\ndef download():\n    data = BytesIO(b'Hello, World!')\n    attachment_filename = 'hello.txt'\n    return send_file(data, as_attachment=True,\n\n", "example_id": "146", "test": "\ncontent_disp = get_content_disp(app1, download)\nassertion_result = 'filename=hello.txt' in content_disp\nassert assertion_result\n", "solution": "download_name=attachment_filename)\n", "type_of_change": "argument or attribute change", "name_of_class_or_func": "flask.send_file", "additional_dependencies": "", "functional": "0", "webdev": "1", "solution_api_call": true, "api_calls": [], "release_date": "2023-09", "docs": ["https://tedboy.github.io/flask/generated/flask.send_file.html", "https://flask.palletsprojects.com/en/stable/changes/"]}
{"python_version": "3.10", "library": "flask", "version": "2.0.1", "problem": "\nImplement the function to load the JSON file to config the Flask app. Do not run the app in your code.", "starting_code": "import json\nimport tempfile\nfrom flask import Flask\n\nconfig_data = {'DEBUG': True, 'SECRET_KEY': 'secret'}\nwith tempfile.NamedTemporaryFile(mode='w+', delete=False, suffix='.json') as tmp:\n    json.dump(config_data, tmp)\n    tmp.flush()\n    config_file = tmp.name\n\napp = Flask(__name__)\n\ndef load_config(config_file: str) -> None:\n    ", "example_id": "147", "test": "load_config(config_file)\nassertion_result= app.config['DEBUG'] is True and app.config['SECRET_KEY'] == 'secret'\nassert assertion_result", "solution": "    app.config.from_json(config_file)\n", "type_of_change": "name change", "name_of_class_or_func": "Flask.config.from_json", "additional_dependencies": "werkzeug==2.0.0", "functional": "1", "webdev": "1", "solution_api_call": true, "api_calls": ["app.config.from_json"], "release_date": "2021-05", "docs": ["https://flask.palletsprojects.com/en/stable/config/", "https://flask.palletsprojects.com/en/stable/changes/"]}
{"python_version": "3.10", "library": "flask", "version": "3.0.0", "problem": "\nLoad the json file to config the Flask app. Do not run the app in your code.", "starting_code": "import json\nimport tempfile\nfrom flask import Flask\n\nconfig_data = {'DEBUG': True, 'SECRET_KEY': 'secret'}\nwith tempfile.NamedTemporaryFile(mode='w+', delete=False, suffix='.json') as tmp:\n    json.dump(config_data, tmp)\n    tmp.flush()\n    config_file = tmp.name\n\napp = Flask(__name__)\n\ndef load_config(config_file: str) -> None:\n    ", "example_id": "148", "test": "load_config(config_file)\nassertion_result= app.config['DEBUG'] is True and app.config['SECRET_KEY'] == 'secret'\nassert assertion_result\n", "solution": "    app.config.from_file(config_file, load=json.load)\n", "type_of_change": "name change", "name_of_class_or_func": "Flask.config.from_file", "additional_dependencies": "", "functional": "1", "webdev": "1", "solution_api_call": true, "api_calls": ["app.config.from_file"], "release_date": "2023-09", "docs": ["https://flask.palletsprojects.com/en/stable/config/", "https://flask.palletsprojects.com/en/stable/changes/"]}
{"python_version": "3.10", "library": "flask", "version": "2.0.1", "problem": "\nComplete the safe_join_fail_404 to safely join the path and the subpath, use other libraries if needed. Do not run the app in your code.", "starting_code": "import flask\nimport werkzeug\n\nerror404 = werkzeug.exceptions.NotFound\n\ndef safe_join_fail_404(base_path: str, sub_path: str) -> str:\n    # Attempt to join the base path and sub path.\n    # If the joined path is outside the base path, raise a 404 error.\n", "example_id": "149", "test": "\nbase_path = '/var/www/myapp'\nsub_path = '../secret.txt'\n\ntry : \n    joined = safe_join_fail_404(base_path, sub_path)\nexcept werkzeug.exceptions.NotFound as e:\n    assertion_result = True\nelse:\n    assertion_result = False\nassert assertion_result\n\nbase_path = '/var/www/myapp'\nsub_path = 'secret.txt'\njoined = safe_join_fail_404(base_path, sub_path)\nassertion_result = joined == '/var/www/myapp/secret.txt'\nassert assertion_result", "solution": "\n    joined = flask.safe_join(base_path, sub_path)\n\n    return joined\n", "type_of_change": "name change", "name_of_class_or_func": "flask.safe_join", "additional_dependencies": "werkzeug==2.0.0", "functional": "1", "webdev": "1", "solution_api_call": true, "api_calls": ["flask.safe_join"], "release_date": "2021-05", "docs": ["https://tedboy.github.io/flask/generated/flask.safe_join.html", "https://flask.palletsprojects.com/en/stable/changes/"]}
{"python_version": "3.10", "library": "flask", "version": "3.0.0", "problem": "\nComplete the safe_join_fail_404 to safely join the path and the subpath, use other libraries if needed. Do not run the app in your code.", "starting_code": "import flask\nimport werkzeug\n\nerror404 = werkzeug.exceptions.NotFound\n\ndef safe_join_fail_404(base_path: str, sub_path: str) -> str:\n    # Attempt to join the base path and sub path.\n    # If the joined path is outside the base path, raise a 404 error.\n", "example_id": "150", "test": "\nbase_path = '/var/www/myapp'\nsub_path = '../secret.txt'\n\ntry : \n    joined = safe_join_fail_404(base_path, sub_path)\nexcept werkzeug.exceptions.NotFound as e:\n    assertion_result = True\nelse:\n    assertion_result = False\nassert assertion_result\n\nbase_path = '/var/www/myapp'\nsub_path = 'secret.txt'\njoined = safe_join_fail_404(base_path, sub_path)\nassertion_result = joined == '/var/www/myapp/secret.txt'\nassert assertion_result", "solution": "\n    joined = werkzeug.utils.safe_join(base_path, sub_path)\n    if joined is None:\n        raise error404\n    return joined\n", "type_of_change": "name change", "name_of_class_or_func": "flask.safe_join", "additional_dependencies": "", "functional": "1", "webdev": "1", "solution_api_call": true, "api_calls": ["werkzeug.utils.safe_join"], "release_date": "2023-09", "docs": ["https://werkzeug.palletsprojects.com/en/stable/utils/", "https://flask.palletsprojects.com/en/stable/changes/"]}
{"python_version": "3.10", "library": "flask", "version": "2.0.1", "problem": "\nComplete the function convert_timedelta_to_seconds, use other libraries if needed. Do not run the app in your code.", "starting_code": "import flask\nimport datetime\n\ndef convert_timedelta_to_seconds(td: datetime.timedelta) -> int:\n    ", "example_id": "151", "test": "\nimport datetime\ntd = datetime.timedelta(hours=2, minutes=30,microseconds=1)\nassertion_results = convert_timedelta_to_seconds(td)==9000\nassert assertion_results", "solution": "\n    return flask.helpers.total_seconds(td)\n", "type_of_change": "name change", "name_of_class_or_func": "helpers.total_seconds", "additional_dependencies": "werkzeug==2.0.0", "functional": "1", "webdev": "1", "solution_api_call": true, "api_calls": ["flask.helpers.total_seconds"], "release_date": "2021-05", "docs": ["https://flask.palletsprojects.com/en/stable/api/", "https://flask.palletsprojects.com/en/stable/changes/"]}
{"python_version": "3.10", "library": "flask", "version": "3.0.0", "problem": "\nComplete the function convert_timedelta_to_seconds, use other libraries if needed. Do not run the app in your code.", "starting_code": "import flask\nimport datetime\n\ndef convert_timedelta_to_seconds(td: datetime.timedelta):\n", "example_id": "152", "test": "\nimport datetime\ntd = datetime.timedelta(hours=2, minutes=30,microseconds=1)\nassertion_results = convert_timedelta_to_seconds(td)==9000.000001\nassert assertion_results", "solution": "\n    return td.total_seconds()\n", "type_of_change": "name change", "name_of_class_or_func": "helpers.total_seconds", "additional_dependencies": "", "functional": "1", "webdev": "1", "solution_api_call": true, "api_calls": ["td.total_seconds"], "release_date": "2023-09", "docs": ["https://flask.palletsprojects.com/en/stable/api/", "https://flask.palletsprojects.com/en/stable/changes/"]}
{"python_version": "3.10", "library": "jinja2", "version": "2.11", "problem": "\nImplement the solution function to return a custom filter. This filter should: \naccept the context and a parameter name, \nretrieve a variable prefix from the context (defaulting to 'Hello' if not provided), \nreturn a greeting message combining the prefix and the name. Do not run the app in your code.", "starting_code": "import jinja2 \nfrom jinja2.runtime import Context\nfrom typing import Callable\n\ndef setup_environment(filtername: str, filter: Callable[[Context, str], str]) -> jinja2.Environment:\n    env = jinja2.Environment()\n    env.filters[filtername] = filter\n    return env\n\ndef solution() -> Callable[[Context, str], str]:\n", "example_id": "153", "test": "greet = solution()\nenv = setup_environment('greet',greet)\ntemplate = env.from_string('''\n{{ 'World'| greet }}''')\nassertion_results = 'Hi, World!' in template.render(prefix='Hi')\nassert assertion_results \nassertion_results = 'Hello, World!' in template.render()\nassert assertion_results \n", "solution": "    @jinja2.contextfilter\n    def greet(ctx, name):\n        prefix = ctx.get('prefix', 'Hello')\n        return f'{prefix}, {name}!'\n        \n    return greet", "type_of_change": "name change", "name_of_class_or_func": "jinja2.contextfilter", "additional_dependencies": "markupsafe==2.0.1", "functional": "1", "webdev": "1", "solution_api_call": true, "api_calls": ["greet", "ctx.get"], "release_date": "2020-01", "docs": ["https://jinja.palletsprojects.com/en/stable/api/", "https://jinja.palletsprojects.com/en/stable/changes/"]}
{"python_version": "3.10", "library": "jinja2", "version": "3.1", "problem": "\nImplement the solution function to return a custom filter. This filter should: \naccept the context and a parameter name, \nretrieve a variable prefix from the context (defaulting to 'Hello' if not provided), \nreturn a greeting message combining the prefix and the name.. Do not run the app in your code.", "starting_code": "import jinja2 \nfrom jinja2.runtime import Context\nfrom typing import Callable\n\ndef setup_environment(filtername: str,filter) -> jinja2.Environment:\n    env = jinja2.Environment()\n    env.filters[filtername] = filter\n    return env\n\ndef solution() -> Callable[[Context, str], str]:\n", "example_id": "154", "test": "greet = solution()\nenv = setup_environment('greet',greet)\ntemplate = env.from_string('''\n{{ 'World'| greet }}''')\nassertion_results = 'Hi, World!' in template.render(prefix='Hi')\nassert assertion_results \nassertion_results = 'Hello, World!' in template.render()\nassert assertion_results \n", "solution": "    @jinja2.pass_context\n    def greet(ctx, name):\n        prefix = ctx.get('prefix', 'Hello')\n        return f'{prefix}, {name}!'\n        \n    return greet", "type_of_change": "name change", "name_of_class_or_func": "jinja2.pass_context", "additional_dependencies": "", "functional": "1", "webdev": "1", "solution_api_call": true, "api_calls": ["greet", "ctx.get"], "release_date": "2022-03", "docs": ["https://jinja.palletsprojects.com/en/stable/api/", "https://jinja.palletsprojects.com/en/stable/changes/"]}
{"python_version": "3.10", "library": "jinja2", "version": "2.11", "problem": "\nImplement the nl2br function which is a custom filter that takes the evaluation context \nand a text value, then replaces every occurrence of the substring 'Hello' \nwith the HTML string '<br>Hello</br>', while respecting the autoescape setting.\nYou can use the nl2br_core function. Do not run the app in your code.", "starting_code": "import re\nfrom jinja2 import Environment, evalcontextfilter\nfrom markupsafe import Markup, escape\nfrom jinja2.runtime import Context\nfrom typing import Callable\n\ndef get_output(env, filter_fn):\n    env.filters['nl2br'] = filter_fn\n    template = env.from_string('{{ text | nl2br }}')\n    output = template.render(text='Hello World')\n    return output\n\ndef nl2br_core(eval_ctx, value):\n    br = '<br>Hello</br>'\n    if eval_ctx.autoescape:\n        value = escape(value)\n        br = Markup(br)\n    result = re.sub(r'Hello', br, value)\n    return Markup(result) if eval_ctx.autoescape else result\n\ndef solution() -> Callable[[Context, str], str]:\n", "example_id": "155", "test": "nl2br = solution()\nenv = Environment(autoescape=True)\noutput = get_output(env,nl2br)\nexpected = '<br>Hello</br> World'\n\nassert output == expected", "solution": "    @evalcontextfilter\n    def nl2br(eval_ctx, value):\n        return nl2br_core(eval_ctx, value)\n\n    return nl2br\n", "type_of_change": "name change", "name_of_class_or_func": "jinja2.evalcontextfilter", "additional_dependencies": "markupsafe==2.0.1", "functional": "1", "webdev": "1", "solution_api_call": true, "api_calls": ["nl2br", "nl2br_core"], "release_date": "2020-01", "docs": ["https://jinja.palletsprojects.com/en/stable/api/", "https://jinja.palletsprojects.com/en/stable/changes/"]}
{"python_version": "3.10", "library": "jinja2", "version": "3.1", "problem": "\nWrite the nl2br function which is a custom filter that takes the evaluation context \nand a text value, then replaces every occurrence of the substring 'Hello' \nwith the HTML string '<br>Hello</br>', while respecting the autoescape setting.\nYou can use the nl2br_core function. Do not run the app in your code.", "starting_code": "import re\nfrom jinja2 import Environment, pass_eval_context\nfrom markupsafe import Markup, escape\nfrom typing import Callable, Union\nfrom jinja2.runtime import EvalContext\n\ndef get_output(env, filter_fn):\n    env.filters['nl2br'] = filter_fn\n    template = env.from_string('{{ text | nl2br }}')\n    output = template.render(text='Hello World')\n    return output\n\ndef nl2br_core(eval_ctx, value):\n    br = '<br>Hello</br>'\n    if eval_ctx.autoescape:\n        value = escape(value)\n        br = Markup(br)\n    result = re.sub(r'Hello', br, value)\n    return Markup(result) if eval_ctx.autoescape else result\n\ndef solution() -> Callable[[EvalContext, str], Markup | str]:", "example_id": "156", "test": "\nnl2br_filter = solution()\n\nenv = Environment(autoescape=True)\noutput = get_output(env,nl2br_filter)\nexpected = '<br>Hello</br> World'\n\nassert output == expected\n", "solution": "\n    @pass_eval_context\n    def nl2br(eval_ctx, value):\n        return nl2br_core(eval_ctx, value)\n    \n    return nl2br", "type_of_change": "name change", "name_of_class_or_func": "jinja2.pass_eval_context", "additional_dependencies": "", "functional": "1", "webdev": "1", "solution_api_call": true, "api_calls": ["nl2br", "nl2br_core"], "release_date": "2022-03", "docs": ["https://jinja.palletsprojects.com/en/stable/api/", "https://jinja.palletsprojects.com/en/stable/changes/"]}
{"python_version": "3.10", "library": "scipy", "version": "1.11.1", "problem": "\ncomplete the following function that check if all the batch of matrices are invertible, using numpy 1.25.1.", "starting_code": "import warnings\nfrom scipy.linalg import det\nimport numpy as np\nwarnings.filterwarnings('error')\n\ndef check_invertibility(matrices: np.ndarray) -> np.bool_:", "example_id": "157", "test": "\nmatrices = np.array([\n    [[1, 2],\n     [3, 4]],\n    \n    [[0, 1],\n     [1, 0]],\n    \n    [[2, 0],\n     [0, 2]]\n])\nassertion_value = check_invertibility(matrices)\nassert assertion_value\nmatrices = np.array([\n    [[1, 2],\n     [3, 4]],\n    \n    [[0, 1],\n     [1, 0]],\n    \n    [[2, 0],\n     [0, 2]],\n\n    [[0, 0],\n     [0, 0]]\n])\nassertion_value = not check_invertibility(matrices)\nassert assertion_value", "solution": "\n    return np.all(det(matrices))\n", "type_of_change": "argument or attribute change", "name_of_class_or_func": "linalg.det", "additional_dependencies": "numpy==1.25.1", "functional": "1", "webdev": "0", "solution_api_call": true, "api_calls": ["numpy.all", "scipy.linalg.det"], "release_date": "2023-06", "docs": ["https://docs.scipy.org/doc/scipy/release/1.11.1-notes.html"]}
{"python_version": "3.10", "library": "scipy", "version": "1.9.1", "problem": "\ncomplete the following function that check if all the batch of matrices are invertible, using numpy 1.21.6", "starting_code": "import warnings\nfrom scipy.linalg import det\nimport numpy as np\nwarnings.filterwarnings('error')\n\ndef check_invertibility(matrices : np.ndarray) -> np.bool_ :", "example_id": "158", "test": "\nmatrices = np.array([\n    [[1, 2],\n     [3, 4]],\n    \n    [[0, 1],\n     [1, 0]],\n    \n    [[2, 0],\n     [0, 2]]\n])\nassertion_value = check_invertibility(matrices)\nassert assertion_value\nmatrices = np.array([\n    [[1, 2],\n     [3, 4]],\n    \n    [[0, 1],\n     [1, 0]],\n    \n    [[2, 0],\n     [0, 2]],\n\n    [[0, 0],\n     [0, 0]]\n])\nassertion_value = not check_invertibility(matrices)\nassert assertion_value", "solution": "\n    return np.alltrue([det(A) for A in matrices])\n", "type_of_change": "argument or attribute change", "name_of_class_or_func": "linalg.det", "additional_dependencies": "numpy==1.21.6", "functional": "1", "webdev": "0", "solution_api_call": true, "api_calls": ["numpy.alltrue", "scipy.linalg.det"], "release_date": "2022-10", "docs": ["https://docs.scipy.org/doc/scipy/release/1.9.1-notes.html"]}
{"python_version": "3.10", "library": "scipy", "version": "1.11.1", "problem": "\nComplete the function count_unique_hmean that takes a 2D numpy array as \ninput and returns the number of unique harmonic mean values across \nthe rows of the array, counting each nan value as unique. We are using numpy 1.25.1", "starting_code": "import numpy as np\nfrom scipy.stats import hmean\n\ndef count_unique_hmean(data: np.ndarray) -> int:\n    # data shape: (n, m)\n    # n: number of arrays\n    # m: number of elements in each array ", "example_id": "159", "test": "\ndata = np.array([\n    [1, 2, 3],\n    [2, 2, 2],\n    [1, np.nan, 3],\n    [4, 5, 6],\n    [np.nan, 1, np.nan],\n    [1, 2, 3]\n])\nassertion_value = count_unique_hmean(data) == 5\nassert assertion_value\n", "solution": "\n    hmean_values = hmean(np.asarray(data), axis=1)\n    unique_vals = np.unique(hmean_values, equal_nan=False).shape[0]\n    return unique_vals\n\n", "type_of_change": "output change", "name_of_class_or_func": "stats.hmean", "additional_dependencies": "numpy==1.25.1", "functional": "1", "webdev": "0", "solution_api_call": true, "api_calls": ["scipy.stats.hmean", "numpy.unique", "numpy.asarray"], "release_date": "2023-06", "docs": ["https://docs.scipy.org/doc/scipy/release/1.11.1-notes.html"]}
{"python_version": "3.10", "library": "scipy", "version": "1.8.1", "problem": "\nComplete the function count_unique_hmean that takes a 2D numpy array as \ninput and returns the number of unique harmonic mean values across \nthe rows of the array, counting each nan value as unique. We are using numpy 1.21.6", "starting_code": "import numpy as np\nfrom scipy.stats import hmean\n\ndef count_unique_hmean(data: np.ndarray) -> int:\n    # data shape: (n, m)\n    # n: number of arrays\n    # m: number of elements in each array ", "example_id": "160", "test": "\n\ndata = np.array([\n    [1, 2, 3],\n    [2, 2, 2],\n    [1, np.nan, 3],\n    [4, 5, 6],\n    [np.nan, 1, np.nan],\n    [1, 2, 3]\n])\nassertion_value = count_unique_hmean(data) == 5\nassert assertion_value\n", "solution": "\n    hmean_values = []\n    for arr in data:\n        if np.isnan(arr).any():\n            hm = np.nan\n        else:\n            hm = hmean(arr)\n        hmean_values.append(hm)\n    \n    hmean_values = np.asarray(hmean_values)\n    non_nan_vals = hmean_values[~np.isnan(hmean_values)]\n    counts_non_nan = np.unique(non_nan_vals).shape[0]\n    nan_count = np.sum(np.isnan(hmean_values))\n    return counts_non_nan + nan_count\n", "type_of_change": "output change", "name_of_class_or_func": "stats.hmean", "additional_dependencies": "numpy==1.21.6", "functional": "1", "webdev": "0", "solution_api_call": true, "api_calls": ["scipy.stats.hmean", "hmean_values.append", "numpy.isnan", "numpy.unique", "numpy.sum", "numpy.asarray", "any"], "release_date": "2022-05", "docs": ["https://docs.scipy.org/doc/scipy/release/1.8.1-notes.html"]}
{"python_version": "3.10", "library": "scipy", "version": "1.11.1", "problem": "\nComplete the function compute_hilbert_transform. We are using numpy 1.25.1", "starting_code": "import numpy as np\nfrom scipy.signal import hilbert\n\ndef compute_hilbert_transform(a, b, dtype=np.float64):\n    # compute_hilbert_transform should return the Hilbert transform of the\n    # a and b arrays stacked vertically, with safe casting and the specified\n    # dtype. \n    # raise TypeError if needed\n    ", "example_id": "161", "test": "\n\na = np.array([1.0, 2.0, 3.0], dtype=np.float32)\nb = np.array([4.0, 5.0, 6.0], dtype=np.float64)\nassertion_value = False\ntry :\n    compute_hilbert_transform(a, b, dtype=np.float32)\nexcept TypeError:\n    assertion_value = True\nassert assertion_value\nb=b.astype(np.float32)\ncomputed = compute_hilbert_transform(a, b, dtype=np.float32)\nexpected = hilbert(np.vstack([a.astype(np.float64), b.astype(np.float64)])).astype(dtype=np.complex64)\nassertion_value = np.allclose(computed, expected) & (computed.dtype == np.complex64)\nassert assertion_value\na=a.astype(np.float64)\nb=b.astype(np.float64)\ncomputed = compute_hilbert_transform(a, b, dtype=np.float64)\nexpected = expected.astype(dtype=np.complex128)\nassertion_value = np.allclose(computed, expected) & (computed.dtype == np.complex128)\nassert assertion_value\n", "solution": "\n    stacked = np.vstack((a, b), dtype=dtype,\n                         casting='safe')\n    return hilbert(stacked)\n", "type_of_change": "argument or attribute change", "name_of_class_or_func": "signal.hilbert", "additional_dependencies": "numpy==1.25.1", "functional": "1", "webdev": "0", "solution_api_call": true, "api_calls": ["scipy.signal.hilbert", "numpy.vstack"], "release_date": "2023-06", "docs": ["https://docs.scipy.org/doc/scipy/release/1.11.1-notes.html"]}
{"python_version": "3.10", "library": "scipy", "version": "1.8.1", "problem": "\nComplete the function compute_hilbert_transform. We are using numpy 1.25.1", "starting_code": "import numpy as np\nfrom scipy.signal import hilbert\n\ndef compute_hilbert_transform(a: np.ndarray, b: np.ndarray, dtype=np.float64) -> np.ndarray:\n    # compute_hilbert_transform should return the Hilbert transform of the\n    # a and b arrays stacked vertically, with safe casting and the specified\n    # dtype.\n    # raise TypeError if needed\n    ", "example_id": "162", "test": "\na = np.array([1.0, 2.0, 3.0], dtype=np.float32)\nb = np.array([4.0, 5.0, 6.0], dtype=np.float64)\nassertion_value = False\ntry :\n    compute_hilbert_transform(a, b, dtype=np.float32)\nexcept TypeError:\n    assertion_value = True\nassert assertion_value\nb=b.astype(np.float32)\ncomputed = compute_hilbert_transform(a, b, dtype=np.float32)\nexpected = hilbert(np.vstack([a.astype(np.float64), b.astype(np.float64)])).astype(dtype=np.complex64)\nassertion_value = np.allclose(computed, expected) & (computed.dtype == np.complex64)\nassert assertion_value\n\na=a.astype(np.float64)\nb=b.astype(np.float64)\ncomputed = compute_hilbert_transform(a, b, dtype=np.float64)\nexpected = expected.astype(dtype=np.complex128)\nassertion_value = np.allclose(computed, expected) & (computed.dtype == np.complex128)\nassert assertion_value\n", "solution": "\n    if not (np.can_cast(a.dtype, dtype, casting='safe') and np.can_cast(b.dtype, dtype, casting='safe')):\n        raise TypeError('Unsafe casting from input dtype to specified dtype')\n    \n    a_cast = a.astype(dtype, copy=False)\n    b_cast = b.astype(dtype, copy=False)\n    \n    stacked = np.vstack((a_cast, b_cast))\n    \n    result = hilbert(stacked)\n    \n    if dtype == np.float32:\n        complex_dtype = np.complex64\n    elif dtype == np.float64:\n        complex_dtype = np.complex128\n    else:\n        complex_dtype = np.complex128\n\n    return result.astype(complex_dtype)\n    ", "type_of_change": "argument or attribute change", "name_of_class_or_func": "signal.hilbert", "additional_dependencies": "numpy==1.21.6", "functional": "1", "webdev": "0", "solution_api_call": true, "api_calls": ["b.astype", "TypeError", "scipy.signal.hilbert", "numpy.vstack", "a.astype", "not", "numpy.can_cast", "result.astype"], "release_date": "2022-05", "docs": ["https://docs.scipy.org/doc/scipy/release/1.8.1-notes.html"]}
{"python_version": "3.10", "library": "flask", "version": "2.0.0", "problem": " \nComplete the app set-up for the json encoding to return only the unique values (each NaN being a different value) contained \nin the numpy array when called, we are using numpy 1.21.6. Do not run the app in your code.", "starting_code": "import flask\nimport json\nimport numpy as np\napp = flask.Flask('test1')\n@app.route('/data')\ndef data(num_arr):\n    return flask.jsonify({'numbers': num_arr})\n\ndef eval(app, data_fn, num_arr):\n    with app.test_request_context():\n        response = data_fn(num_arr)\n        return response.get_data(as_text=False)\n\nclass MyCustomJSONHandler(json.JSONEncoder):\n    def default(self, obj: object) -> object:\n        if isinstance(obj, np.ndarray):", "example_id": "163", "test": "\napp2 = flask.Flask('test2')\n@app2.route('/data2')\ndef data2(num_arr):\n    return flask.jsonify({'numbers': num_arr})\nclass MyCustomJSONHandler2(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, np.ndarray):\n            n_nan = np.sum(np.isnan(obj))\n            unique_vals = obj[~np.isnan(obj)]\n            unique_vals = np.append(np.unique(unique_vals), [np.nan]*n_nan).tolist()\n            return unique_vals\n        return super().default(obj)\n\napp2.json_encoder = MyCustomJSONHandler2\nassertion_results = eval(app2, data2,np.array([3, 3, 1, np.nan, 2, 6, 5, np.nan])) == eval(app, data,np.array([3, 3, 1, np.nan, 2, 6, 5, np.nan]))\nassert assertion_results\n", "solution": "\n            n_nan = np.sum(np.isnan(obj))\n            unique_vals = obj[~np.isnan(obj)]\n            unique_vals = np.append(np.unique(unique_vals), [np.nan]*n_nan).tolist()\n            return unique_vals\n        return super().default(obj)\n\napp.json_encoder = MyCustomJSONHandler\n", "type_of_change": "argument or attribute change", "name_of_class_or_func": "app.json_encoder", "additional_dependencies": "numpy==1.21.6 werkzeug==2.0.0", "functional": "0", "webdev": "1", "solution_api_call": true, "api_calls": ["numpy.append", "default", "numpy.unique", "numpy.isnan", "numpy.sum", "tolist", "super"], "release_date": "2021-05", "docs": ["https://flask.palletsprojects.com/en/stable/api/", "https://numpy.org/devdocs/release/1.21.0-notes.html", "https://flask.palletsprojects.com/en/stable/changes/"]}
{"python_version": "3.10", "library": "flask", "version": "3.0.0", "problem": " \nComplete the app set-up for the json encoding to return only the unique values (each NaN being a different value) contained \nin the numpy array when called, we are using numpy 1.25.1. Do not run the app in your code.", "starting_code": "import flask\nimport numpy as np\n\napp = flask.Flask('test1')\n\n@app.route('/data')\ndef data(num_arr):\n    return flask.jsonify({'numbers': num_arr})\n\ndef eval_app(app, data_fn, num_arr):\n    with app.test_request_context():\n        response = data_fn(num_arr)\n        return response.get_data(as_text=True)\n\nclass MyCustomJSONHandler(flask.json.provider.DefaultJSONProvider):\n    def default(self, obj: object) -> object:\n    ", "example_id": "164", "test": "\napp2 = flask.Flask('test2')\n\n@app2.route('/data2')\ndef data2(num_arr):\n    return flask.jsonify({'numbers': num_arr})\n\nclass MyCustomJSONHandler2(flask.json.provider.DefaultJSONProvider):\n    def default(self, obj):\n        if isinstance(obj, np.ndarray):\n            n_nan = np.sum(np.isnan(obj))\n            unique_vals = obj[~np.isnan(obj)]\n            unique_vals = np.append(np.unique(unique_vals), [np.nan]*n_nan).tolist()\n            return unique_vals\n        return super().default(obj)\n\napp2.json_provider_class = MyCustomJSONHandler2\napp2.json = app2.json_provider_class(app2)\n\nassertion_results = eval_app(app2, data2,np.array([3, 3, 1, np.nan, 2, 6, 5, np.nan])) == eval_app(app, data,np.array([3, 3, 1, np.nan, 2, 6, 5, np.nan]))\nassert assertion_results\n", "solution": "\n        if isinstance(obj, np.ndarray):\n            unique_vals = np.unique(obj, equal_nan=False)\n            return unique_vals.tolist()\n        return super().default(obj)\n\napp.json_provider_class = MyCustomJSONHandler\napp.json = app.json_provider_class(app)\n", "type_of_change": "argument or attribute change", "name_of_class_or_func": "app.json_encoder", "additional_dependencies": "numpy==1.25.1", "functional": "0", "webdev": "1", "solution_api_call": true, "api_calls": ["default", "isinstance", "numpy.unique", "unique_vals.tolist", "super", "app.json_provider_class"], "release_date": "2023-09", "docs": ["https://flask.palletsprojects.com/en/stable/api/", "https://numpy.org/doc/2.2/release/1.25.0-notes.html", "https://flask.palletsprojects.com/en/stable/changes/"]}
{"python_version": "3.10", "library": "flask", "version": "2.0.0", "problem": " \nComplete the app set-up for the json encoding to perform a fast copy and \ntranspose when given a numpy array before flattening and converting \nthe result to a list,we are using numpy 1.21.6. Do not run the app in your code.", "starting_code": "import flask\nimport json\nimport numpy as np\nfrom numpy import fastCopyAndTranspose \napp = flask.Flask('test1')\n@app.route('/data')\ndef data(num_arr):\n    return flask.jsonify({'numbers': num_arr})\n\ndef eval(app, data_fn, num_arr):\n    with app.test_request_context():\n        response = data_fn(num_arr)\n        return response.get_data(as_text=False)\n\nclass MyCustomJSONHandler(json.JSONEncoder):\n    def default(self, obj: object) -> object:\n        if isinstance(obj, np.ndarray):", "example_id": "165", "test": "\napp2 = flask.Flask('test2')\n@app2.route('/data2')\ndef data2(num_arr):\n    return flask.jsonify({'numbers': num_arr})\nclass MyCustomJSONHandler2(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, np.ndarray):\n            res = obj.T.copy().flatten().tolist()\n            return res\n        return super().default(obj)\n\napp2.json_encoder = MyCustomJSONHandler2\nassertion_results = eval(app2, data2,np.array([[3, 3, 1,], [2,2,4],[1,1,1]])) == eval(app, data,np.array([[3, 3, 1,], [2,2,4],[1,1,1]]))\nassert assertion_results\n", "solution": "\n            res = fastCopyAndTranspose(obj).flatten().tolist()\n            return res\n        return super().default(obj)\n\napp.json_encoder = MyCustomJSONHandler\n\n", "type_of_change": "argument or attribute change", "name_of_class_or_func": "app.json_encoder", "additional_dependencies": "numpy==1.21.6 werkzeug==2.0.0", "functional": "0", "webdev": "1", "solution_api_call": true, "api_calls": ["default", "numpy.fastCopyAndTranspose", "flatten", "tolist", "super"], "release_date": "2021-05", "docs": ["https://flask.palletsprojects.com/en/stable/api/", "https://numpy.org/devdocs/release/1.24.0-notes.html", "https://flask.palletsprojects.com/en/stable/changes/"]}
{"python_version": "3.10", "library": "flask", "version": "3.0.0", "problem": " \nComplete the app set-up for the json encoding to perform a fast copy and \ntranspose when given a numpy array before flattening and converting \nthe result to a list, we are using numpy 1.25.1. Do not run the app in your code.", "starting_code": "import flask\nimport numpy as np\nimport warnings\nfrom numpy import fastCopyAndTranspose \nwarnings.filterwarnings('error')\n\napp = flask.Flask('test1')\n\n@app.route('/data')\ndef data(num_list):\n    return flask.jsonify({'numbers': num_list})\n\ndef eval_app(app, data_fn, num_arr):\n    with app.test_request_context():\n        response = data_fn(num_arr)\n        return response.get_data(as_text=True)\n\nclass MyCustomJSONHandler(flask.json.provider.DefaultJSONProvider):\n    def default(self, obj: object) -> object:\n        if isinstance(obj, np.ndarray):", "example_id": "166", "test": "\napp2 = flask.Flask('test2')\n\n@app2.route('/data2')\ndef data2(num_arr):\n    return flask.jsonify({'numbers': num_arr})\n\nclass MyCustomJSONHandler2(flask.json.provider.DefaultJSONProvider):\n    def default(self, obj):\n        if isinstance(obj, np.ndarray):\n            res = obj.T.copy().flatten().tolist()\n            return res\n        return super().default(obj)\n\napp2.json_provider_class = MyCustomJSONHandler2\napp2.json = app2.json_provider_class(app2)\n\nassertion_results = eval_app(app2, data2,np.array([[3, 3, 1,], [2,2,4],[1,1,1]])) == eval_app(app, data,np.array([[3, 3, 1,], [2,2,4],[1,1,1]]))\nassert assertion_results\n", "solution": "\n            res = obj.T.copy().flatten().tolist()\n            return res\n        return super().default(obj)\n\napp.json_provider_class = MyCustomJSONHandler\napp.json = app.json_provider_class(app)\n", "type_of_change": "argument or attribute change", "name_of_class_or_func": "app.json_encoder", "additional_dependencies": "numpy==1.25.1", "functional": "0", "webdev": "1", "solution_api_call": true, "api_calls": ["default", "obj.T.copy", "flatten", "tolist", "super", "app.json_provider_class"], "release_date": "2023-09", "docs": ["https://flask.palletsprojects.com/en/stable/api/", "https://numpy.org/doc/2.2/release/1.25.0-notes.html", "https://flask.palletsprojects.com/en/stable/changes/"]}
{"python_version": "3.10", "library": "flask", "version": "2.0.0", "problem": " \nComplete the stack_and_save function, we are using numpy 1.21.6. Do not run the app in your code.", "starting_code": "import flask\nimport werkzeug\nimport numpy as np\n\nerror404 = werkzeug.exceptions.NotFound\n\ndef stack_and_save(arr_list: list[np.ndarray],base_path : str,sub_path : str, casting_policy: str, out_dtype: type) -> tuple[str, np.ndarray]:\n    # Attempt to join the base path and sub path.\n    # If the joined path is outside the base path, raise a 404 error.\n    # stack the arrays in arr_list with the casting policy and the out_dtype.\n    # if the out_dtype is not compatible with the casting policy, raise a TypeError\n    # and out_dtype could be np.float32 or np.float64\n    # casting policy could be safe or unsafe\n    # Return the joined path and the stacked array to be saved ", "example_id": "167", "test": "\n\nbase_path = '/var/www/myapp'\nsub_path = '../secret.txt'\na = np.array([[1, 2, 3], [4, 5, 6]]).astype(np.float32)\nb = np.array([[7, 8, 9], [10, 11, 12]]).astype(np.float64)\narr_list = [a, b]\ncasting_policy = 'safe'\nout_dtype=np.float64\nstacked_correct = np.vstack(arr_list).astype(np.float64)\ntry : \n    joined, stacked = stack_and_save(arr_list,base_path, sub_path,  casting_policy,out_dtype)\nexcept werkzeug.exceptions.NotFound as e:\n    assertion_result = True\nelse:\n    assertion_result = False\nassert assertion_result\n\nbase_path = '/var/www/myapp'\nsub_path = 'secret.txt'\njoined,stacked = stack_and_save(arr_list,base_path, sub_path,  casting_policy,out_dtype)\nassertion_result = joined == '/var/www/myapp/secret.txt' and np.array_equal(stacked,stacked_correct) and stacked.dtype == np.float64\nassert assertion_result\n\nstacked_correct = stacked_correct.astype(np.float32)\nout_dtype=np.float32\ntry : \n    joined,stacked = stack_and_save(arr_list,base_path, sub_path,  casting_policy,out_dtype)\nexcept TypeError as e:\n    assertion_result = True\nelse:\n    assertion_result = False\nassert assertion_result\n\nstacked_correct = stacked_correct.astype(np.float32)\nout_dtype=np.float32\ncasting_policy = 'unsafe'\njoined,stacked = stack_and_save(arr_list,base_path, sub_path,  casting_policy,out_dtype)\n\nassertion_result = joined == '/var/www/myapp/secret.txt' and np.array_equal(stacked,stacked_correct) and stacked.dtype == np.float32\nassert assertion_result\n", "solution": "\n    joined = flask.safe_join(base_path, sub_path)\n    casted_list = []\n\n    for arr in arr_list:\n        if not np.can_cast(arr.dtype, out_dtype, casting=casting_policy):\n            raise TypeError('Cannot cast array')\n        casted_list.append(arr.astype(out_dtype, copy=False))\n    \n    stacked = np.vstack(casted_list)\n    return joined, stacked\n", "type_of_change": "name change", "name_of_class_or_func": "flask.safe_join", "additional_dependencies": "numpy==1.21.6 werkzeug==2.0.0", "functional": "1", "webdev": "1", "solution_api_call": true, "api_calls": ["flask.safe_join", "TypeError", "arr.astype", "numpy.vstack", "casted_list.append", "numpy.can_cast"], "release_date": "2021-05", "docs": ["https://tedboy.github.io/flask/werk_doc.tutorial.html", "https://flask.palletsprojects.com/en/stable/changes/", "https://numpy.org/devdocs/release/1.21.0-notes.html"]}
{"python_version": "3.10", "library": "flask", "version": "3.0.0", "problem": " \nComplete the stack_and_save function, we are using numpy 1.25.1. Do not run the app in your code.", "starting_code": "import flask\nimport werkzeug\nimport numpy as np\n\nerror404 = werkzeug.exceptions.NotFound\n\ndef stack_and_save(arr_list: list[np.ndarray],base_path : str,sub_path : str, casting_policy: str, out_dtype: type) -> tuple[str, np.ndarray]:\n    # Attempt to join the base path and sub path.\n    # If the joined path is outside the base path, raise a 404 error.\n    # stack the arrays in arr_list with the casting policy and the out_dtype.\n    # if the out_dtype is not compatible with the casting policy, raise a TypeError\n    # and out_dtype could be np.float32 or np.float64\n    # casting policy could be safe or unsafe\n    # Return the joined path and the stacked array to be saved ", "example_id": "168", "test": "\nbase_path = '/var/www/myapp'\nsub_path = '../secret.txt'\n\n\na = np.array([[1, 2, 3], [4, 5, 6]]).astype(np.float32)\nb = np.array([[7, 8, 9], [10, 11, 12]]).astype(np.float64)\narr_list = [a, b]\ncasting_policy = 'safe'\nout_dtype=np.float64\nstacked_correct = np.vstack(arr_list).astype(np.float64)\ntry : \n    joined, stacked = stack_and_save(arr_list,base_path, sub_path,  casting_policy,out_dtype)\nexcept werkzeug.exceptions.NotFound as e:\n    assertion_result = True\nelse:\n    assertion_result = False\nassert assertion_result\n\nbase_path = '/var/www/myapp'\nsub_path = 'secret.txt'\njoined,stacked = stack_and_save(arr_list,base_path, sub_path,  casting_policy,out_dtype)\nassertion_result = joined == '/var/www/myapp/secret.txt' and np.array_equal(stacked,stacked_correct) and stacked.dtype == np.float64\nassert assertion_result\n\nstacked_correct = stacked_correct.astype(np.float32)\nout_dtype=np.float32\ntry : \n    joined,stacked = stack_and_save(arr_list,base_path, sub_path,  casting_policy,out_dtype)\nexcept TypeError as e:\n    assertion_result = True\nelse:\n    assertion_result = False\nassert assertion_result\n\nstacked_correct = stacked_correct.astype(np.float32)\nout_dtype=np.float32\ncasting_policy = 'unsafe'\njoined,stacked = stack_and_save(arr_list,base_path, sub_path,  casting_policy,out_dtype)\n\nassertion_result = joined == '/var/www/myapp/secret.txt' and np.array_equal(stacked,stacked_correct) and stacked.dtype == np.float32\nassert assertion_result\n", "solution": "\n    joined = werkzeug.utils.safe_join(base_path, sub_path)\n    if joined is None:\n        raise error404\n    stacked = np.vstack(arr_list,casting=casting_policy,dtype=out_dtype)\n    return joined, stacked\n", "type_of_change": "name change", "name_of_class_or_func": "flask.safe_join", "additional_dependencies": "numpy==1.25.1", "functional": "1", "webdev": "1", "solution_api_call": true, "api_calls": ["numpy.vstack", "werkzeug.utils.safe_join"], "release_date": "2023-09", "docs": ["https://tedboy.github.io/flask/werk_doc.tutorial.html", "https://flask.palletsprojects.com/en/stable/changes/", "https://numpy.org/doc/2.2/release/1.25.0-notes.html"]}
{"python_version": "3.10", "library": "flask", "version": "3.0.0", "problem": " \nComplete the app set-up so that, when given a batch of matrix,\nthe json encoding compute the determinants of each matrix, \nbefore flattening and converting the result to a list, we are using scipy 1.11.1. Do not run the app in your code.", "starting_code": "import flask\nimport numpy as np\nfrom scipy import linalg\n\napp = flask.Flask('test1')\n@app.route('/data')\ndef data(num_list):\n    return flask.jsonify({'numbers': num_list})\ndef eval_app(app, data_fn, num_arr):\n    with app.test_request_context():\n        response = data_fn(num_arr)\n        return response.get_data(as_text=True)\n\nclass MyCustomJSONHandler(flask.json.provider.DefaultJSONProvider):\n    def default(self, obj):\n        if isinstance(obj, np.ndarray) and len(obj.shape)==3 and obj.shape[-1]==obj.shape[-2] : ", "example_id": "169", "test": "\napp2 = flask.Flask('test2')\n\n@app2.route('/data2')\ndef data2(num_arr):\n    return flask.jsonify({'numbers': num_arr})\n\nclass MyCustomJSONHandler2(flask.json.provider.DefaultJSONProvider):\n    def default(self, obj):\n        if isinstance(obj, np.ndarray) and len(obj.shape)==3 and obj.shape[-1]==obj.shape[-2] :\n            res = np.zeros(obj.shape[0])\n            for i in range(obj.shape[0]):\n                res[i] = linalg.det(obj[i])\n            return res.tolist()\n        return super().default(obj)\n\napp2.json_provider_class = MyCustomJSONHandler2\napp2.json = app2.json_provider_class(app2)\na = np.random.random((6,3,3))\n\nassertion_results = eval_app(app2, data2,a) == eval_app(app, data,a)\nassert assertion_results", "solution": "\n\n            res = linalg.det(obj)\n            return res.tolist()\n        return super().default(obj)\n\napp.json_provider_class = MyCustomJSONHandler\napp.json = app.json_provider_class(app) ", "type_of_change": "argument or attribute change", "name_of_class_or_func": "app.json_encoder", "additional_dependencies": "scipy==1.11.1", "functional": "0", "webdev": "1", "solution_api_call": true, "api_calls": ["default", "scipy.linalg.det", "res.tolist", "super", "app.json_provider_class"], "release_date": "2023-09", "docs": ["https://flask.palletsprojects.com/en/stable/api/", "https://flask.palletsprojects.com/en/stable/changes/", "https://docs.scipy.org/doc/scipy/release/1.11.1-notes.html"]}
{"python_version": "3.10", "library": "flask", "version": "2.0.0", "problem": " \nComplete the app set-up so that, when given a batch of matrix,\nthe json encoding compute the determinants of each matrix, \nbefore flattening and converting the result to a list, we are using scipy 1.8.1. Do not run the app in your code.", "starting_code": "import flask\nimport json\nimport numpy as np\nfrom scipy import linalg\n\napp = flask.Flask('test1')\n@app.route('/data')\ndef data(num_arr):\n    return flask.jsonify({'numbers': num_arr})\n\ndef eval(app, data_fn, num_arr):\n    with app.test_request_context():\n        response = data_fn(num_arr)\n        return response.get_data(as_text=False)\n\nclass MyCustomJSONHandler(json.JSONEncoder):\n    def default(self, obj: object) -> object:\n        if isinstance(obj, np.ndarray) and len(obj.shape)==3 and obj.shape[-1]==obj.shape[-2] :", "example_id": "170", "test": "\napp2 = flask.Flask('test2')\n@app2.route('/data2')\ndef data2(num_arr):\n    return flask.jsonify({'numbers': num_arr})\nclass MyCustomJSONHandler2(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, np.ndarray) and len(obj.shape)==3 and obj.shape[-1]==obj.shape[-2] :\n            res = np.zeros(obj.shape[0])\n            for i in range(obj.shape[0]):\n                res[i] = linalg.det(obj[i])\n            return res.tolist()\n        return super().default(obj)\n\napp2.json_encoder = MyCustomJSONHandler2\na = np.random.random((6,3,3))\n\nassertion_results = eval(app2, data2,a) == eval(app, data,a)\nassert assertion_results\n", "solution": "\n            res = np.zeros(obj.shape[0])\n            for i in range(obj.shape[0]):\n                res[i] = linalg.det(obj[i])\n            return res.tolist()\n        return super().default(obj)\n\napp.json_encoder = MyCustomJSONHandler\n", "type_of_change": "argument or attribute change", "name_of_class_or_func": "app.json_encoder", "additional_dependencies": "scipy==1.8.1 Werkzeug==2.0.0", "functional": "0", "webdev": "1", "solution_api_call": true, "api_calls": ["default", "scipy.linalg.det", "numpy.zeros", "range", "res.tolist", "super"], "release_date": "2021-05", "docs": ["https://flask.palletsprojects.com/en/stable/api/", "https://flask.palletsprojects.com/en/stable/changes/", "https://docs.scipy.org/doc/scipy/release/1.8.1-notes.html"]}
{"python_version": "3.10", "library": "flask", "version": "3.0.0", "problem": " \nComplete the app set-up so that, when given a batch (first dim) of values,\ncompute the harmonic mean along the second dimension (It should handle nan values),\nbefore flattening and converting the result to a list, we are using scipy 1.11.1. Do not run the app in your code.", "starting_code": "import flask\nimport numpy as np\nfrom scipy.stats import hmean\n\napp = flask.Flask('test1')\n\n@app.route('/data')\ndef data(num_list):\n    return flask.jsonify({'numbers': num_list})\n\ndef eval_app(app, data_fn, num_arr):\n    with app.test_request_context():\n        response = data_fn(num_arr)\n        return response.get_data(as_text=True)\n\nclass MyCustomJSONHandler(flask.json.provider.DefaultJSONProvider):\n    def default(self, obj: object) -> object:\n        if isinstance(obj, np.ndarray):", "example_id": "171", "test": "\napp2 = flask.Flask('test2')\n\n@app2.route('/data2')\ndef data2(num_arr):\n    return flask.jsonify({'numbers': num_arr})\n\nclass MyCustomJSONHandler2(flask.json.provider.DefaultJSONProvider):\n    def default(self, obj):\n        if isinstance(obj, np.ndarray):\n            res = hmean(obj,axis=1).tolist()\n            return res\n        return super().default(obj)\n\napp2.json_provider_class = MyCustomJSONHandler2\napp2.json = app2.json_provider_class(app2)\nassertion_results = eval_app(app2, data2,np.array([[3, 3, np.nan,], [np.nan,2,4],[1,2,1]])) == eval_app(app, data,np.array([[3, 3, np.nan,], [np.nan,2,4],[1,2,1]]))\nassert assertion_results\n\n", "solution": "\n            res = hmean(obj,axis=1).tolist()\n            return res\n        return super().default(obj)\n\napp.json_provider_class = MyCustomJSONHandler\napp.json = app.json_provider_class(app)\n", "type_of_change": "argument or attribute change", "name_of_class_or_func": "app.json_encoder", "additional_dependencies": "scipy==1.11.1", "functional": "0", "webdev": "1", "solution_api_call": true, "api_calls": ["default", "scipy.stats.hmean", "tolist", "super", "app.json_provider_class"], "release_date": "2023-09", "docs": ["https://flask.palletsprojects.com/en/stable/api/", "https://flask.palletsprojects.com/en/stable/changes/", "https://docs.scipy.org/doc/scipy/release/1.11.1-notes.html"]}
{"python_version": "3.10", "library": "flask", "version": "2.0.0", "problem": " \nComplete the app set-up so that, when given a batch (first dim) of values,\ncompute the harmonic mean along the second dimension (It should handle nan values),\nbefore flattening and converting the result to a list, we are using scipy 1.8.1. Do not run the app in your code.", "starting_code": "import flask\nimport json\nimport numpy as np\nfrom scipy.stats import hmean\n\napp = flask.Flask('test1')\n@app.route('/data')\ndef data(num_arr):\n    return flask.jsonify({'numbers': num_arr})\n\ndef eval(app, data_fn, num_arr):\n    with app.test_request_context():\n        response = data_fn(num_arr)\n        return response.get_data(as_text=False)\n\nclass MyCustomJSONHandler(json.JSONEncoder):\n    def default(self, obj: object) -> object:\n        if isinstance(obj, np.ndarray):", "example_id": "172", "test": "\napp2 = flask.Flask('test2')\n@app2.route('/data2')\ndef data2(num_arr):\n    return flask.jsonify({'numbers': num_arr})\nclass MyCustomJSONHandler2(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, np.ndarray):\n            res = np.zeros((obj.shape[0],1))\n            for i_arr in range(obj.shape[0]):\n                if np.isnan(obj[i_arr]).any():\n                    res[i_arr] = np.nan\n                else:\n                    res[i_arr]  = hmean(obj[i_arr])\n            res = res.flatten().tolist()\n            return res\n        return super().default(obj)\n\napp2.json_encoder = MyCustomJSONHandler2\nassertion_results = eval(app2, data2,np.array([[3, 3, np.nan,], [np.nan,2,4],[1,2,1]])) == eval(app, data,np.array([[3, 3, np.nan,], [np.nan,2,4],[1,2,1]]))\nassert assertion_results\n", "solution": "\n            res = np.zeros((obj.shape[0],1))\n            for i_arr in range(obj.shape[0]):\n                if np.isnan(obj[i_arr]).any():\n                    res[i_arr] = np.nan\n                else:\n                    res[i_arr]  = hmean(obj[i_arr])\n            res = res.flatten().tolist()\n            return res\n        return super().default(obj)\n\napp.json_encoder = MyCustomJSONHandler\n", "type_of_change": "argument or attribute change", "name_of_class_or_func": "app.json_encoder", "additional_dependencies": "scipy==1.8.1 Werkzeug==2.0.0", "functional": "0", "webdev": "1", "solution_api_call": true, "api_calls": ["res.flatten", "tolist", "default", "scipy.stats.hmean", "numpy.zeros", "numpy.isnan", "range", "any", "super"], "release_date": "2021-05", "docs": ["https://flask.palletsprojects.com/en/stable/api/", "https://flask.palletsprojects.com/en/stable/changes/", "https://docs.scipy.org/doc/scipy/release/1.8.1-notes.html"]}
{"python_version": "3.10", "library": "flask", "version": "3.0.0", "problem": " \nComplete the save_exponential function, we are using scipy 1.11.1. Do not run the app in your code.", "starting_code": "import flask\nimport werkzeug\nfrom scipy import linalg\nimport numpy as np\n\nerror404 = werkzeug.exceptions.NotFound\n\ndef save_exponential(A: np.ndarray, base_path: str, sub_path: str) -> tuple[str, np.ndarray]:\n    # Attempt to join the base path and sub path.\n    # If the joined path is outside the base path, raise a 404 error.\n    # compute the exponential of the batched matrices (m, m) in A (n,m,m)\n    # return the save_path and the exponential of the matrices\n    ", "example_id": "173", "test": "\nbase_path = '/var/www/myapp'\nsub_path = '../secret.txt'\nimport numpy as np\n\na = np.random.random((4,3,3))\nexpected = np.zeros(a.shape)\nfor i in range(expected.shape[0]):\n    expected[i] = linalg.expm(a[i])\n\ntry : \n    joined, results = save_exponential(a,base_path, sub_path)\nexcept werkzeug.exceptions.NotFound as e:\n    assertion_result = True\nelse:\n    assertion_result = False\nassert assertion_result\n\nbase_path = '/var/www/myapp'\nsub_path = 'secret.txt'\n\njoined, results = save_exponential(a,base_path, sub_path)\nassertion_result = joined == '/var/www/myapp/secret.txt' and np.allclose(results, expected)\nassert assertion_result\n", "solution": "\n    joined = werkzeug.utils.safe_join(base_path, sub_path)\n    if joined is None:\n        raise error404\n    output = linalg.expm(A)\n    return joined, output\n", "type_of_change": "name change", "name_of_class_or_func": "flask.safe_join", "additional_dependencies": "scipy==1.11.1", "functional": "1", "webdev": "1", "solution_api_call": true, "api_calls": ["scipy.linalg.expm", "werkzeug.utils.safe_join"], "release_date": "2023-09", "docs": ["https://tedboy.github.io/flask/werk_doc.tutorial.html", "https://flask.palletsprojects.com/en/stable/changes/", "https://docs.scipy.org/doc/scipy/release/1.11.1-notes.html"]}
{"python_version": "3.10", "library": "flask", "version": "2.0.0", "problem": " \nComplete the save_exponential function, we are using scipy 1.8.1. Do not run the app in your code.", "starting_code": "import flask\nimport werkzeug\nfrom scipy import linalg\nimport numpy as np\n\nerror404 = werkzeug.exceptions.NotFound\n\ndef save_exponential(A: np.ndarray, base_path: str, sub_path: str) -> tuple[str, np.ndarray]:\n    # Attempt to join the base path and sub path.\n    # If the joined path is outside the base path, raise a 404 error.\n    # compute the exponential of the batched matrices (m, m) in A (n,m,m)\n    # return the save_path and the exponential of the matrices\n    ", "example_id": "174", "test": "\nbase_path = '/var/www/myapp'\nsub_path = '../secret.txt'\nimport numpy as np\n\na = np.random.random((4,3,3))\nexpected = np.zeros(a.shape)\nfor i in range(expected.shape[0]):\n    expected[i] = linalg.expm(a[i])\n\ntry : \n    joined, results = save_exponential(a,base_path, sub_path)\nexcept werkzeug.exceptions.NotFound as e:\n    assertion_result = True\nelse:\n    assertion_result = False\nassert assertion_result\n\nbase_path = '/var/www/myapp'\nsub_path = 'secret.txt'\n\njoined, results = save_exponential(a,base_path, sub_path)\nassertion_result = joined == '/var/www/myapp/secret.txt' and np.allclose(results, expected)\nassert assertion_result\n", "solution": "\n    joined = flask.safe_join(base_path, sub_path)\n    output = np.zeros(A.shape)\n    for i in range(A.shape[0]):\n        output[i] = linalg.expm(A[i])\n    return joined, output\n", "type_of_change": "name change", "name_of_class_or_func": "flask.safe_join", "additional_dependencies": "scipy==1.8.1 Werkzeug==2.0.0", "functional": "1", "webdev": "1", "solution_api_call": true, "api_calls": ["range", "numpy.zeros", "scipy.linalg.expm", "flask.safe_join"], "release_date": "2021-05", "docs": ["https://tedboy.github.io/flask/werk_doc.tutorial.html", "https://flask.palletsprojects.com/en/stable/changes/", "https://docs.scipy.org/doc/scipy/release/1.8.1-notes.html"]}
{"python_version": "3.9", "library": "sympy", "version": "1.9", "problem": "Create a function named custom_generateRandomSampleDice that accepts two parameters: a die object and an integer X. The function should generate and return a list containing X random outcomes sampled from the given die", "starting_code": "from typing import List\nfrom sympy.stats import Die, sample\nimport sympy.stats.rv \n\ndef custom_generateRandomSampleDice(dice: sympy.stats.rv.RandomSymbol, X: int) -> List[int]:\n    return ", "example_id": "175", "test": "\ndice = Die('X', 6)\nimport warnings\nfrom sympy.utilities.exceptions import SymPyDeprecationWarning\n\ndef test_custom_generateRandomSampleDice():\n    with warnings.catch_warnings(record=True) as w:\n        warnings.simplefilter(\"always\", SymPyDeprecationWarning)  # Capture all warnings\n        output = custom_generateRandomSampleDice(dice, 3)\n        assert isinstance(output, list), \"Test Failed: Output is not a list!\"\n        assert len(output) == 3, \"Test Failed: Output length does not match expected!\"\n        assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), \"Test Failed: Deprecation warning was triggered!\"\n\ntest_custom_generateRandomSampleDice()", "solution": "[sample(dice) for i in range(X)]", "type_of_change": "new func/method/class", "name_of_class_or_func": "stats.sample", "additional_dependencies": "scipy==1.8.0", "docs": ["https://docs.sympy.org/latest/modules/stats.html", "https://docs.sympy.org/latest/explanation/active-deprecations.html"], "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["range", "sample"], "release_date": "2021-06"}
{"python_version": "3.9", "library": "sympy", "version": "1.9", "problem": "Create a custom_computeDFT function that accepts an integer parameter n. This function should  compute the Discrete Fourier Transform (DFT) matrix of size  n times n and show it explicitly.", "starting_code": "import sympy\nfrom sympy.matrices.expressions.fourier import DFT\n\ndef custom_computeDFT(n: int) -> sympy.ImmutableDenseMatrix:\n    return ", "example_id": "176", "test": "\nimport warnings\nfrom sympy.utilities.exceptions import SymPyDeprecationWarning\nfrom sympy import Matrix, I, Rational\n\ndef test_custom_computeDFT():\n    with warnings.catch_warnings(record=True) as w:\n        warnings.simplefilter(\"always\", SymPyDeprecationWarning)  # Capture all warnings\n        output = custom_computeDFT(4)\n        expect = Matrix([\n            [Rational(1,2), Rational(1,2), Rational(1,2), Rational(1,2)],\n            [Rational(1,2), -I/2, -Rational(1,2), I/2],\n            [Rational(1,2), -Rational(1,2), Rational(1,2), -Rational(1,2)],\n            [Rational(1,2), I/2, -Rational(1,2), -I/2]\n        ])\n\n        assert output == expect\n        assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), \"Test Failed: Deprecation warning was triggered!\"\n\ntest_custom_computeDFT()", "solution": "DFT(n).as_explicit()", "type_of_change": "new func/method/class", "name_of_class_or_func": "sympy.physics.matrices.mdft", "additional_dependencies": "", "docs": ["https://docs.sympy.org/latest/modules/physics/matrices.html", "https://docs.sympy.org/latest/explanation/active-deprecations.html"], "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["sympy.matrices.expressions.fourier.DFT", "as_explicit"], "release_date": "2021-06"}
{"python_version": "3.9", "library": "sympy", "version": "1.9", "problem": "Create a function named custom_laplace_transform that takes two parameters, t and z. This function should compute the Laplace transform of the 22 identity matrix (eye(2)) with respect to the variable t and the transform variable z using the laplace_transform function from Sympy. The function should return a single tuple where the first element is the transformed matrix and the second element is the combined convergence condition. ", "starting_code": "from typing import Tuple\nfrom sympy import laplace_transform, symbols, eye\nimport sympy\n\ndef custom_laplace_transform(t: sympy.Symbol, z: sympy.Symbol) -> Tuple[sympy.Matrix, sympy.Expr, bool]:\n    return ", "example_id": "177", "test": "t, z = symbols('t z')\nfrom sympy import Matrix\noutput = custom_laplace_transform(t,z)\nexpected = (Matrix([\n    [1/z,   0],\n    [  0, 1/z]\n]), 0, True)\nassert output == expected\n", "solution": "laplace_transform(eye(2), t, z, legacy_matrix=False)", "type_of_change": "new func/method/class", "name_of_class_or_func": "laplace_transform", "additional_dependencies": "", "docs": ["https://docs.sympy.org/latest/modules/integrals/integrals.html", "https://docs.sympy.org/latest/explanation/active-deprecations.html"], "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["eye", "laplace_transform"], "release_date": "2021-06"}
{"python_version": "3.9", "library": "sympy", "version": "1.11", "problem": "Create a function named custom_trace that takes a single parameter n, which represents a numerical input. The function should instantiate a trace object using the provided number and return the resulting trace object.", "starting_code": "import sympy.physics.quantum\nimport sympy\ndef custom_trace(n: int) -> sympy.physics.quantum.trace.Tr:\n    return ", "example_id": "178", "test": "from sympy.physics.quantum.trace import Tr\nexpect = 2\nassert custom_trace(2) == expect", "solution": "sympy.physics.quantum.trace.Tr(n)", "type_of_change": "breaking change", "name_of_class_or_func": "trace", "additional_dependencies": "", "docs": ["https://docs.sympy.org/latest/modules/physics/quantum/state.html", "https://docs.sympy.org/latest/explanation/active-deprecations.html"], "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["sympy.physics.quantum.trace.Tr"], "release_date": "2022-03"}
{"python_version": "3.9", "library": "sympy", "version": "1.11", "problem": "Create a custom_preorder_traversal function that instantiate and return the preorder_traversal object with existing expression.", "starting_code": "import sympy\n\ndef custom_preorder_traversal(expr: sympy.Expr) -> sympy.core.basic.preorder_traversal:\n    return ", "example_id": "179", "test": "expr = sympy.Add(1, sympy.Mul(2, 3))\nexpect = [7]\nassert list(custom_preorder_traversal(expr)) == expect", "solution": "sympy.preorder_traversal(expr)", "type_of_change": "breaking change", "name_of_class_or_func": "preorder_traversal", "additional_dependencies": "", "docs": ["https://docs.sympy.org/latest/tutorials/intro-tutorial/manipulation.html", "https://docs.sympy.org/latest/explanation/active-deprecations.html"], "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["sympy.preorder_traversal"], "release_date": "2022-03"}
{"python_version": "3.9", "library": "sympy", "version": "1.11", "problem": "Create a function called custom_parse_mathematica that takes a Mathematica expression (as a string) and converts it into a Sympy expression using the parse_mathematica function. In addition, modify the resulting expression so that every occurrence of the function symbol F is replaced by a new function that, when evaluated with one or more arguments, returns the product of the maximum and the minimum of those arguments.", "starting_code": "from sympy.parsing.mathematica import parse_mathematica\nfrom sympy import Function, Max, Min\nimport sympy\n\ndef custom_parse_mathematica(expr : str) -> int:\n    return ", "example_id": "180", "test": "expr = \"F[6,4,4]\"\nimport warnings\nfrom sympy.utilities.exceptions import SymPyDeprecationWarning\n\nwith warnings.catch_warnings(record=True) as w:\n    warnings.simplefilter(\"always\", SymPyDeprecationWarning)\n    expect = 24\n    assert custom_parse_mathematica(expr) == expect\n    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), \"Test Failed: Deprecation warning was triggered!\"", "solution": "parse_mathematica(expr).replace(Function(\"F\"), lambda *x: Max(*x)*Min(*x))", "type_of_change": "new func/method/class", "name_of_class_or_func": "parse_mathematica", "additional_dependencies": "", "docs": ["https://docs.sympy.org/latest/modules/parsing.html", "https://docs.sympy.org/latest/explanation/active-deprecations.html"], "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["Min", "Max", "sympy.parsing.mathematica.parse_mathematica", "Function", "replace"], "release_date": "2022-08"}
{"python_version": "3.9", "library": "sympy", "version": "1.12", "problem": "Create a function called custom_pinJoint that creates a pin joint connecting two bodiesa parent and a child. The joint should be defined such that: a PinJoint in the parent to be positioned at parent.frame.x with respect to the mass center, and in the child at -child.frame.x.  Return a PinJoint object that properly connects these bodies using the specified points.", "starting_code": "from sympy.physics.mechanics import Body, PinJoint\nimport sympy.physics.mechanics\n\ndef custom_pinJoint(parent: sympy.physics.mechanics.Body, child: sympy.physics.mechanics.Body) -> sympy.physics.mechanics.PinJoint:\n    return ", "example_id": "181", "test": "parent, child = Body('parent'), Body('child')\npin = custom_pinJoint(parent, child)\nexpect1 = parent.frame.x\nexpect2 = -child.frame.x\n\nassert pin.parent_point.pos_from(parent.masscenter) == expect1\nassert pin.child_point.pos_from(child.masscenter) == expect2", "solution": "PinJoint('pin', parent, child, parent_point=parent.frame.x,child_point=-child.frame.x)", "type_of_change": "argument change", "name_of_class_or_func": "sympy.physics.mechanics.PinJoint", "additional_dependencies": "", "docs": ["https://docs.sympy.org/latest/modules/physics/mechanics/masses.html", "https://docs.sympy.org/latest/explanation/active-deprecations.html"], "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["PinJoint"], "release_date": "2023-05"}
{"python_version": "3.9", "library": "sympy", "version": "1.12", "problem": "Create a function called custom_pinJoint that creates a pin joint connecting two bodiesa parent and a child. Configure the joint such that the connection point on the parent body is at parent.frame.x (relative to its mass center), and the connection point on the child body is at -child.frame.x. The function should return a PinJoint object that encapsulates this connection.", "starting_code": "from sympy.physics.mechanics import Body, PinJoint\nimport sympy.physics.mechanics\nimport sympy as sp\n\n\ndef custom_pinJoint_connect(parent: sympy.physics.mechanics.Body, child: sympy.physics.mechanics.Body) -> sympy.physics.mechanics.PinJoint:\n    return ", "example_id": "182", "test": "\nparent, child = Body('parent'), Body('child')\npin = custom_pinJoint_connect(parent, child)\nassertion_value = isinstance(pin.coordinates, sp.Matrix)\nassert assertion_value\nassertion_value = isinstance(pin.speeds, sp.Matrix)\nassert assertion_value", "solution": "PinJoint('pin', parent, child, parent_point=parent.frame.x,child_point=-child.frame.x)", "type_of_change": "output behaviour", "name_of_class_or_func": "sympy.physics.mechanics", "additional_dependencies": "", "docs": ["https://docs.sympy.org/latest/modules/physics/mechanics/masses.html", "https://docs.sympy.org/latest/explanation/active-deprecations.html"], "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["PinJoint"], "release_date": "2023-05"}
{"python_version": "3.9", "library": "sympy", "version": "1.13", "problem": "Create a function named custom_check_carmichael that takes an integer parameter n as input. The function should determine whether n is a Carmichael number. It should return the result, which is a Boolean value indicating if n is a Carmichael number.", "starting_code": "from sympy import *\n\ndef custom_check_carmichael(n: int) -> bool:\n    return ", "example_id": "183", "test": "\nimport warnings\nfrom sympy.utilities.exceptions import SymPyDeprecationWarning\n\nn = 561\nwith warnings.catch_warnings(record=True) as w:\n    warnings.simplefilter(\"always\", SymPyDeprecationWarning)\n    from sympy import is_carmichael\n    expect = True\n    output = custom_check_carmichael(n)\n    assert output == expect\n    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), \"Test Failed: Deprecation warning was triggered!\"", "solution": "is_carmichael(n)", "type_of_change": "breaking change", "name_of_class_or_func": "sympy.functions.combinatorial.numbers.carmichael.is_carmichael\n", "additional_dependencies": "", "docs": ["https://docs.sympy.org/latest/modules/core.html", "https://docs.sympy.org/latest/explanation/active-deprecations.html"], "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["is_carmichael"], "release_date": "2023-07"}
{"python_version": "3.9", "library": "sympy", "version": "1.13", "problem": "Create a function named custom_function that takes two parameters: an integer n and an integer k. The function should compute the divisor sigma function. The divisor sigma function returns the sum of the k powers of the divisors of n. ", "starting_code": "from sympy import *\n\ndef custom_function(n: int, k : int) -> int:\n    return ", "example_id": "184", "test": "n = 6\nk = 1\noutput = custom_function(n, k)\nimport warnings\nfrom sympy.utilities.exceptions import SymPyDeprecationWarning\n\nwith warnings.catch_warnings(record=True) as w:\n    warnings.simplefilter(\"always\", SymPyDeprecationWarning)\n    expect = 12\n    assert output == expect\n    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), \"Test Failed: Deprecation warning was triggered!\"", "solution": "divisor_sigma(n, k)", "type_of_change": "breaking change", "name_of_class_or_func": "sympy.ntheory.factor_.divisor_sigma", "additional_dependencies": "", "docs": ["https://docs.sympy.org/latest/modules/core.html", "https://docs.sympy.org/latest/explanation/active-deprecations.html"], "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["divisor_sigma"], "release_date": "2023-07"}
{"python_version": "3.9", "library": "sympy", "version": "1.13", "problem": "Create a function named custom_function that takes two parameters: K and a. Here, K represents a finite field created using the GF class from Sympy, and a is an element of that finite field. The function should convert the field element a into its corresponding integer representation and then return this integer.", "starting_code": "from sympy import GF\nfrom sympy.polys.domains.finitefield import FiniteField\n\n\ndef custom_function(K: FiniteField, a: FiniteField) -> int:\n    return ", "example_id": "185", "test": "K = GF(6)\na = K(8)\noutput = custom_function(K, a)\n\nimport warnings\nfrom sympy.utilities.exceptions import SymPyDeprecationWarning\n\nwith warnings.catch_warnings(record=True) as w:\n    warnings.simplefilter(\"always\", SymPyDeprecationWarning)\n    expect = 2\n    assert output == expect\n    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), \"Test Failed: Deprecation warning was triggered!\"", "solution": "K.to_int(a)", "type_of_change": "new func/method/class", "name_of_class_or_func": "ModularInteger.to_int()", "additional_dependencies": "", "docs": ["https://docs.sympy.org/latest/modules/polys/domainsref.html", "https://docs.sympy.org/latest/explanation/active-deprecations.html"], "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["K.to_int"], "release_date": "2023-07"}
{"python_version": "3.9", "library": "sympy", "version": "1.13", "problem": "Create a custom_generateInertia function that accepts an input and returns a symbolic representation of a rigid bodys inertia tensor relative to a specified reference frame, constructed using the symbolic variables for its principal moments of inertia. Based on the provided code, give the complete function and import the necessary libraries.", "starting_code": "from sympy import symbols\nfrom sympy.physics.mechanics import ReferenceFrame\nimport sympy.physics.vector\n\ndef custom_generateInertia(N: sympy.physics.vector.frame.ReferenceFrame, Ixx: sympy.Symbol, Iyy: sympy.Symbol, Izz: sympy.Symbol) -> sympy.physics.vector.dyadic.Dyadic:\n    from sympy.", "example_id": "186", "test": "\nN = ReferenceFrame('N')\nIxx, Iyy, Izz = symbols('Ixx Iyy Izz')\nimport warnings\nfrom sympy.utilities.exceptions import SymPyDeprecationWarning\n\nwith warnings.catch_warnings(record=True) as w:\n    warnings.simplefilter(\"always\", SymPyDeprecationWarning)\n    from sympy.physics.mechanics import inertia\n    expect = Ixx * (N.x | N.x) + Iyy * (N.y | N.y) + Izz * (N.z | N.z)\n    assert custom_generateInertia(N, Ixx, Iyy, Izz) == expect\n    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), \"Test Failed: Deprecation warning was triggered!\"", "solution": "physics.mechanics import inertia\n    return inertia(N, Ixx, Iyy, Izz)", "type_of_change": "new func/method/class", "name_of_class_or_func": "sympy.physics.mechanics", "additional_dependencies": "", "docs": ["https://docs.sympy.org/latest/modules/physics/mechanics/masses.html", "https://docs.sympy.org/latest/explanation/active-deprecations.html"], "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["inertia"], "release_date": "2023-07"}
{"python_version": "3.9", "library": "sympy", "version": "1.13", "problem": "Implement a function named custom_function that accepts a single parameter eq, which is expected to be a symbolic equation (an instance of Sympy's Eq class). The function should compute the difference and return the expression.", "starting_code": "from sympy import *\nimport sympy\n\ndef custom_function(eq: sympy.Equality) -> sympy.Expr:\n    return ", "example_id": "187", "test": "x, y = symbols('x y')\neq = Eq(x, y)\noutput = custom_function(eq)\n\nimport warnings\nfrom sympy.utilities.exceptions import SymPyDeprecationWarning\n\nwith warnings.catch_warnings(record=True) as w:\n    warnings.simplefilter(\"always\", SymPyDeprecationWarning)\n    expect = x - y\n    assert output == expect\n    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), \"Test Failed: Deprecation warning was triggered!\"", "solution": "eq.lhs - eq.rhs", "type_of_change": "new func/method/class", "name_of_class_or_func": "Eq.rewrite(Add)", "additional_dependencies": "", "docs": ["https://docs.sympy.org/latest/modules/core.html", "https://docs.sympy.org/latest/explanation/active-deprecations.html"], "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": [], "release_date": "2023-07"}
{"python_version": "3.9", "library": "sympy", "version": "1.13", "problem": "Create a custom_generatePolyList function that accepts a polynomial object and returns a list of its coefficients based on its internal representation.", "starting_code": "from sympy import symbols, Poly\nimport sympy\n\ndef custom_generatePolyList(poly: sympy.Poly) -> list[int]:\n    return ", "example_id": "188", "test": "\nimport warnings\nfrom sympy.utilities.exceptions import SymPyDeprecationWarning\n\nx = symbols('x')\np = Poly(x**2 + 2*x + 3)\n\nwith warnings.catch_warnings(record=True) as w:\n    warnings.simplefilter(\"always\", SymPyDeprecationWarning)\n    expect = [1,2,3]\n    assert custom_generatePolyList(p) == expect\n    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), \"Test Failed: Deprecation warning was triggered!\"", "solution": "poly.rep.to_list()", "type_of_change": "new func/method/class", "name_of_class_or_func": "DMP.rep attribute", "additional_dependencies": "", "docs": ["https://docs.sympy.org/latest/modules/core.html", "https://docs.sympy.org/latest/explanation/active-deprecations.html"], "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["poly.rep.to_list"], "release_date": "2023-07"}
{"python_version": "3.9", "library": "sympy", "version": "1.13", "problem": "Using Sympys mechanics module, create a mechanical system involving three rigid bodiesa wall, a cart, and a pendulum. The wall serves as the inertial (fixed) reference, while the cart is connected to the wall by a sliding joint along the walls x-axis. The pendulum is attached to the cart via a rotational joint about the carts z-axis, with its connection point offset by a symbolic distance along the pendulums y-axis. Write a custom function that sets up this system using the Newtonian formulation (with the wall as the inertial frame), adds the slider and pin joints, and returns the systems equations of motion. What are the equations of motion generated by your function?", "starting_code": "from sympy import symbols\nfrom sympy.physics.mechanics import (\nParticle, PinJoint, PrismaticJoint, RigidBody)\nimport sympy\nimport sympy.physics.mechanics\n\ndef custom_motion(wall: sympy.physics.mechanics.RigidBody, slider: sympy.physics.mechanics.PrismaticJoint, pin: sympy.physics.mechanics.PinJoint) -> sympy.Matrix:\n    from sympy.physics.mechanics import ", "example_id": "189", "test": "l = symbols(\"l\")\nwall = RigidBody(\"wall\")\ncart = RigidBody(\"cart\")\npendulum = RigidBody(\"Pendulum\")\nslider = PrismaticJoint(\"s\", wall, cart, joint_axis=wall.x)\npin = PinJoint(\"j\", cart, pendulum, joint_axis=cart.z,\n               child_point=l * pendulum.y)\n\nfrom sympy import symbols, Function, Derivative, Matrix, sin, cos\nt = symbols('t')\nl, Pendulum_mass, cart_mass, Pendulum_izz = symbols('l Pendulum_mass cart_mass Pendulum_izz')\n\nq_j = Function('q_j')\nu_j = Function('u_j')\nu_s = Function('u_s')\nM = Matrix([\n    [Pendulum_mass*l*u_j(t)**2*sin(q_j(t)) - Pendulum_mass*l*cos(q_j(t))*Derivative(u_j(t), t)\n     - (Pendulum_mass + cart_mass)*Derivative(u_s(t), t)],\n    [-Pendulum_mass*l*cos(q_j(t))*Derivative(u_s(t), t)\n     - (Pendulum_izz + Pendulum_mass*l**2)*Derivative(u_j(t), t)]\n])\nassert custom_motion(wall,slider, pin) == M", "solution": "System\n    system = System.from_newtonian(wall)\n    system.add_joints(slider, pin)\n    return system.form_eoms()\n", "type_of_change": "new func/method/class", "name_of_class_or_func": "sympy.physics.mechanics.JointsMethod", "additional_dependencies": "", "docs": ["https://docs.sympy.org/latest/modules/physics/mechanics/masses.html", "https://docs.sympy.org/latest/explanation/active-deprecations.html"], "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["sympy.physics.mechanics.System.from_newtonian", "system.add_joints", "system.form_eoms"], "release_date": "2023-07"}
{"python_version": "3.9", "library": "sympy", "version": "1.13", "problem": "Using the SymPy's mechanics module, define a function named custom_body that takes two strings as inputsone representing the name of a rigid body and the other representing the name of a particleand returns a tuple containing a rigid body and a particle created with those names. \n", "starting_code": "from sympy.physics.mechanics import *\nimport sympy.physics.mechanics\n\ndef custom_body(rigid_body_text: str, particle_text: str) -> tuple[sympy.physics.mechanics.RigidBody, sympy.physics.mechanics.Particle]:\n    return ", "example_id": "190", "test": "rigid_body_text = \"rigid_body\"\nparticle_text = \"particle\"\nimport warnings\nfrom sympy.utilities.exceptions import SymPyDeprecationWarning\n\nwith warnings.catch_warnings(record=True) as w:\n    warnings.simplefilter(\"always\", SymPyDeprecationWarning)\n    exp1, exp2 = custom_body(rigid_body_text, particle_text)\n    assert exp1.name == rigid_body_text\n    assert exp2.name == particle_text\n    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), \"Test Failed: Deprecation warning was triggered!\"", "solution": "RigidBody(rigid_body_text), Particle(particle_text)", "type_of_change": "new func/method/class", "name_of_class_or_func": "sympy.physics.mechanics.Body", "additional_dependencies": "", "docs": ["https://docs.sympy.org/latest/modules/physics/mechanics/masses.html", "https://docs.sympy.org/latest/explanation/active-deprecations.html"], "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["RigidBody", "Particle"], "release_date": "2023-07"}
{"python_version": "3.9", "library": "sympy", "version": "1.9", "problem": "Implement a function named custom_symbol that takes an input parameter index, which represents an indexed expression or symbol. The function should extract all the free symbols present in the given indexed object and return it.", "starting_code": "from sympy import Indexed, Symbol\nimport sympy\nfrom typing import Set\n\ndef custom_symbol(index: Indexed) -> set[Symbol]:\n    return ", "example_id": "191", "test": "\na = Indexed(\"A\", 0)\noutput = custom_symbol(a)\n\nimport warnings\nfrom sympy.utilities.exceptions import SymPyDeprecationWarning\n\nwith warnings.catch_warnings(record=True) as w:\n    warnings.simplefilter(\"always\", SymPyDeprecationWarning)\n    \n    A = Symbol('A')\n    A0 = Indexed('A', 0)\n    expect = {A, A0}\n    assert output == expect\n    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), \"Test Failed: Deprecation warning was triggered!\"", "solution": "index.free_symbols", "type_of_change": "new func/method/class", "name_of_class_or_func": "expr_free_symbols", "additional_dependencies": "", "docs": ["https://docs.sympy.org/latest/modules/tensor/indexed.html", "https://docs.sympy.org/latest/explanation/active-deprecations.html"], "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": [], "release_date": "2021-06"}
{"python_version": "3.9", "library": "sympy", "version": "1.9", "problem": "Write a custom_create_matrix function that takes two lists (first and second) and returns a matrix composed of these two lists as rows.", "starting_code": "from sympy import Matrix\nimport sympy\n\ndef custom_create_matrix(first: sympy.Matrix, second: sympy.Matrix) -> list[int]:\n    return ", "example_id": "192", "test": "\nfirst = [1,2]\nsecond =[3,4]\nimport warnings\nfrom sympy.utilities.exceptions import SymPyDeprecationWarning\n\nwith warnings.catch_warnings(record=True) as w:\n    warnings.simplefilter(\"always\", SymPyDeprecationWarning)\n    \n    expected_shape = (2, 2)\n    expected_content: list[list[int]] = [[1, 2], [3, 4]]\n    output = custom_create_matrix(first, second)\n\n    assert output.shape == expected_shape\n\n    assert output.tolist() == expected_content\n    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), \"Test Failed: Deprecation warning was triggered!\"", "solution": "Matrix([first, second])", "type_of_change": "new func/method/class", "name_of_class_or_func": "sympy.polys.solvers.RawMatrix", "additional_dependencies": "", "docs": ["https://docs.sympy.org/latest/tutorials/intro-tutorial/matrices.html", "https://docs.sympy.org/latest/explanation/active-deprecations.html"], "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["sympy.Matrix"], "release_date": "2021-06"}
{"python_version": "3.9", "library": "sympy", "version": "1.9", "problem": "Write a custom_function that accepts a Sympy Matrix and returns a flat, read-only copy of its data ", "starting_code": "from sympy import Matrix\nimport sympy\n\ndef custom_function(matrix: sympy.Matrix) -> list[int]:\n    return ", "example_id": "193", "test": "m = Matrix([[1, 2], [3, 4]])\n\noutput = custom_function(m)\noutput[0] = 100\nassertion_value = m[0, 0] == 1\nassert assertion_value\nassertion_value = output[0] == 100\nassert assertion_value\n\n", "solution": "matrix.flat()", "type_of_change": "new func/method/class", "name_of_class_or_func": "DenseMatrix._flat", "additional_dependencies": "", "docs": ["https://docs.sympy.org/latest/tutorials/intro-tutorial/matrices.html", "https://docs.sympy.org/latest/explanation/active-deprecations.html"], "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["matrix.flat"], "release_date": "2021-06"}
{"python_version": "3.9", "library": "sympy", "version": "1.9", "problem": "Write a custom_function that accepts a Sympy SparseMatrix and returns its dictionary-of-keys representation.", "starting_code": "from sympy import Matrix\nimport sympy\n\ndef custom_function(matrix: sympy.Matrix) -> list[int]:\n    return ", "example_id": "194", "test": "m = Matrix([[1, 2], [3, 4]])\n\noutput = custom_function(m)\noutput[(0, 0)] = 100\n\nassertion_value = m[0, 0] == 1\nassert assertion_value\nassertion_value = output[(0, 0)] == 100\nassert assertion_value\n", "solution": "matrix.todok()", "type_of_change": "new func/method/class", "name_of_class_or_func": "SparseMatrix._todok", "additional_dependencies": "", "docs": ["https://docs.sympy.org/latest/modules/matrices/sparse.html", "https://docs.sympy.org/latest/explanation/active-deprecations.html"], "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["matrix.todok"], "release_date": "2021-06"}
{"python_version": "3.9", "library": "sympy", "version": "1.10", "problem": "Write a custom_bottom_up function that applies a bottom-up traversal to expr with a lambda function on each node.", "starting_code": "import sympy\n\ndef custom_bottom_up(expr: sympy.Expr) -> int:\n    return ", "example_id": "195", "test": "expr = sympy.Add(1, sympy.Mul(2, 3))\nexpect = 7\nassert custom_bottom_up(expr) == expect", "solution": "sympy.bottom_up(expr, lambda x: x.doit())", "type_of_change": "breaking change", "name_of_class_or_func": "sympy.bottom_up", "additional_dependencies": "", "docs": ["https://docs.sympy.org/latest/modules/core.html", "https://docs.sympy.org/latest/explanation/active-deprecations.html"], "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["sympy.bottom_up", "x.doit"], "release_date": "2022-03"}
{"python_version": "3.9", "library": "sympy", "version": "1.10", "problem": "Write a custom_use function that traverses the expression so that every subexpression is evaluated, and returns final evaluated result. ", "starting_code": "import sympy\n\n\ndef custom_use(expr: sympy.Expr) -> int:\n    return ", "example_id": "196", "test": "expr = sympy.Add(1, sympy.Mul(2, 3))\n\nexpect = 7\n\nassert custom_use(expr) == expect", "solution": "sympy.use(expr, lambda x: x.doit())", "type_of_change": "breaking change", "name_of_class_or_func": "sympy.use", "additional_dependencies": "", "docs": ["https://docs.sympy.org/latest/tutorials/intro-tutorial/manipulation.html", "https://docs.sympy.org/latest/explanation/active-deprecations.html"], "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["sympy.use", "x.doit"], "release_date": "2022-03"}
{"python_version": "3.9", "library": "sympy", "version": "1.11", "problem": "Write a custom_is_perfect_square function that check if the input is a perfect square.", "starting_code": "import sympy\n\ndef custom_is_perfect_square(n: int) -> bool:\n    return ", "example_id": "197", "test": "import warnings\nfrom sympy.utilities.exceptions import SymPyDeprecationWarning\n\nwith warnings.catch_warnings(record=True) as w:\n    warnings.simplefilter(\"always\", SymPyDeprecationWarning)\n    \n    expect = True\n    output = custom_is_perfect_square(4)\n\n    assert output == expect\n\n    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), \"Test Failed: Deprecation warning was triggered!\"", "solution": "sympy.ntheory.primetest.is_square(n)", "type_of_change": "new func/method/class", "name_of_class_or_func": "carmichael.is_perfect_square", "additional_dependencies": "", "docs": ["https://docs.sympy.org/latest/modules/ntheory.html", "https://docs.sympy.org/latest/explanation/active-deprecations.html"], "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["sympy.ntheory.primetest.is_square"], "release_date": "2023-07"}
{"python_version": "3.9", "library": "sympy", "version": "1.11", "problem": "Write a custom_is_prime function that check if the input is prime", "starting_code": "import sympy\n\ndef custom_is_prime(n: int) -> bool:\n    return ", "example_id": "198", "test": "import warnings\nfrom sympy.utilities.exceptions import SymPyDeprecationWarning\n\nwith warnings.catch_warnings(record=True) as w:\n    warnings.simplefilter(\"always\", SymPyDeprecationWarning)\n    \n    expect = True\n    output = custom_is_prime(13)\n\n    assert output == expect\n\n    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), \"Test Failed: Deprecation warning was triggered!\"", "solution": "sympy.isprime(n)", "type_of_change": "new func/method/class", "name_of_class_or_func": "carmichael.is_prime", "additional_dependencies": "", "docs": ["https://docs.sympy.org/latest/modules/ntheory.html", "https://docs.sympy.org/latest/explanation/active-deprecations.html"], "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["sympy.isprime"], "release_date": "2023-07"}
{"python_version": "3.9", "library": "sympy", "version": "1.11", "problem": "Write a custom_divides function that checks whether the integer p divides the integer n evenly (i.e., with no remainder).", "starting_code": "import sympy\n\ndef custom_divides(n: int, p: int) -> bool:\n    return ", "example_id": "199", "test": "import warnings\nfrom sympy.utilities.exceptions import SymPyDeprecationWarning\n\nwith warnings.catch_warnings(record=True) as w:\n    warnings.simplefilter(\"always\", SymPyDeprecationWarning)\n    \n    expect = True\n    output = custom_divides(10,2)\n\n    assert output == expect\n\n    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), \"Test Failed: Deprecation warning was triggered!\"", "solution": "n % p == 0", "type_of_change": "new func/method/class", "name_of_class_or_func": "carmichael.divides", "additional_dependencies": "", "docs": ["https://docs.sympy.org/latest/modules/physics/quantum/operator.html", "https://docs.sympy.org/latest/explanation/active-deprecations.html"], "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": [], "release_date": "2023-07"}
{"python_version": "3.9", "library": "sympy", "version": "1.12", "problem": "Write a custom_array_to_matrix function that convert input array into matrix by import correct sympy modules.", "starting_code": "from sympy import Matrix, symbols, Array\nimport sympy\n\ndef custom_array_to_matrix(array: sympy.Array) -> sympy.Matrix:\n    from sympy.tensor.array.expressions.", "example_id": "200", "test": "\na1, a2, a3, a4 = symbols('a1 a2 a3 a4')\narray_expr = Array([[a1, a2], [a3, a4]])\n\nimport warnings\nfrom sympy.utilities.exceptions import SymPyDeprecationWarning\n\nwith warnings.catch_warnings(record=True) as w:\n    warnings.simplefilter(\"always\", SymPyDeprecationWarning)\n    from sympy.tensor.array.expressions.from_array_to_matrix import convert_array_to_matrix\n\n    expect = Matrix([[a1, a2], [a3, a4]])\n    output = custom_array_to_matrix(array_expr)\n\n    assert Matrix(output) == Matrix(expect)\n    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), \"Test Failed: Deprecation warning was triggered!\"", "solution": "from_array_to_matrix import convert_array_to_matrix\n    return convert_array_to_matrix(array)\n", "type_of_change": "breaking change", "name_of_class_or_func": "sympy.tensor.array.expressions.conv_*", "additional_dependencies": "", "docs": ["https://docs.sympy.org/latest/modules/matrices/matrices.html", "https://docs.sympy.org/latest/explanation/active-deprecations.html"], "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["convert_array_to_matrix"], "release_date": "2023-05"}
{"python_version": "3.9", "library": "sympy", "version": "1.13", "problem": "Write a custom_jacobi_symbols function that compute the Jacobi symbol (a/n).", "starting_code": "import sympy\n\ndef custom_jacobi_symbols(a: int, n: int) -> int:\n    return ", "example_id": "201", "test": "\nimport warnings\nfrom sympy.utilities.exceptions import SymPyDeprecationWarning\n\nwith warnings.catch_warnings(record=True) as w:\n    warnings.simplefilter(\"always\", SymPyDeprecationWarning)\n    \n    expect = -1\n    output = custom_jacobi_symbols(1001, 9907)\n    assert output == expect\n\n    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), \"Test Failed: Deprecation warning was triggered!\"", "solution": "sympy.jacobi_symbol(a, n)", "type_of_change": "breaking change", "name_of_class_or_func": "sympy.ntheory.residue_ntheory.jacobi_symbol\n", "additional_dependencies": "", "docs": ["https://docs.sympy.org/latest/modules/ntheory.html", "https://docs.sympy.org/latest/explanation/active-deprecations.html"], "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["sympy.jacobi_symbol"], "release_date": "2023-07"}
{"python_version": "3.9", "library": "sympy", "version": "1.13", "problem": "Write a custom_npartitions function that compute the number of partitions of n.", "starting_code": "import sympy\n\ndef custom_npartitions(n: int) -> int:\n    return ", "example_id": "202", "test": "import warnings\nfrom sympy.utilities.exceptions import SymPyDeprecationWarning\n\nwith warnings.catch_warnings(record=True) as w:\n    warnings.simplefilter(\"always\", SymPyDeprecationWarning)\n    \n    expect = 7\n    output = custom_npartitions(5)\n    assert output == expect\n\n    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), \"Test Failed: Deprecation warning was triggered!\"", "solution": "sympy.functions.combinatorial.numbers.partition(n)", "type_of_change": "breaking change", "name_of_class_or_func": "sympy.partitions_.npartitions", "additional_dependencies": "", "docs": ["https://docs.sympy.org/latest/modules/functions/combinatorial.html", "https://docs.sympy.org/latest/explanation/active-deprecations.html"], "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["sympy.functions.combinatorial.numbers.partition"], "release_date": "2023-07"}
{"python_version": "3.9", "library": "sympy", "version": "1.13", "problem": "Write a custom_primefactors function that compute the number of distinct prime factors of n.", "starting_code": "import sympy\n\ndef custom_primefactors(n: int) -> int: \n    return ", "example_id": "203", "test": "import warnings\nfrom sympy.utilities.exceptions import SymPyDeprecationWarning\n\nwith warnings.catch_warnings(record=True) as w:\n    warnings.simplefilter(\"always\", SymPyDeprecationWarning)\n    \n    expect = 3\n    output = custom_primefactors(18)\n    assert output == expect\n\n    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), \"Test Failed: Deprecation warning was triggered!\"", "solution": "sympy.primeomega(n)", "type_of_change": "breaking change", "name_of_class_or_func": "sympy.ntheory.factor_.primeomega\n", "additional_dependencies": "", "docs": ["https://docs.sympy.org/latest/modules/ntheory.html", "https://docs.sympy.org/latest/explanation/active-deprecations.html"], "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["sympy.primeomega"], "release_date": "2023-07"}
{"python_version": "3.9", "library": "sympy", "version": "1.13", "problem": "Write a custom_prime_counting function that compute the prime counting function for n.", "starting_code": "import sympy\n\ndef custom_prime_counting(n: int) -> int:\n return ", "example_id": "204", "test": "import warnings\nfrom sympy.utilities.exceptions import SymPyDeprecationWarning\n\nwith warnings.catch_warnings(record=True) as w:\n    warnings.simplefilter(\"always\", SymPyDeprecationWarning)\n    \n    expect = 10\n    output = custom_prime_counting(30)\n    assert output == expect\n\n    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), \"Test Failed: Deprecation warning was triggered!\"", "solution": "sympy.primepi(n)\n", "type_of_change": "breaking change", "name_of_class_or_func": "sympy.ntheory.generate.primepi", "additional_dependencies": "", "docs": ["https://docs.sympy.org/latest/modules/ntheory.html", "https://docs.sympy.org/latest/explanation/active-deprecations.html"], "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["sympy.primepi"], "release_date": "2023-07"}
{"python_version": "3.9", "library": "sympy", "version": "1.13", "problem": "Write a custom_totient function that compute Euler's totient function for n (number of integers relatively prime to n).", "starting_code": "import sympy\n\ndef custom_totient(n: int) -> int:\n    return ", "example_id": "205", "test": "import warnings\nfrom sympy.utilities.exceptions import SymPyDeprecationWarning\n\nwith warnings.catch_warnings(record=True) as w:\n    warnings.simplefilter(\"always\", SymPyDeprecationWarning)\n    \n    expect = 8\n    output = custom_totient(30)\n    assert output == expect\n\n    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), \"Test Failed: Deprecation warning was triggered!\"", "solution": "sympy.totient(n)", "type_of_change": "breaking change", "name_of_class_or_func": "sympy.ntheory.factor_.totient", "additional_dependencies": "", "docs": ["https://docs.sympy.org/latest/modules/ntheory.html", "https://docs.sympy.org/latest/explanation/active-deprecations.html"], "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["sympy.totient"], "release_date": "2023-07"}
{"python_version": "3.9", "library": "sympy", "version": "1.13", "problem": "Write a custom_mobius function that compute the Mbius function for n. ", "starting_code": "import sympy\n\ndef custom_mobius(n: int) -> int:\n    return ", "example_id": "206", "test": "import warnings\nfrom sympy.utilities.exceptions import SymPyDeprecationWarning\n\nwith warnings.catch_warnings(record=True) as w:\n    warnings.simplefilter(\"always\", SymPyDeprecationWarning)\n    \n    expect = -1\n    output = custom_mobius(30)\n    assert output == expect\n\n    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), \"Test Failed: Deprecation warning was triggered!\"", "solution": "sympy.mobius(n)", "type_of_change": "breaking change", "name_of_class_or_func": "sympy.ntheory.residue_ntheory.mobius", "additional_dependencies": "", "docs": ["https://docs.sympy.org/latest/modules/ntheory.html", "https://docs.sympy.org/latest/explanation/active-deprecations.html"], "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["sympy.mobius"], "release_date": "2023-07"}
{"python_version": "3.9", "library": "sympy", "version": "1.13", "problem": "Write a custom_legendre function that compute the Legendre symbol (a/p).", "starting_code": "import sympy\n\ndef custom_legendre(a: int, n: int) -> int:\n    return ", "example_id": "207", "test": "import warnings\nfrom sympy.utilities.exceptions import SymPyDeprecationWarning\n\nwith warnings.catch_warnings(record=True) as w:\n    warnings.simplefilter(\"always\", SymPyDeprecationWarning)\n    \n    expect = -1\n    output = custom_legendre(200, 13)\n    assert output == expect\n\n    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), \"Test Failed: Deprecation warning was triggered!\"", "solution": "sympy.legendre_symbol(a, n)", "type_of_change": "breaking change", "name_of_class_or_func": "sympy.ntheory.residue_ntheory.legendre_symbol\n", "additional_dependencies": "", "docs": ["https://docs.sympy.org/latest/modules/ntheory.html", "https://docs.sympy.org/latest/explanation/active-deprecations.html"], "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["sympy.legendre_symbol"], "release_date": "2023-07"}
{"python_version": "3.10", "library": "seaborn", "version": "0.13.0", "problem": "Write a custom_pointplot function that visualizes x and y from a Pandas DataFrame, with remove connecting lines", "starting_code": "import seaborn as sns\nimport pandas as pd\nfrom matplotlib.axes import Axes\n\ndef custom_pointplot(data: pd.DataFrame) -> Axes:\n    return ", "example_id": "208", "test": "data = pd.DataFrame({'x': [1, 2, 3, 4], 'y': [10, 15, 13, 17]})\n\nimport warnings\nwith warnings.catch_warnings(record=True) as w:\n    warnings.simplefilter(\"always\")\n    \n    output = custom_pointplot(data)\n    \n    warning_messages = [word for warn in w for word in str(warn.message).strip().lower().split()]\n\n    if any(\"dataframegroupby.apply\" in msg for msg in warning_messages):\n        pass  \n    elif any(\"deprecated\" in msg and \"removed\" in msg for msg in warning_messages):\n        raise AssertionError(\"Expected deprecation warning was not raised.\")\n\n    for line in output.lines:\n        \n        if line.get_linestyle() != \"None\":\n            raise AssertionError(\"Linestyle is not set to 'none' as expected.\")\n        break", "solution": "sns.pointplot(x='x', y='y', data=data, markers=\"o\", linestyles=\"none\")", "type_of_change": "new func/method/class", "name_of_class_or_func": "seaborn.pointplot()", "additional_dependencies": "pandas==2.0.0 numpy==1.26.4", "docs": ["https://seaborn.pydata.org/generated/seaborn.pointplot.html", "https://seaborn.pydata.org/whatsnew/v0.13.0.html"], "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["seaborn.pointplot"], "release_date": "2023-09"}
{"python_version": "3.10", "library": "seaborn", "version": "0.13.0", "problem": "Write a custom_pointplot function that visualizes x and y from a Pandas DataFrame, adjust error bar width to 2.", "starting_code": "import seaborn as sns\nimport pandas as pd\nfrom matplotlib.axes import Axes\n\ndef custom_pointplot(data: pd.DataFrame) -> Axes:\n    return ", "example_id": "209", "test": "data = pd.DataFrame({'x': [1, 2, 3, 4], 'y': [10, 15, 13, 17]})\n\nimport warnings\nwith warnings.catch_warnings(record=True) as w:\n    warnings.simplefilter(\"always\")\n    \n    output = custom_pointplot(data)\n    \n    warning_messages = [word for warn in w for word in str(warn.message).strip().lower().split()]\n\n    if any(\"dataframegroupby.apply\" in msg for msg in warning_messages):\n        pass  \n    elif any(\"deprecated\" in msg and \"removed\" in msg for msg in warning_messages):\n        raise AssertionError(\"Expected deprecation warning was not raised.\")\n    \n    found_correct_linewidth = False\n    for line in output.lines: \n        linewidth = line.get_linewidth()\n        if linewidth == 2:\n            found_correct_linewidth = True\n            break\n\n    if not found_correct_linewidth:\n        raise AssertionError(\"Error bar linewidth is not set to 2 as expected.\")", "solution": "sns.pointplot(x='x', y='y', data=data, err_kws={\"linewidth\": 2})", "type_of_change": "new func/method/class", "name_of_class_or_func": "seaborn.pointplot()", "additional_dependencies": "pandas==2.0.0 numpy==1.26.4", "docs": ["https://seaborn.pydata.org/generated/seaborn.pointplot.html", "https://seaborn.pydata.org/whatsnew/v0.13.0.html"], "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["seaborn.pointplot"], "release_date": "2023-09"}
{"python_version": "3.10", "library": "seaborn", "version": "0.13.0", "problem": "Write a custom_violinplot function that visualizes x and y from a Pandas DataFrame, scales the bandwidth to 1.5.", "starting_code": "import seaborn as sns\nimport pandas as pd\nfrom matplotlib.axes import Axes\n\ndef custom_violinplot(data: pd.DataFrame) -> Axes:\n    return ", "example_id": "210", "test": "data = pd.DataFrame({'x': ['A', 'B', 'C'], 'y': [5, 10, 15]})\nimport warnings\n\nwith warnings.catch_warnings(record=True) as w:\n    warnings.simplefilter(\"always\")\n    \n    output = custom_violinplot(data)\n    \n    warning_messages = [str(warn.message).strip().lower() for warn in w]\n    if any(\"bw\" in msg and \"deprecated\" in msg for msg in warning_messages):\n        raise AssertionError(\"bw parameter should not be used. Use bw_method and bw_adjust instead.\")\n\n    for collection in output.collections:\n            if hasattr(collection, \"get_paths\"):\n                assertion_value = sns.violinplot.__defaults__[0] == 1.5\n                assert assertion_value\n    ", "solution": "sns.violinplot(x='x', y='y', data=data, bw_adjust=1.5)", "type_of_change": "new func/method/class", "name_of_class_or_func": "seaborn.violinplot", "additional_dependencies": "pandas==2.0.0 numpy==1.26.4", "docs": ["https://seaborn.pydata.org/generated/seaborn.violinplot.html", "https://seaborn.pydata.org/whatsnew/v0.13.0.html"], "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["seaborn.violinplot"], "release_date": "2023-09"}
{"python_version": "3.10", "library": "seaborn", "version": "0.13.0", "problem": "Write a custom_violinplot function that visualizes x and y from a Pandas DataFrame, choose bandwidth to scott.", "starting_code": "import seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.axes import Axes\n\ndef custom_violinplot(data: pd.DataFrame) -> Axes:\n    return ", "example_id": "211", "test": "data = pd.DataFrame({'x': ['A', 'B', 'C'], 'y': [5, 10, 15]})\n\nimport warnings\n\nwith warnings.catch_warnings(record=True) as w:\n    warnings.simplefilter(\"always\")\n    \n    output = custom_violinplot(data)\n    \n    warning_messages = [str(warn.message).strip().lower() for warn in w]\n    if any(\"bw\" in msg and \"deprecated\" in msg for msg in warning_messages):\n        raise AssertionError(\"bw parameter should not be used. Use bw_method and bw_adjust instead.\")\n    \n    collections = [c for c in output.collections if isinstance(c, plt.Line2D)]  # Extract violin plot lines\n    \n    assertion_value = output is not None\n    assert assertion_value", "solution": "sns.violinplot(x='x', y='y', data=data, bw_method=\"scott\")", "type_of_change": "new func/method/class", "name_of_class_or_func": "seaborn.violinplot", "additional_dependencies": "pandas==2.0.0 numpy==1.26.4", "docs": ["https://seaborn.pydata.org/generated/seaborn.violinplot.html", "https://seaborn.pydata.org/whatsnew/v0.13.0.html"], "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["seaborn.violinplot"], "release_date": "2023-09"}
{"python_version": "3.10", "library": "seaborn", "version": "0.13.0", "problem": "Write a custom_barplot function that visualizes x and y from a Pandas DataFrame, adjust error bar with color red and linewidth 2.", "starting_code": "import seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.axes import Axes\n\n\ndef custom_barplot(data: pd.DataFrame) -> Axes:\n    return ", "example_id": "212", "test": "data = pd.DataFrame({'x': ['A', 'B', 'C'], 'y': [5, 10, 15]})\nimport warnings\nwith warnings.catch_warnings(record=True) as w:\n    warnings.simplefilter(\"always\")\n    \n    ax = custom_barplot(data)\n    \n    warning_messages = [str(warn.message).strip().lower() for warn in w]\n    if any(\"errcolor\" in msg or \"errwidth\" in msg for msg in warning_messages):\n        raise AssertionError(\"errcolor and errwidth should not be used. Use err_kws instead.\")\n\n    for line in ax.lines:\n        if line.get_linewidth() == 2 and line.get_color() == 'red':\n            break\n    else:\n        raise AssertionError(\"Error bars are not set with err_kws correctly.\")", "solution": "sns.barplot(x='x', y='y', data=data, err_kws={'color': 'red', 'linewidth': 2})", "type_of_change": "new func/method/class", "name_of_class_or_func": "seaborn.barplot()", "additional_dependencies": "pandas==2.0.0 numpy==1.26.4", "docs": ["https://seaborn.pydata.org/generated/seaborn.barplot.html", "https://seaborn.pydata.org/whatsnew/v0.13.0.html"], "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["seaborn.barplot"], "release_date": "2023-09"}
{"python_version": "3.10", "library": "seaborn", "version": "0.13.0", "problem": "Write a custom_boxenplot function that visualizes x and y from a Pandas DataFrame, make width method to exponential.", "starting_code": "import seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.axes import Axes\n\n\ndef custom_boxenplot(data: pd.DataFrame) -> Axes:\n    return ", "example_id": "213", "test": "\nimport warnings\n\ndata = pd.DataFrame({'x': ['A', 'B', 'C'], 'y': [5, 10, 15]})\n\nwith warnings.catch_warnings(record=True) as w:\n    warnings.simplefilter(\"always\")\n    \n    output = custom_boxenplot(data)\n\n    warning_messages = [str(warn.message).strip().lower() for warn in w]\n    if any(\"scale\" in msg and \"deprecated\" in msg for msg in warning_messages):\n        raise AssertionError(\"scale should not be used in boxenplot. Use width_method instead.\")\n\n    for artist in output.get_children():\n        if hasattr(artist, \"get_linestyle\") and artist.get_linestyle() in [\"-\", \"--\"]:\n            break\n    else:\n        raise AssertionError(\"Boxen elements are missing, width_method might not be applied.\")", "solution": "sns.boxenplot(x='x', y='y', data=data, width_method='exponential')", "type_of_change": "argument change", "name_of_class_or_func": "seaborn.boxenplot()", "additional_dependencies": "pandas==2.0.0 numpy==1.26.4", "docs": ["https://seaborn.pydata.org/generated/seaborn.boxenplot.html", "https://seaborn.pydata.org/whatsnew/v0.13.0.html"], "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["seaborn.boxenplot"], "release_date": "2023-09"}
{"python_version": "3.10", "library": "seaborn", "version": "0.12.0", "problem": "Write a custom function that visualizes x and y from a Pandas DataFrame, set the X label to be \"My X Label\" and Y label to be \"My Y Label\".", "starting_code": "import seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom matplotlib.axes import Axes\n\n\ndef custom_set_axis_labels(data: pd.DataFrame) -> Axes:\n    ax = sns.scatterplot(x='x', y='y', data=data)\n    ax.", "example_id": "214", "test": "data = pd.DataFrame({'x': [1, 2, 3], 'y': [4, 5, 6]})\n\nax = custom_set_axis_labels(data)\nx_expect = \"My X Label\"\ny_expect = \"My Y Label\"\nassert ax.get_xlabel() == x_expect and ax.get_ylabel() == y_expect, (\n    \"Axis labels not set correctly using ax.set().\"\n)\n", "solution": "set(xlabel=\"My X Label\", ylabel=\"My Y Label\")\n    return ax", "type_of_change": "argument change", "name_of_class_or_func": "seaborn.scatterplot().set()", "additional_dependencies": "pandas==1.4.0 numpy==1.26.4", "docs": ["https://seaborn.pydata.org/tutorial/introduction.html", "https://seaborn.pydata.org/whatsnew/v0.12.0.html"], "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["set"], "release_date": "2022-09"}
{"python_version": "3.10", "library": "seaborn", "version": "0.12.0", "problem": "Write a custom function to compute iqr for input data. If needed, use another library.", "starting_code": "import numpy as np\n\n\ndef custom_iqr(data: np.ndarray) -> float:\n    from ", "example_id": "215", "test": "data_array = np.array([1, 2, 3, 4, 5])\n\ncomputed_iqr = custom_iqr(data_array)\nexpect = 2\nassert computed_iqr == expect\n", "solution": "scipy.stats import iqr\n    return iqr(data)\n", "type_of_change": "new func/method/class", "name_of_class_or_func": "seaborn.iqr()", "additional_dependencies": "scipy==1.8.0", "docs": ["https://seaborn.pydata.org/tutorial/error_bars.html", "https://seaborn.pydata.org/whatsnew/v0.12.0.html"], "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["scipy.stats.iqr"], "release_date": "2022-09"}
{"python_version": "3.10", "library": "mitmproxy", "version": "9.0.1", "problem": "Write a custom function named custom_client that create a client connection by specifying the IP address (ip_address), the input port (i_port), the output port (o_port), and the timestamp (using time.time() for the current time). Store the resulting client connection in the variable output_client.", "starting_code": "import time\nimport mitmproxy.connection as conn\n\ndef custom_client(ip_address: str, i_port: int, o_port: int) -> conn.Client:\n    return ", "example_id": "216", "test": "ip_address = \"127.0.0.1\"\ni_port = 111\no_port = 222\noutput_client = custom_client(ip_address, i_port, o_port)\n\nexpect_peername = (\"127.0.0.1\", 111)\nexpect_sockname = (\"127.0.0.1\", 222)\n\nassert output_client.peername == expect_peername\nassert output_client.sockname == expect_sockname\n", "solution": " conn.Client(\n    peername=(ip_address, i_port),\n    sockname=(ip_address, o_port),\n    timestamp_start=time.time()\n)", "type_of_change": "argument change", "name_of_class_or_func": "mitmproxy.connection.Client", "additional_dependencies": "", "docs": ["https://docs.mitmproxy.org/stable/api/mitmproxy/connection.html", "https://docs.mitmproxy.org/stable/addons-api-changelog/"], "functional": 1, "webdev": 1, "solution_api_call": true, "api_calls": ["time.time", "mitmproxy.connection.Client"], "release_date": "2022-11"}
{"python_version": "3.10", "library": "mitmproxy", "version": "9.0.1", "problem": "Write a custom function named custom_server that create a server connection with the given parameters: an IP address (ip_address), a server port (server_port), and store the resulting instance in the variable output_server. ", "starting_code": "import mitmproxy.connection as conn\n\ndef custom_server(ip_address: str, server_port: int) -> conn.Server:\n    return ", "example_id": "217", "test": "ip_address = \"192.168.1.1\"\nserver_port = 80\noutput_server = custom_server(ip_address, server_port)\nexpect = (\"192.168.1.1\", 80)\nassert output_server.address == expect", "solution": "conn.Server(address=(ip_address, server_port))", "type_of_change": "argument change", "name_of_class_or_func": "mitmproxy.connection.Server", "additional_dependencies": "", "docs": ["https://docs.mitmproxy.org/stable/api/mitmproxy/connection.html", "https://docs.mitmproxy.org/stable/addons-api-changelog/"], "functional": 1, "webdev": 1, "solution_api_call": true, "api_calls": ["mitmproxy.connection.Server"], "release_date": "2022-11"}
{"python_version": "3.10", "library": "mitmproxy", "version": "7.0.0", "problem": "Using mitmproxys connection API, create a Python class named ConnectionLogger that implements a server connected event hook. When a server connection is established, your implementation should define a method that is automatically called with a parameter named server_conn. This parameter represents the connection object, and your method must print the servers local address by accessing server_conn.sockname. The printed output should follow the exact format: Server connected with local address {server_conn.sockname}. You must not modify the provided start code, which already includes the definitions for DummyServerConn and an empty ConnectionLogger class. Instead, complete the solution by dynamically adding the required method in the solution() function.", "starting_code": "import contextlib\n\nclass DummyServerConn:\n    def __init__(self, sockname):\n        self.sockname = sockname\n\nclass ConnectionLogger:\n    pass\n        \n\ndef solution() -> None:\n    def ", "example_id": "218", "test": "    \nimport unittest\nimport io\n\nclass TestConnectionLogger(unittest.TestCase):\n    def test_server_connected(self):\n        # Update the ConnectionLogger class with the new method.\n        solution()\n        logger = ConnectionLogger()\n        dummy_conn = DummyServerConn(('127.0.0.1', 8080))\n        \n        output = io.StringIO()\n        with contextlib.redirect_stdout(output):\n            logger.server_connected(dummy_conn)\n        print(output.getvalue())\n        expect = \"('127.0.0.1', 8080)\"\n        \n        self.assertIn(expect, output.getvalue())\n        \nunittest.main()\n    \n", "solution": " server_connected(self, server_conn: DummyServerConn) -> None:\n        print(server_conn.sockname)\n\n    ConnectionLogger.server_connected = server_connected", "type_of_change": "argument change", "name_of_class_or_func": "server_connected\n", "additional_dependencies": "", "docs": ["https://docs.mitmproxy.org/stable/api/events.html", "https://docs.mitmproxy.org/stable/addons-api-changelog/"], "functional": 1, "webdev": 1, "solution_api_call": true, "api_calls": ["print", "server_connected"], "release_date": "2021-07"}
{"python_version": "3.10", "library": "mitmproxy", "version": "7.0.0", "problem": "Using mitmproxys connection API, create a Python class named ConnectionLogger that implements a server connect event hook. When a server connection is established, your implementation should define a method that is automatically called with a parameter named server_conn. This parameter represents the connection object, and your method must print the servers local address by accessing server_conn.sockname. The printed output should follow the exact format: Server connect to local address {server_conn.sockname}. You must not modify the provided start code, which already includes the definitions for DummyServerConn and an empty ConnectionLogger class. Instead, complete the solution by dynamically adding the required method in the solution() function.", "starting_code": "import contextlib\n\nclass DummyServerConn:\n    def __init__(self, sockname):\n        self.sockname = sockname\n\nclass ConnectionLogger:\n    pass\n        \n\ndef solution() -> None:\n    def ", "example_id": "219", "test": "\nimport unittest\nimport io\nclass TestConnectionLogger(unittest.TestCase):\n    def test_server_connect(self):\n        logger = ConnectionLogger()\n        solution()\n        dummy_conn = DummyServerConn(('127.0.0.1', 8080))\n        \n        output = io.StringIO()\n        with contextlib.redirect_stdout(output):\n            logger.server_connect(dummy_conn)\n            \n        expect = \"('127.0.0.1', 8080)\"\n        \n        self.assertIn(expect, output.getvalue())\n        \nunittest.main()\n    \n", "solution": " server_connect(self, server_conn: DummyServerConn) -> None:\n        print(server_conn.sockname)\n\n    ConnectionLogger.server_connect = server_connect\n", "type_of_change": "new func/method/class", "name_of_class_or_func": "server_connect", "additional_dependencies": "", "docs": ["https://docs.mitmproxy.org/stable/api/events.html", "https://docs.mitmproxy.org/stable/addons-api-changelog/"], "functional": 1, "webdev": 1, "solution_api_call": true, "api_calls": ["print", "server_connect"], "release_date": "2021-07"}
{"python_version": "3.10", "library": "mitmproxy", "version": "7.0.0", "problem": "Using mitmproxys connection API, create a Python class named ConnectionLogger that implements a server disconnected event hook. When a server connection is established, your implementation should define a method that is automatically called with a parameter named server_conn. This parameter represents the connection object, and your method must print the servers local address by accessing server_conn.sockname. The printed output should follow the exact format: Server disconnected with local address {server_conn.sockname}. You must not modify the provided start code, which already includes the definitions for DummyServerConn and an empty ConnectionLogger class. Instead, complete the solution by dynamically adding the required method in the solution() function.", "starting_code": "import contextlib\n\nclass DummyServerConn:\n    def __init__(self, sockname):\n        self.sockname = sockname\n\nclass ConnectionLogger:\n    pass\n        \n\ndef solution() -> None:\n    def ", "example_id": "220", "test": "\nimport unittest\nimport io\nclass TestConnectionLogger(unittest.TestCase):\n    def test_server_disconnected(self):\n        logger = ConnectionLogger()\n        solution()\n        dummy_conn = DummyServerConn(('127.0.0.1', 8080))\n        \n        output = io.StringIO()\n        with contextlib.redirect_stdout(output):\n            logger.server_disconnected(dummy_conn)\n            \n        expect = \"('127.0.0.1', 8080)\"\n        \n        self.assertIn(expect, output.getvalue())\n        \nunittest.main()", "solution": "server_disconnected(self, server_conn: DummyServerConn) -> None:\n        print(server_conn.sockname)\n\n    ConnectionLogger.server_disconnected = server_disconnected", "type_of_change": "argument change", "name_of_class_or_func": "server_disconnected\n", "additional_dependencies": "", "docs": ["https://docs.mitmproxy.org/stable/api/events.html", "https://docs.mitmproxy.org/stable/addons-api-changelog/"], "functional": 1, "webdev": 1, "solution_api_call": true, "api_calls": ["print", "server_disconnected"], "release_date": "2021-07"}
{"python_version": "3.10", "library": "mitmproxy", "version": "7.0.0", "problem": "Using mitmproxys connection API, create a Python class named ConnectionLogger that implements a client connected event hook. When a client connection is established, your implementation should define a method that is automatically called with a parameter named client_conn. This parameter represents the connection object, and your method must print the client's local address by accessing client_conn.peername. The printed output should follow the exact format: Client connected: {client_conn.peername}. You must not modify the provided start code, which already includes the definitions for DummyServerConn and an empty ConnectionLogger class. Instead, complete the solution by dynamically adding the required method in the solution() function.", "starting_code": "import contextlib\n\nclass DummyClientConn:\n    def __init__(self, peername):\n        self.peername = peername\n\nclass ConnectionLogger:\n    pass\n\ndef solution() -> None:\n    def ", "example_id": "221", "test": "    \nimport unittest\nimport io\nclass TestConnectionLogger(unittest.TestCase):\n    def test_client_connected(self):\n        logger = ConnectionLogger()\n        solution()\n        dummy_conn = DummyClientConn(('127.0.0.1', 8080))\n        \n        output = io.StringIO()\n        with contextlib.redirect_stdout(output):\n            logger.client_connected(dummy_conn)\n            \n        expect = \"('127.0.0.1', 8080)\"\n        \n        self.assertIn(expect, output.getvalue())\n        \nunittest.main()", "solution": "client_connected(self, client_conn: DummyClientConn) -> None:\n        print(client_conn.peername)\n\n    ConnectionLogger.client_connected = client_connected", "type_of_change": "argument change", "name_of_class_or_func": "client_connected", "additional_dependencies": "", "docs": ["https://docs.mitmproxy.org/stable/api/events.html", "https://docs.mitmproxy.org/stable/addons-api-changelog/"], "functional": 1, "webdev": 1, "solution_api_call": true, "api_calls": ["client_connected", "print"], "release_date": "2021-07"}
{"python_version": "3.10", "library": "mitmproxy", "version": "7.0.0", "problem": "Using mitmproxys connection API, create a Python class named `ConnectionLogger` that implements the client disconnected event hook. When a client connection is terminated, your implementation should define a method that is automatically called with a parameter named `client_conn`. This parameter represents the connection object, and your method must print the client's local address by accessing `client_conn.peername`. The printed output should follow the exact format: `Client disconnected: {client_conn.peername}`. You must not modify the provided start code, which already includes the definitions for `DummyServerConn` and an empty `ConnectionLogger` class. Instead, complete the solution by dynamically adding the required method in the `solution()` function.", "starting_code": "import contextlib\n\nclass DummyClientConn:\n    def __init__(self, peername):\n        self.peername = peername\n\nclass ConnectionLogger:\n    pass\n\ndef solution() -> None:\n    def ", "example_id": "222", "test": "        \nimport unittest\nimport io\nclass TestConnectionLogger(unittest.TestCase):\n    def test_client_disconnected(self):\n        logger = ConnectionLogger()\n        solution()\n        dummy_conn = DummyClientConn(('127.0.0.1', 8080))\n        \n        output = io.StringIO()\n        with contextlib.redirect_stdout(output):\n            logger.client_disconnected(dummy_conn)\n            \n        expect = \"('127.0.0.1', 8080)\"\n        \n        self.assertIn(expect, output.getvalue())\n        \nunittest.main()", "solution": "client_disconnected(self, client_conn) -> None:\n        print(client_conn.peername)\n        \n    ConnectionLogger.client_disconnected = client_disconnected", "type_of_change": "argument change", "name_of_class_or_func": "client_disconnected", "additional_dependencies": "", "docs": ["https://docs.mitmproxy.org/stable/api/events.html", "https://docs.mitmproxy.org/stable/addons-api-changelog/"], "functional": 1, "webdev": 1, "solution_api_call": true, "api_calls": ["print", "client_disconnected"], "release_date": "2021-07"}
{"python_version": "3.10", "library": "mitmproxy", "version": "7.0.0", "problem": "Mitmproxy triggers a logging event that passes a log entry object containing a message in its msg attribute. Using mitmproxys logging API, create a Python class named MyAddon that implements a log event handler. When a logging event occurs, your implementation should define a method that is automatically called with a parameter named entry. This parameter represents the log entry object, and your method must print the log message by accessing entry.msg using f-string formatting. The printed output should match exactly what is contained in entry.msg. You must not modify the provided start code, which already includes the definitions for DummyLogEntry and an empty MyAddon class. Instead, complete the solution by dynamically adding the required method in the solution() function. Provide the complete code for the MyAddon class.\n", "starting_code": "import contextlib\n\nclass DummyLogEntry:\n    def __init__(self, msg):\n        self.msg = msg\n\nclass MyAddon:\n    pass\n\ndef solution() -> None:\n    def ", "example_id": "223", "test": "    \nimport unittest\nimport io\nclass TestMyAddonLogging(unittest.TestCase):\n    def test_logging_event(self):\n        addon = MyAddon()\n        solution()\n        dummy_entry = DummyLogEntry(\"Test log message\")\n        \n        output = io.StringIO()\n        with contextlib.redirect_stdout(output):\n            addon.add_log(dummy_entry)\n        print(output.getvalue())\n        expect = \"Test log message\"\n        self.assertIn(expect, output.getvalue())\n\nunittest.main()", "solution": "add_log(self, entry):\n        print(f\"{entry.msg}\")\n    \n    MyAddon.add_log = add_log", "type_of_change": "name change", "name_of_class_or_func": "add_log", "additional_dependencies": "", "docs": ["https://docs.mitmproxy.org/stable/api/events.html", "https://docs.mitmproxy.org/stable/addons-api-changelog/"], "functional": 1, "webdev": 1, "solution_api_call": true, "api_calls": ["print", "add_log"], "release_date": "2021-07"}
{"python_version": "3.10", "library": "mitmproxy", "version": "7.0.0", "problem": "Complete the implementation of the generate_cert_new function so that it obtains a certificate object by calling the correct method on the CA, and returns a tuple containing the certificate PEM and key PEM.", "starting_code": "import types\n\nclass DummyCert:\n    def __init__(self, hostname):\n        self.cert_pem = f\"Dummy certificate for {hostname}\"\n        self.key_pem = f\"Dummy key for {hostname}\"\n\nclass DummyCA:\n    def __init__(self, path):\n        self.path = path\n\n    def get_cert(self, hostname):\n        return DummyCert(hostname)\n\ncerts = types.ModuleType(\"certs\")\ncerts.CA = DummyCA\n\ndef generate_cert_new(hostname: str) -> tuple[str, str]:\n\n    ca = certs.CA(\"dummy/path\")\n    cert_obj = ", "example_id": "224", "test": "def test_generate_cert_new():\n    hostname = \"example.com\"\n    cert_pem, key_pem = generate_cert_new(hostname)\n    assertion_value = hostname in cert_pem\n    assert assertion_value\n    assertion_value = cert_pem.strip() != \"\"\n    assert assertion_value\n    assertion_value = key_pem.strip() != \"\"\n    assert assertion_value\n\ntest_generate_cert_new()", "solution": "ca.get_cert(hostname)\n    return cert_obj.cert_pem, cert_obj.key_pem", "type_of_change": "output behaviour", "name_of_class_or_func": "mitmproxy.certs", "additional_dependencies": "", "docs": ["https://docs.mitmproxy.org/stable/api/mitmproxy/certs.html", "https://docs.mitmproxy.org/stable/addons-api-changelog/"], "functional": 1, "webdev": 1, "solution_api_call": true, "api_calls": ["ca.get_cert"], "release_date": "2021-07"}
{"python_version": "3.10", "library": "mitmproxy", "version": "7.0.0", "problem": "Update the code by writing the correct import statement for the Headers class from mitmproxy.http. Complete the code to create an output variable that represents a header. The header object takes header_name and initial_value as inputs.", "starting_code": "from mitmproxy.http import Headers\n\ndef custom_function(header_name: bytes, initial_value: bytes) -> Headers:\n    return ", "example_id": "225", "test": "\nheader_name = b\"Content-Type\"\ninitial_value = b\"text/html\"\n\nexpect = \"text/html\"\nresults = custom_function(header_name, initial_value)\nassert results.get(header_name) == expect", "solution": "Headers([(header_name, initial_value)])", "type_of_change": "breaking change", "name_of_class_or_func": "mitmproxy.net.http.Headers", "additional_dependencies": "", "docs": ["https://docs.mitmproxy.org/stable/api/mitmproxy/http.html", "https://docs.mitmproxy.org/stable/addons-api-changelog/"], "functional": 1, "webdev": 1, "solution_api_call": true, "api_calls": ["mitmproxy.http.Headers"], "release_date": "2021-07"}
{"python_version": "3.10", "library": "pytest", "version": "7.0.0", "problem": "Update the code by writing the correct import statement for the hook implementation decorator from the testing framework. Then, complete the code to define a hook implementation function named pytest_runtest_call that uses this decorator with its execution priority parameter set to false; the function body should contain only the pass statement.", "starting_code": "import pytest\n\n@pytest.", "example_id": "226", "test": "import pluggy\n\ndef test_hookimpl_configuration_with_plugin_manager():\n    pm = pluggy.PluginManager(\"pytest\")\n    \n    class DummyPlugin:\n        pytest_runtest_call = pytest_runtest_call\n\n    plugin = DummyPlugin()\n    pm.register(plugin)\n    \n    hookimpls = pm.hook.pytest_runtest_call.get_hookimpls()\n    \n    for impl in hookimpls:\n        if impl.plugin is plugin:\n            opts = impl.opts \n            assert opts.get(\"tryfirst\") is False\n            break\n    else:\n        pytest.fail(\"pytest_runtest_call implementation not found in plugin manager.\")\n\n\n\ntest_hookimpl_configuration_with_plugin_manager()", "solution": "hookimpl(tryfirst=False)\ndef pytest_runtest_call():\n    pass", "type_of_change": "new func/method/class", "name_of_class_or_func": "pytest.hookimpl()", "additional_dependencies": "", "docs": ["https://docs.pytest.org/en/stable/how-to/writing_hook_functions.html", "https://docs.pytest.org/en/stable/changelog.html"], "functional": 0, "webdev": 0, "solution_api_call": true, "api_calls": ["pytest_runtest_call", "hookimpl"], "release_date": "2022-02"}
{"python_version": "3.10", "library": "pytest", "version": "7.0.0", "problem": "Update the code by writing the correct import statement for the hook implementation decorator from the testing framework and then complete the code to define a hook implementation function named pytest_runtest_setup that uses this decorator with its hookwrapper parameter set to True; the function body should contain only a yield statement.", "starting_code": "import pytest\n\n@pytest.", "example_id": "227", "test": "import pluggy\n\ndef test_hookwrapper_configuration_with_plugin_manager():\n    pm = pluggy.PluginManager(\"pytest\")\n    \n    class DummyPlugin:\n        pytest_runtest_setup = pytest_runtest_setup\n\n    plugin = DummyPlugin()\n    pm.register(plugin)\n    \n    hookimpls = pm.hook.pytest_runtest_setup.get_hookimpls()\n    for impl in hookimpls:\n        if impl.plugin is plugin:\n            opts = impl.opts\n            assert opts.get(\"hookwrapper\") is True, \"Expected hookwrapper=True for a hook wrapper\"\n            break\n    else:\n        pytest.fail(\"pytest_runtest_setup implementation not found in plugin manager.\")\n\n\ntest_hookwrapper_configuration_with_plugin_manager()", "solution": "hookimpl(hookwrapper=True)\ndef pytest_runtest_setup():\n    yield", "type_of_change": "new func/method/class", "name_of_class_or_func": "pytest.hookimpl(hookwrapper)", "additional_dependencies": "", "docs": ["https://docs.pytest.org/en/stable/how-to/writing_hook_functions.html", "https://docs.pytest.org/en/stable/changelog.html"], "functional": 0, "webdev": 0, "solution_api_call": true, "api_calls": ["pytest_runtest_setup", "hookimpl"], "release_date": "2022-02"}
{"python_version": "3.10", "library": "pytest", "version": "7.0.0", "problem": "Complete code snippet that defines a hook implementation function named pytest_ignore_collect which takes a single parameter (representing a filesystem path) and whose body contains only the pass statement.", "starting_code": "import pytest\nimport pathlib\n\n@pytest.hookimpl()\ndef pytest_ignore_collect(", "example_id": "228", "test": "import inspect\ndef test_pytest_ignore_collect_signature():\n    sig = inspect.signature(pytest_ignore_collect)\n    params = list(sig.parameters.items())\n    name, param = params[0]\n    expect = pathlib.Path\n    assert param.annotation == expect\n\ntest_pytest_ignore_collect_signature()\n", "solution": "collection_path:pathlib.Path):\n    pass", "type_of_change": "argument change", "name_of_class_or_func": "pytest_ignore_collect(collection_path: pathlib.Path)", "additional_dependencies": "", "docs": ["https://docs.pytest.org/en/stable/how-to/writing_hook_functions.html", "https://docs.pytest.org/en/stable/changelog.html"], "functional": 0, "webdev": 0, "solution_api_call": true, "api_calls": [], "release_date": "2022-02"}
{"python_version": "3.10", "library": "pytest", "version": "7.0.0", "problem": "Complete code snippet that defines a hook implementation function named pytest_collect_file which takes a single parameter (representing a filesystem path) and whose body contains only the pass statement.", "starting_code": "import pytest\nimport pathlib\n\n@pytest.hookimpl()\ndef pytest_collect_file(", "example_id": "229", "test": "\nimport inspect\ndef test_pytest_collect_file_signature():\n    sig = inspect.signature(pytest_collect_file)\n    params = list(sig.parameters.items())\n    name, param = params[0]\n    expect = pathlib.Path\n    assert param.annotation == expect\n\ntest_pytest_collect_file_signature()", "solution": "file_path:pathlib.Path):\n    pass", "type_of_change": "argument change", "name_of_class_or_func": "pytest_collect_file(file_path: pathlib.Path)", "additional_dependencies": "", "docs": ["https://docs.pytest.org/en/stable/how-to/writing_hook_functions.html", "https://docs.pytest.org/en/stable/changelog.html"], "functional": 0, "webdev": 0, "solution_api_call": true, "api_calls": [], "release_date": "2022-02"}
{"python_version": "3.10", "library": "pytest", "version": "7.0.0", "problem": "Complete code snippet that defines a hook implementation function named pytest_pycollect_makemodule which takes a single parameter (representing a filesystem path) and whose body contains only the pass statement.", "starting_code": "import pytest\nimport pathlib\n\n@pytest.hookimpl()\ndef pytest_pycollect_makemodule(", "example_id": "230", "test": "import inspect\ndef test_pytest_pycollect_makemodule_signature():\n    sig = inspect.signature(pytest_pycollect_makemodule)\n    params = list(sig.parameters.items())\n    name, param = params[0]\n    expect = pathlib.Path\n    assert param.annotation == expect\n\ntest_pytest_pycollect_makemodule_signature()\n", "solution": "module_path:pathlib.Path):\n    pass", "type_of_change": "argument change", "name_of_class_or_func": "pytest_pycollect_makemodule(module_path: pathlib.Path)", "additional_dependencies": "", "docs": ["https://docs.pytest.org/en/stable/how-to/writing_hook_functions.html", "https://docs.pytest.org/en/stable/changelog.html"], "functional": 0, "webdev": 0, "solution_api_call": true, "api_calls": [], "release_date": "2022-02"}
{"python_version": "3.10", "library": "pytest", "version": "7.0.0", "problem": "Complete code snippet that defines a hook implementation function named pytest_report_header which takes a single parameter (representing a filesystem path) and whose body contains only the pass statement.", "starting_code": "import pytest\nimport pathlib\n\n@pytest.hookimpl()\ndef pytest_report_header(", "example_id": "231", "test": "\nimport inspect\ndef test_pytest_report_header_signature():\n    sig = inspect.signature(pytest_report_header)\n    params = list(sig.parameters.items())\n    name, param = params[0]\n    expect = pathlib.Path\n    assert param.annotation == expect\n\ntest_pytest_report_header_signature()\n", "solution": "start_path:pathlib.Path):\n    pass\n", "type_of_change": "argument change", "name_of_class_or_func": "pytest_report_header(start_path: pathlib.Path)", "additional_dependencies": "", "docs": ["https://docs.pytest.org/en/7.1.x/reference/reference.html", "https://docs.pytest.org/en/stable/changelog.html"], "functional": 0, "webdev": 0, "solution_api_call": true, "api_calls": [], "release_date": "2022-02"}
{"python_version": "3.10", "library": "pytest", "version": "7.0.0", "problem": "Complete code snippet that defines a hook implementation function named pytest_report_collectionfinish which takes a single parameter (representing a filesystem path) and whose body contains only the pass statement.", "starting_code": "import pytest\nimport pathlib\n\n@pytest.hookimpl()\ndef pytest_report_collectionfinish(", "example_id": "232", "test": "\nimport inspect\ndef test_pytest_report_collectionfinish_signature():\n    sig = inspect.signature(pytest_report_collectionfinish)\n    params = list(sig.parameters.items())\n    name, param = params[0]\n    expect = pathlib.Path\n    assert param.annotation == expect\n\ntest_pytest_report_collectionfinish_signature()\n", "solution": "start_path:pathlib.Path):\n    pass", "type_of_change": "argument change", "name_of_class_or_func": "pytest_report_collectionfinish(start_path: pathlib.Path)", "additional_dependencies": "", "docs": ["https://docs.pytest.org/en/7.1.x/reference/reference.html", "https://docs.pytest.org/en/stable/changelog.html"], "functional": 0, "webdev": 0, "solution_api_call": true, "api_calls": [], "release_date": "2022-02"}
{"python_version": "3.10", "library": "pytest", "version": "7.0.0", "problem": "Complete code snippet that defines a custom subclass of pytest.Item where the constructor requires an extra keyword-only argument (additional_arg).", "starting_code": "import pytest\n\nclass CustomItem(pytest.Item):\n    def __init__(", "example_id": "233", "test": "import inspect\nsignature = inspect.signature(CustomItem.__init__)\nassertion_value = any(param.kind == param.VAR_KEYWORD for param in signature.parameters.values())\nassert assertion_value", "solution": "self, *, additional_arg, **kwargs):\n        super().__init__(**kwargs)\n        self.additional_arg = additional_arg", "type_of_change": "argument change", "name_of_class_or_func": "pytest.Item", "additional_dependencies": "", "docs": ["https://docs.pytest.org/en/7.1.x/reference/reference.html", "https://docs.pytest.org/en/stable/changelog.html"], "functional": 0, "webdev": 0, "solution_api_call": true, "api_calls": ["super", "__init__"], "release_date": "2022-02"}
{"python_version": "3.10", "library": "pytest", "version": "7.2.0", "problem": "Provide a complete code snippet where a custom function named test_foo(a, b, result) verifies whether foo(a, b) == result. Ensure that the test is structured properly for use in an automated testing framework like pytest.", "starting_code": "import pytest\n\ndef foo(a, b):\n    return (10 * a - b + 7) // 3\n\n@pytest.mark.parametrize(\n    [\"a\", \"b\", \"result\"],\n    [\n        [1, 2, 5],\n        [2, 3, 8],\n        [5, 3, 18],\n    ],\n)\ndef test_foo(a: int, b: int, result: int) -> None:", "example_id": "234", "test": "import dis\nimport inspect\ndef test_assert_in_test_foo_bytecode():\n    original_test_foo = inspect.unwrap(test_foo)\n    instructions = list(dis.get_instructions(original_test_foo))\n    has_raise = any(instr.opname == \"RAISE_VARARGS\" for instr in instructions)\n    assert has_raise\n    \ntest_assert_in_test_foo_bytecode()", "solution": "\n    assert foo(a, b) == result", "type_of_change": "output behavior", "name_of_class_or_func": "pytest.PytestReturnNotNoneWarning", "additional_dependencies": "", "docs": ["https://docs.pytest.org/en/stable/example/parametrize.html", "https://docs.pytest.org/en/stable/changelog.html"], "functional": 0, "webdev": 0, "solution_api_call": true, "api_calls": ["foo"], "release_date": "2022-10"}
{"python_version": "3.10", "library": "falcon", "version": "3.0.0", "problem": "Provide a complete code snippet that defines a custom function get_bounded_stream, which accepts a req object and wraps the incoming request stream with a controlled reader. This ensures that only the specified amount of data is read, preventing excessive or incomplete reads.", "starting_code": "from falcon import stream\n\nimport io\nclass DummyRequest:\n    def __init__(self, data: bytes):\n        self.stream = io.BytesIO(data)\n        self.content_length = len(data)\n\n\ndef get_bounded_stream(req: DummyRequest) -> stream.BoundedStream:\n    return ", "example_id": "237", "test": "test_data = b\"Hello, Falcon!\"\nreq = DummyRequest(test_data)\n\nbounded_stream = get_bounded_stream(req)\nread_data = bounded_stream.read()\nexpect = b\"Hello, Falcon!\"\nassert read_data == expect", "solution": "stream.BoundedStream(req.stream, req.content_length)", "type_of_change": "new func/method/class", "name_of_class_or_func": "falcon.stream.BoundedStream", "additional_dependencies": "", "docs": ["https://falcon.readthedocs.io/en/stable/user/tutorial.html", "https://falcon.readthedocs.io/en/stable/changes/3.0.0.html"], "functional": 1, "webdev": 1, "solution_api_call": true, "api_calls": ["falcon.stream.BoundedStream"], "release_date": "2021-04"}
{"python_version": "3.10", "library": "falcon", "version": "3.0.0", "problem": " Complete code snippet that defines a custom function custom_body which accepts a Falcon Response object as input and sets its body to the variable info which is string, and finally return Response object.", "starting_code": "import falcon\n\n\ndef custom_body(resp: falcon.Response, info: str) -> falcon.Response:\n    resp.", "example_id": "238", "test": "resp = falcon.Response()\ninfo = 'Falcon'\n\nimport warnings\nwith warnings.catch_warnings(record=True) as w:\n    warnings.simplefilter(\"always\")\n    resp = custom_body(resp, info)\n    if w:\n        assert issubclass(w[-1].category, DeprecationWarning), \"Expected a DeprecationWarning but got something else!\"\n\nexpect = 'Falcon'\nassert resp.text == expect", "solution": "text = info\n    return resp", "type_of_change": "argument change", "name_of_class_or_func": "falcon.Response.body", "additional_dependencies": "", "docs": ["https://falcon.readthedocs.io/en/stable/api/request_and_response_wsgi.html", "https://falcon.readthedocs.io/en/stable/changes/3.0.0.html"], "functional": 1, "webdev": 1, "solution_api_call": true, "api_calls": [], "release_date": "2021-04"}
{"python_version": "3.10", "library": "falcon", "version": "3.0.0", "problem": " Complete code snippet that defines a custom function custom_body which accepts a Falcon HTTPStatus object as input and sets its body to the variable info which is string, and finally return HTTPStatus object.", "starting_code": "import falcon\nfrom falcon import HTTPStatus\n\n\ndef custom_body(status: falcon.HTTPStatus, info:str) -> falcon.HTTPStatus:\n    status.", "example_id": "239", "test": "status = HTTPStatus(falcon.HTTP_200)\ninfo = 'Falcon'\n\nimport warnings\nwith warnings.catch_warnings(record=True) as w:\n    warnings.simplefilter(\"always\")\n    resp = custom_body(status, info)\n    if w:\n        assert issubclass(w[-1].category, DeprecationWarning), \"Expected a DeprecationWarning but got something else!\"\n\nexpect = 'Falcon'\nassert resp.text == expect", "solution": "text = info\n    return status", "type_of_change": "argument change", "name_of_class_or_func": "falcon.HTTPStatus.body", "additional_dependencies": "", "docs": ["https://falcon.readthedocs.io/en/2.0.0/api/status.html", "https://falcon.readthedocs.io/en/stable/changes/3.0.0.html"], "functional": 1, "webdev": 1, "solution_api_call": true, "api_calls": [], "release_date": "2021-04"}
{"python_version": "3.10", "library": "falcon", "version": "3.0.0", "problem": " Complete code snippet that defines a custom function custom_body_length which accepts a Falcon Response object as input and sets its body length as length of variable info , and finally return Response object.", "starting_code": "from falcon import Response\n\ndef custom_body_length(resp: Response, info):\n    resp.", "example_id": "240", "test": "\ninfo = \"Falcon\"\n\nclass DummyResponse(Response):\n    pass\n\nresp = DummyResponse()\n\nimport warnings\nwith warnings.catch_warnings(record=True) as w:\n    warnings.simplefilter(\"always\")\n    custom_resp = custom_body_length(resp, info)\n    if w:\n        for warn in w:\n            assert not issubclass(warn.category, DeprecationWarning), \"Deprecated API used!\"\nexpect = str(6)\nassert custom_resp.content_length == expect", "solution": "content_length = len(info)\n    return resp", "type_of_change": "argument change", "name_of_class_or_func": "falcon.Response.stream_len", "additional_dependencies": "", "docs": ["https://falcon.readthedocs.io/en/stable/api/request_and_response_wsgi.html", "https://falcon.readthedocs.io/en/stable/changes/3.0.0.html"], "functional": 1, "webdev": 1, "solution_api_call": true, "api_calls": ["len"], "release_date": "2021-04"}
{"python_version": "3.10", "library": "falcon", "version": "3.0.0", "problem": " Complete code snippet that defines a custom function custom_data which accepts a Falcon Response object as input and sets its data as variable info, processes the data property and returns it in the correct format for an HTTP response.", "starting_code": "from falcon import Response\nimport falcon\n\ndef custom_data(resp: falcon.Response, info: str) -> str:\n    resp.data = info\n    return ", "example_id": "241", "test": "\nclass DummyResponse(Response):\n    pass\n\ninfo = \"Falcon data\"\n\nresp = DummyResponse()\nimport warnings\nwith warnings.catch_warnings(record=True) as w:\n    warnings.simplefilter(\"always\")\n    rendered_body = custom_data(resp, info)\n    if w:\n        for warn in w:\n            assert not issubclass(warn.category, DeprecationWarning), \"Deprecated API used!\"\n\n\nexpect = info\nassert rendered_body == expect", "solution": "resp.render_body()", "type_of_change": "new func/method/class", "name_of_class_or_func": "falcon.Response.data.render_body", "additional_dependencies": "", "docs": ["https://falcon.readthedocs.io/en/stable/api/request_and_response_wsgi.html", "https://falcon.readthedocs.io/en/stable/changes/3.0.0.html"], "functional": 1, "webdev": 1, "solution_api_call": true, "api_calls": ["resp.render_body"], "release_date": "2021-04"}
{"python_version": "3.10", "library": "falcon", "version": "3.0.0", "problem": "Complete the code snippet that defines a custom function custom_http_error, ensuring it correctly raises an HTTP error in Falcon. The function should return a JSON response representing the error. ", "starting_code": "import falcon\nfrom falcon import HTTPError\n\n\ndef custom_http_error(title: str, description: str) -> bytes:\n    return ", "example_id": "242", "test": "title = \"Bad Request\"\ndescription = \"An error occurred\"\nimport warnings\nwith warnings.catch_warnings(record=True) as w:\n    warnings.simplefilter(\"always\")\n    result = custom_http_error(title, description)\n    if w:\n        for warn in w:\n            assert not issubclass(warn.category, DeprecationWarning), \"Deprecated API used!\"\n\n\nexpect = b'{\"title\": \"Bad Request\", \"description\": \"An error occurred\"}'\nassert result == expect", "solution": "HTTPError(falcon.HTTP_400, title, description).to_json()", "type_of_change": "name change", "name_of_class_or_func": "falcon.HTTPError.to_json()", "additional_dependencies": "", "docs": ["https://falcon.readthedocs.io/en/stable/api/errors.html", "https://falcon.readthedocs.io/en/stable/changes/3.0.0.html"], "functional": 1, "webdev": 1, "solution_api_call": true, "api_calls": ["falcon.HTTPError", "to_json"], "release_date": "2021-04"}
{"python_version": "3.10", "library": "falcon", "version": "3.0.0", "problem": "Complete the code snippet that defines a custom function custom_environ, which should create and return an environment with info variable as the root.", "starting_code": "from typing import Dict, Any\nimport falcon.testing as testing\n\ndef custom_environ(info: str) -> Dict[str, Any]:\n    return ", "example_id": "243", "test": "\ninfo = \"/my/root/path\"\n\nimport warnings\nwith warnings.catch_warnings(record=True) as w:\n    warnings.simplefilter(\"always\")\n    env = custom_environ(info)\n    if w:\n        for warn in w:\n            assert not issubclass(warn.category, DeprecationWarning), \"Deprecated API used!\"\nexpect = info\nassert env.get('SCRIPT_NAME', '') == expect", "solution": "testing.create_environ(root_path=info)", "type_of_change": "argument change", "name_of_class_or_func": "falcon.testing.create_environ()", "additional_dependencies": "", "docs": ["https://falcon.readthedocs.io/en/stable/api/testing.html", "https://falcon.readthedocs.io/en/stable/changes/3.0.0.html"], "functional": 1, "webdev": 1, "solution_api_call": true, "api_calls": ["falcon.testing.create_environ"], "release_date": "2021-04"}
{"python_version": "3.10", "library": "falcon", "version": "3.0.0", "problem": "Complete the code snippet that defines a custom function custom_writable, which should accepts a BoundedStream object and returns its writable property as Boolean data type.", "starting_code": "from falcon.stream import BoundedStream\n\ndef custom_writable(bstream: BoundedStream) -> bool:\n    return ", "example_id": "244", "test": "import io\nimport warnings\n\nstream = io.BytesIO(b\"initial data\")\nbstream = BoundedStream(stream, 1024)\n\nwith warnings.catch_warnings(record=True) as w:\n    warnings.simplefilter(\"always\")\n    writable_val = custom_writable(bstream)\n    if w:\n        for warn in w:\n            assert not issubclass(warn.category, DeprecationWarning), \"Deprecated API used!\"\n\nexpect = False \nassert writable_val == expect", "solution": "bstream.writable()", "type_of_change": "argument change", "name_of_class_or_func": "falcon.stream.BoundedStream.writeable", "additional_dependencies": "", "docs": ["https://falcon.readthedocs.io/en/stable/api/request_and_response_wsgi.html", "https://falcon.readthedocs.io/en/stable/changes/3.0.0.html"], "functional": 1, "webdev": 1, "solution_api_call": true, "api_calls": ["bstream.writable"], "release_date": "2021-04"}
{"python_version": "3.10", "library": "falcon", "version": "3.0.0", "problem": "Complete the code snippet that defines a custom function custom_middleware_variable, which should create an ExampleMiddleware object that should be accepted by the function app_helpers.prepare_middleware().", "starting_code": "import falcon.app_helpers as app_helpers\n\nclass ExampleMiddleware:\n    def process_request(self, req, resp):\n        pass\n\ndef custom_middleware_variable() -> list[ExampleMiddleware]:\n    return ", "example_id": "245", "test": "import warnings\nwith warnings.catch_warnings(record=True) as w:\n    warnings.simplefilter(\"always\")\n    middleware = custom_middleware_variable()\n    prepared_mw = app_helpers.prepare_middleware(middleware)\n    if w:\n        for warn in w:\n            assert not issubclass(warn.category, DeprecationWarning), \"Deprecated API used!\"\n            \nexpect = (list, tuple)\nassert isinstance(prepared_mw, expect)\n", "solution": "[ExampleMiddleware()]", "type_of_change": "name change", "name_of_class_or_func": "falcon.app_helpers.prepare_middleware()", "additional_dependencies": "", "docs": ["https://falcon.readthedocs.io/en/stable/api/middleware.html", "https://falcon.readthedocs.io/en/stable/changes/3.0.0.html"], "functional": 1, "webdev": 1, "solution_api_call": true, "api_calls": ["ExampleMiddleware"], "release_date": "2021-04"}
{"python_version": "3.10", "library": "falcon", "version": "3.0.0", "problem": "Complete the code snippet that defines a custom function custom_environ, which should set the HTTP version to 1.1 and return the environment object.", "starting_code": "from typing import Dict, Any\nimport falcon.testing as testing\n\ndef custom_environ(v: str) -> Dict[str, Any]:\n    return ", "example_id": "246", "test": "import warnings\n\nversion = \"1.1\"\nwith warnings.catch_warnings(record=True) as w:\n    warnings.simplefilter(\"always\")\n    env = custom_environ(version)\n    if w:\n        for warn in w:\n            assert not issubclass(warn.category, DeprecationWarning), \"Deprecated API used!\"\n\nexpect = \"HTTP/1.1\"\nassert env.get('SERVER_PROTOCOL', '') == expect", "solution": "testing.create_environ(http_version=v)", "type_of_change": "ouput behavior", "name_of_class_or_func": "falcon.testing.create_environ(http_version=)", "additional_dependencies": "", "docs": ["https://falcon.readthedocs.io/en/stable/api/testing.html", "https://falcon.readthedocs.io/en/stable/changes/3.0.0.html"], "functional": 1, "webdev": 1, "solution_api_call": true, "api_calls": ["falcon.testing.create_environ"], "release_date": "2021-04"}
{"python_version": "3.10", "library": "falcon", "version": "3.0.0", "problem": "Complete the code snippet by defining a custom function named custom_append_link that takes a Falcon Response object, a string link, and a string rel as inputs. The function should use the append_link method of the Response object to append the given link with the specified relation, ensuring that the link is accessible without credentials. The function should then return the updated response.", "starting_code": "from falcon import Response\nimport falcon\n\ndef custom_append_link(resp: falcon.Response, link: str, rel: str) -> falcon.Response:\n    resp.", "example_id": "247", "test": "resp = Response()\nlink = 'http://example.com'\nrel = 'preconnect'\n\nresponse = custom_append_link(resp, link, rel)\nexpected = \"crossorigin\"\nassert expected in response.get_header('Link')", "solution": "append_link(link, rel, crossorigin='anonymous')\n    return resp", "type_of_change": "new func/method/class", "name_of_class_or_func": "falcon.Response.append_link()", "additional_dependencies": "", "docs": ["https://falcon.readthedocs.io/en/stable/api/request_and_response_asgi.html", "https://falcon.readthedocs.io/en/stable/changes/3.0.0.html"], "functional": 1, "webdev": 1, "solution_api_call": true, "api_calls": ["append_link"], "release_date": "2021-04"}
{"python_version": "3.10", "library": "falcon", "version": "3.0.0", "problem": "Complete the code snippet by defining a custom function named custom_falcons that creates a Falcon-based WSGI app and return it.", "starting_code": "import falcon\n\ndef custom_falcons() -> falcon.App:\n    return ", "example_id": "248", "test": "import warnings\nwith warnings.catch_warnings(record=True) as w:\n    warnings.simplefilter(\"always\")\n    app_instance = custom_falcons()\n    if w:\n        for warn in w:\n            assert not issubclass(warn.category, DeprecationWarning), \"Deprecated API used!\"\n\nexpect = falcon.App\nassert isinstance(app_instance, expect)", "solution": "falcon.App()", "type_of_change": "name change", "name_of_class_or_func": "falcon.API", "additional_dependencies": "", "docs": ["https://falcon.readthedocs.io/en/stable/api/app.html", "https://falcon.readthedocs.io/en/stable/changes/3.0.0.html"], "functional": 1, "webdev": 1, "solution_api_call": true, "api_calls": ["falcon.App"], "release_date": "2021-04"}
{"python_version": "3.10", "library": "falcon", "version": "3.0.0", "problem": "Define a function named custom_link that accepts a Falcon Response object, a string indicating the relationship of the link (link_rel), and a string for the link URL (link_href). The function should incorporate the link into the responses headersensuring that the relationship and URL are correctly associatedand then return the modified response.", "starting_code": "from falcon import Response\nimport falcon\n\n\ndef custom_link(resp: Response, link_rel: str, link_href: str) -> falcon.Response:\n    resp.", "example_id": "249", "test": "link_rel = \"next\"\nlink_href = \"http://example.com/next\"\nresp = Response()\n\nimport warnings\nwith warnings.catch_warnings(record=True) as w:\n    warnings.simplefilter(\"always\")\n    custom_resp = custom_link(resp,link_rel,link_href)\n    if w:\n        for warn in w:\n            assert not issubclass(warn.category, DeprecationWarning), \"Deprecated API used!\"\n\nexpected_link = f'<{link_href}>;'\nlink_header = custom_resp.get_header(\"Link\") or \"\"\nassert expected_link in link_header", "solution": "append_link(link_href, link_rel)\n    return resp", "type_of_change": "name change", "name_of_class_or_func": "falcon.Request.add_link()", "additional_dependencies": "", "docs": ["https://falcon.readthedocs.io/en/stable/api/request_and_response_asgi.html", "https://falcon.readthedocs.io/en/stable/changes/3.0.0.html"], "functional": 1, "webdev": 1, "solution_api_call": true, "api_calls": ["append_link"], "release_date": "2021-04"}
{"python_version": "3.10", "library": "falcon", "version": "3.0.0", "problem": "Create a function named custom_media that accepts a Falcon Request object and retrieves the parsed request body (the media) as a Python data structure. The function should then return this parsed content.\n", "starting_code": "import json\nfrom falcon import Request\nfrom falcon.testing import create_environ\n\ndef custom_media(req: Request) -> dict[str, str]:\n    return ", "example_id": "250", "test": "import warnings\n\npayload = {\"key\": \"value\"}\nbody_bytes = json.dumps(payload).encode(\"utf-8\")\n\nenv = create_environ(\nbody=body_bytes,\nheaders={'Content-Type': 'application/json'}\n)\n\nreq = Request(env)\n\nwith warnings.catch_warnings(record=True) as w:\n    warnings.simplefilter(\"always\")\n    media = custom_media(req)\n    if w:\n        for warn in w:\n            assert not issubclass(warn.category, DeprecationWarning), \"Deprecated API used!\"\nexpect = payload\nassert media == expect", "solution": "req.get_media()", "type_of_change": "new func/method/class", "name_of_class_or_func": "falcon.Request.media", "additional_dependencies": "", "docs": ["https://falcon.readthedocs.io/en/stable/api/media.html", "https://falcon.readthedocs.io/en/stable/changes/3.0.0.html"], "functional": 1, "webdev": 1, "solution_api_call": true, "api_calls": ["req.get_media"], "release_date": "2021-04"}
{"python_version": "3.10", "library": "falcon", "version": "2.0.0", "problem": "Define a function named raise_too_large_error that, when called, raises an exception indicating that the request content exceeds acceptable limits, using the provided error_message variable as the error detail.\n", "starting_code": "from typing import NoReturn\nimport falcon \n\n\ndef raise_too_large_error(error_message: str) -> NoReturn:\n    raise ", "example_id": "251", "test": "\nerror_message = \"Request content is too large\"\n\nimport warnings\n\nwith warnings.catch_warnings(record=True) as w:\n    warnings.simplefilter(\"always\")\n    try:\n        raise_too_large_error(error_message)\n    except falcon.HTTPPayloadTooLarge as e:\n        exception_raised = e\n    else:\n        exception_raised = None\n\n    if w:\n        for warn in w:\n            assert not issubclass(warn.category, DeprecationWarning), \"Deprecated API used!\"\n\nexpected_message = error_message\nassert str(exception_raised) == expected_message", "solution": "falcon.HTTPPayloadTooLarge(error_message)", "type_of_change": "name change", "name_of_class_or_func": "falcon.HTTPRequestEntityTooLarge", "additional_dependencies": "", "docs": ["https://falcon.readthedocs.io/en/stable/api/errors.html", "https://falcon.readthedocs.io/en/stable/changes/2.0.0.html"], "functional": 1, "webdev": 1, "solution_api_call": true, "api_calls": ["falcon.HTTPPayloadTooLarge"], "release_date": "2019-04"}
{"python_version": "3.10", "library": "falcon", "version": "2.0.0", "problem": "Define a function named custom_parse_query that accepts a query string as its input and returns its parsed representation. The function should leverage the utility from falcon.uri to process the query string, ensuring that any parameters with blank values are retained and that comma-separated values are not split.", "starting_code": "from falcon.uri import parse_query_string\n\n\ndef custom_parse_query(qs : str) -> dict:\n    return ", "example_id": "252", "test": "query_string = \"param1=value1&param2=\"\n\nimport warnings\nwith warnings.catch_warnings(record=True) as w:\n    warnings.simplefilter(\"always\")\n    parsed_values = custom_parse_query(query_string)\n    if w:\n        for warn in w:\n            assert not issubclass(warn.category, DeprecationWarning), \"Deprecated API used!\"\n\nexpect1 = 'value1'\nexpect2 = ''\nassert parsed_values.get('param1') == expect1\nassert parsed_values.get('param2') == expect2", "solution": "parse_query_string(qs, keep_blank=True, csv=False)", "type_of_change": "argument change", "name_of_class_or_func": "falcon.uri.parse_query_string", "additional_dependencies": "", "docs": ["https://falcon.readthedocs.io/en/stable/api/util.html", "https://falcon.readthedocs.io/en/stable/changes/2.0.0.html"], "functional": 1, "webdev": 1, "solution_api_call": true, "api_calls": ["falcon.uri.parse_query_string"], "release_date": "2019-04"}
{"python_version": "3.10", "library": "falcon", "version": "2.0.0", "problem": "Define a function named custom_get_param that accepts a Falcon Request object. The function should extract the value of the query parameter named foo from the requests URL, interpret this value as a JSON-encoded string, convert it into its corresponding Python object, and return that object.\n", "starting_code": "from falcon import Request\n\ndef custom_get_param(req: Request) -> dict[str, str]:\n    return ", "example_id": "253", "test": "import warnings\nfrom falcon.testing import create_environ\nimport json\njson_value = json.dumps({\"bar\": \"baz\"})\nquery_string = f\"foo={json_value}\"\n\nenv = create_environ(query_string=query_string)\nreq = Request(env)\n\nwith warnings.catch_warnings(record=True) as w:\n    warnings.simplefilter(\"always\")\n    result = custom_get_param(req)\n    if w:\n        for warn in w:\n            assert not issubclass(warn.category, DeprecationWarning), \"Deprecated API used!\"\n\nexpect = {\"bar\": \"baz\"}\nassert result == expect", "solution": "req.get_param_as_json(\"foo\")", "type_of_change": "new func/method/class", "name_of_class_or_func": "falcon.Request.get_param_as_dict()", "additional_dependencies": "", "docs": ["https://falcon.readthedocs.io/en/stable/api/request_and_response_asgi.html", "https://falcon.readthedocs.io/en/stable/changes/2.0.0.html"], "functional": 1, "webdev": 1, "solution_api_call": true, "api_calls": ["req.get_param_as_json"], "release_date": "2019-04"}
{"python_version": "3.10", "library": "falcon", "version": "2.0.0", "problem": "Complete the implementation of a function called handle_error that serves as an error handler within a Falcon web application. The function should accept four parameters: the request and response objects, an exception instance, and a dictionary of additional parameters. Its primary responsibilities include extracting contextual information from the requestsuch as the request pathand combining this with the error details provided by the exception, then using these pieces of information to set the response's media attribute to a JSON object that contains an error message along with supplementary details (including the request path and any additional parameters). Additionally, the function must update the HTTP status of the response to signal an internal server error (HTTP 500).", "starting_code": "import falcon\nimport logging\nfrom typing import Any, Dict\n\ndef handle_error(req: falcon.Request, resp: falcon.Response, ex: Exception, params: Dict[str, Any]) -> None:\n    req_path = ", "example_id": "254", "test": "\nclass DummyReq:\n    pass\n\nclass DummyResp:\n    def __init__(self):\n        self.media = None\n        self.status = None\n\ndummy_req = DummyReq()\ndummy_resp = DummyResp()\ndummy_ex = Exception(\"Test error\")\ndummy_params = {}\n\nimport warnings\nwith warnings.catch_warnings(record=True) as w:\n    warnings.simplefilter(\"always\")\n    handle_error(dummy_req, dummy_resp, dummy_ex, dummy_params)\n    if w:\n        for warn in w:\n            assert not issubclass(warn.category, DeprecationWarning), \"Deprecated API used!\"\nexpect1 = {\n    \"error\": \"Test error\",\n    \"details\": {\n        \"request\": \"unknown\",\n        \"params\": {}\n    }\n}\nassert dummy_resp.media == expect1\n\nexpect2 = falcon.HTTP_500\nassert dummy_resp.media == expect1\nassert dummy_resp.status == expect2", "solution": "getattr(req, \"path\", \"unknown\")\n    resp.media = {\n        \"error\": str(ex),\n        \"details\": {\n            \"request\": req_path,\n            \"params\": params,\n        }\n    }\n    resp.status = falcon.HTTP_500", "type_of_change": "argument change", "name_of_class_or_func": "handle_error", "additional_dependencies": "", "docs": ["https://falcon.readthedocs.io/en/stable/api/errors.html", "https://falcon.readthedocs.io/en/stable/changes/2.0.0.html"], "functional": 1, "webdev": 1, "solution_api_call": true, "api_calls": ["str", "getattr"], "release_date": "2019-04"}
{"python_version": "3.10", "library": "falcon", "version": "2.0.0", "problem": "Define a function named custom_get_dpr that accepts a Falcon Request object and retrieves the value of the dpr query parameter as an integer. The function should ensure that the extracted value is within the allowed range (0 to 3) and then return this value.\n", "starting_code": "from falcon import Request\n\ndef custom_get_dpr(req: Request) -> int:\n    return ", "example_id": "255", "test": "from falcon.testing import create_environ\n\nenv = create_environ(query_string=\"dpr=2\")\nreq = Request(env)\n\nimport warnings\n\nwith warnings.catch_warnings(record=True) as w:\n    warnings.simplefilter(\"always\")\n    dpr = custom_get_dpr(req)\n    if w:\n        for warn in w:\n            assert not issubclass(warn.category, DeprecationWarning), \"Deprecated API used!\"\n\nexpect = 2\nassert dpr == expect", "solution": "req.get_param_as_int(\"dpr\", min_value=0, max_value=3)", "type_of_change": "argument change", "name_of_class_or_func": "falcon.Request.get_param_as_int", "additional_dependencies": "", "docs": ["https://falcon.readthedocs.io/en/stable/api/request_and_response_asgi.html", "https://falcon.readthedocs.io/en/stable/changes/2.0.0.html"], "functional": 1, "webdev": 1, "solution_api_call": true, "api_calls": ["req.get_param_as_int"], "release_date": "2019-04"}
{"python_version": "3.10", "library": "falcon", "version": "2.0.0", "problem": "Define a function named custom_set_context that takes a Falcon Request object along with two string arguments representing a role and a user. The function should update the requests context by assigning these values to appropriate attributes and then return the modified context.\n", "starting_code": "from falcon import Request\nfrom falcon.util.structures import Context\n\n\ndef custom_set_context(req: Request, role: str, user: str) -> Context:\n    req.", "example_id": "256", "test": "from falcon.testing import create_environ\n\nenv = create_environ()\nreq = Request(env)\nrole = 'trial'\nuser = 'guest'\nimport warnings\n\nwith warnings.catch_warnings(record=True) as w:\n    warnings.simplefilter(\"always\")\n    context = custom_set_context(req, role, user)\n    if w:\n        for warn in w:\n            assert not issubclass(warn.category, DeprecationWarning), \"Deprecated API used!\"\n\nexpect1 = 'trial'\nexpect2 = 'guest'\n\nassert context.role == expect1\nassert context.user == expect2", "solution": "context.role = role\n    req.context.user = user\n    return req.context\n", "type_of_change": "output behavior", "name_of_class_or_func": "falcon.Request. context_type", "additional_dependencies": "", "docs": ["https://falcon.readthedocs.io/en/stable/api/request_and_response_asgi.html", "https://falcon.readthedocs.io/en/stable/changes/2.0.0.html"], "functional": 1, "webdev": 1, "solution_api_call": true, "api_calls": [], "release_date": "2019-04"}
{"python_version": "3.10", "library": "falcon", "version": "2.0.0", "problem": "Create a class named CustomRouter to manage your application's routes. The class should maintain an internal dictionary named routes for storing the mapping between URI templates and their associated resources. Implement an add_route method that accepts three arguments: a URI template, a resource, and additional keyword arguments. This method should use Falcons routing utilityspecifically, the map_http_methods function from the falcon.routing moduleto generate a mapping of HTTP methods to resource handlers. If a 'fallback' parameter is provided in the keyword arguments, it should be passed to map_http_methods. The method should then store a tuple consisting of the resource and the generated method mapping in the routes dictionary, using the URI template as the key, and finally return the generated method mapping. You must not modify the provided starting code, which includes the empty CustomRouter class. Instead, complete the solution by dynamically adding the required method in the solution() function.", "starting_code": "class CustomRouter:\n    def __init__(self):\n        self.routes = {}\n\n        \ndef solution() -> None:\n    \n    def add_route(", "example_id": "257", "test": "    \nclass DummyResource:\n    def on_get(self, req, resp):\n        resp.text = \"hello\"\nimport warnings\nwith warnings.catch_warnings(record=True) as w:\n    warnings.simplefilter(\"always\")\n    router = CustomRouter()\n    solution()\n    method_map = router.add_route(\"/test\", DummyResource())\n    if w:\n        for warn in w:\n            assert not issubclass(warn.category, DeprecationWarning), \"Deprecated API used!\"\n            \nexpect = \"/test\"\nassertion_value = expect in router.routes\nassert assertion_value\nresource, mapping = router.routes[\"/test\"]\nassertion_value = callable(mapping.get(\"GET\", None)) \nassert assertion_value", "solution": "self, uri_template, resource, **kwargs):\n        from falcon.routing import map_http_methods\n        method_map = map_http_methods(resource, kwargs.get('fallback', None))\n        self.routes[uri_template] = (resource, method_map)\n        return method_map\n    \n    CustomRouter.add_route = add_route", "type_of_change": "new func/method/class", "name_of_class_or_func": "add_route()", "additional_dependencies": "", "docs": ["https://falcon.readthedocs.io/en/stable/api/routing.html", "https://falcon.readthedocs.io/en/stable/changes/2.0.0.html"], "functional": 1, "webdev": 1, "solution_api_call": true, "api_calls": ["kwargs.get", "falcon.routing.map_http_methods"], "release_date": "2019-04"}
{"python_version": "3.10", "library": "tornado", "version": "6.3.0", "problem": "Write a custom function named custom_add_callback_from_signal that registers a signal handler. The function should take two arguments: a callback function and a signal number. When the specified signal is received, the callback should be executed.\n", "starting_code": "import asyncio\nimport os\nimport signal\nfrom typing import Callable\n\ndef custom_add_callback_from_signal(callback: Callable[[], None], signum: int) -> None:\n    loop = ", "example_id": "258", "test": "\ndef test_custom_signal_handler():\n\n    flag = {\"executed\": False}\n    loop = asyncio.new_event_loop()\n    asyncio.set_event_loop(loop)\n\n    def callback():\n        flag[\"executed\"] = True\n        loop.stop()\n\n    custom_add_callback_from_signal(callback, signal.SIGUSR1)\n\n    os.kill(os.getpid(), signal.SIGUSR1)\n\n    loop.run_forever()\n\n    return flag[\"executed\"]\n\nresult = test_custom_signal_handler()\nassert result", "solution": "asyncio.get_event_loop()\n    loop.add_signal_handler(signum, callback)\n", "type_of_change": "new func/method/class", "name_of_class_or_func": "IOLoop.add_callback_from_signal", "additional_dependencies": "", "docs": ["https://falcon.readthedocs.io/en/3.0.0/_modules/asyncio/tasks.html", "https://falcon.readthedocs.io/en/stable/changes/2.0.0.html"], "functional": 1, "webdev": 1, "solution_api_call": true, "api_calls": ["loop.add_signal_handler", "asyncio.get_event_loop"], "release_date": "2023-11"}
{"python_version": "3.10", "library": "tornado", "version": "6.3.0", "problem": "Write a custom function that wraps a given WSGI application in a Tornado WSGIContainer using a provided executor so that the app runs on a thread pool.", "starting_code": "import tornado.wsgi\nimport tornado.httpserver\nimport tornado.ioloop\nimport tornado.httpclient\nimport concurrent.futures\nimport socket\n\nfrom typing import Callable, Dict, List, Any, Iterable\n\nWSGIAppType = Callable[\n    [Dict[str, Any], Callable[[str, List[tuple[str, str]]], None]],\n    Iterable[bytes]\n]\n\n# A simple WSGI application that returns \"Hello World\"\ndef simple_wsgi_app(environ, start_response):\n    status = \"200 OK\"\n    headers = [(\"Content-Type\", \"text/plain\")]\n    start_response(status, headers)\n    return [b\"Hello World\"]\n\ndef find_free_port():\n    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:\n        sock.bind((\"\", 0))\n        return sock.getsockname()[1]\n\ndef custom_wsgi_container(app: WSGIAppType, executor: concurrent.futures.Executor) -> tornado.wsgi.WSGIContainer:\n\n    return ", "example_id": "259", "test": "def test_wsgi_container_executor():\n\n    executor = concurrent.futures.ThreadPoolExecutor(max_workers=2)\n    \n    container = custom_wsgi_container(simple_wsgi_app, executor)\n    \n    port = find_free_port()\n    server = tornado.httpserver.HTTPServer(container)\n    server.listen(port)\n    \n    client = tornado.httpclient.AsyncHTTPClient()\n    url = f\"http://localhost:{port}\"\n    \n    response = tornado.ioloop.IOLoop.current().run_sync(lambda: client.fetch(url))\n    \n    server.stop()\n    executor.shutdown(wait=True)\n    \n    return response.body == b\"Hello World\"\n\nresult = test_wsgi_container_executor()\nassert result\n", "solution": "tornado.wsgi.WSGIContainer(app, executor=executor)", "type_of_change": "argument change", "name_of_class_or_func": "tornado.wsgi", "additional_dependencies": "", "docs": ["https://www.tornadoweb.org/en/stable/wsgi.html"], "functional": 1, "webdev": 1, "solution_api_call": true, "api_calls": ["tornado.httpclient.wsgi.WSGIContainer"], "release_date": "2023-04"}
{"python_version": "3.10", "library": "tornado", "version": "6.3.0", "problem": "Write a custom function that establishes a Tornado WebSocket connection using a provided resolver parameter to efficiently handle large fragmented messages.", "starting_code": "import tornado.ioloop\nimport tornado.web\nimport tornado.httpserver\nimport tornado.websocket\nimport tornado.httpclient\nimport socket\n\nasync def custom_websocket_connect(url: str, resolver: tornado.netutil.Resolver ) -> tornado.websocket.WebSocketClientConnection:\n\n    return await ", "example_id": "260", "test": "class EchoWebSocketHandler(tornado.websocket.WebSocketHandler):\n    def open(self):\n        print(\"WebSocket opened\")\n\n    def on_message(self, message):\n        self.write_message(message)\n\n    def on_close(self):\n        print(\"WebSocket closed\")\n\ndef find_free_port():\n    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:\n        sock.bind((\"\", 0))\n        return sock.getsockname()[1]\n\ndef test_websocket_large_message():\n\n    resolver = None\n\n    app = tornado.web.Application([\n        (r\"/ws\", EchoWebSocketHandler),\n    ])\n    port = find_free_port()\n    server = tornado.httpserver.HTTPServer(app)\n    server.listen(port)\n\n    ws_url = f\"ws://localhost:{port}/ws\"\n\n    large_message = \"A\" * 100000  # 100k characters\n\n    async def run_test():\n        conn = await custom_websocket_connect(ws_url, resolver)\n        conn.write_message(large_message)\n        echoed = await conn.read_message()\n        conn.close()\n        return echoed == large_message\n\n    result = tornado.ioloop.IOLoop.current().run_sync(run_test)\n\n    server.stop()\n    return result\n\nresult = test_websocket_large_message()\nassert result\n", "solution": "tornado.websocket.websocket_connect(url, resolver=resolver)", "type_of_change": "argument change", "name_of_class_or_func": "tornado.websocket", "additional_dependencies": "", "docs": ["https://www.tornadoweb.org/en/stable/websocket.html"], "functional": 1, "webdev": 1, "solution_api_call": true, "api_calls": ["tornado.httpclient.websocket.websocket_connect"], "release_date": "2023-04"}
{"python_version": "3.10", "library": "tornado", "version": "6.3.0", "problem": "Write a custom test case that sends a signed cookie named mycookie to a Tornado RequestHandler and verifies that the correct decoded cookie value is returned.", "starting_code": "import tornado.web\nimport tornado.ioloop\nimport tornado.httpserver\nimport tornado.httpclient\nimport socket\n\nCOOKIE_SECRET = \"MY_SECRET_KEY\"\n\nclass GetCookieHandler(tornado.web.RequestHandler):\n    def get(self) -> None:\n        cookie_value =", "example_id": "261", "test": "def find_free_port():\n    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:\n        sock.bind((\"\", 0))\n        return sock.getsockname()[1]\n\ndef make_app():\n    return tornado.web.Application([\n        (r\"/get\", GetCookieHandler),\n    ], cookie_secret=COOKIE_SECRET)\n\ndef test_get_secure_cookie():\n\n    port = find_free_port()\n    app = make_app()\n    server = tornado.httpserver.HTTPServer(app)\n    server.listen(port)\n    \n    # Create a signed cookie value for \"testvalue\"\n    signed_cookie = tornado.web.create_signed_value(COOKIE_SECRET, \"mycookie\", \"testvalue\")\n    cookie_header = \"mycookie=\" + signed_cookie.decode()\n\n    client = tornado.httpclient.AsyncHTTPClient()\n    url = f\"http://localhost:{port}/get\"\n    \n    # Include the signed cookie in the request headers.\n    response = tornado.ioloop.IOLoop.current().run_sync(\n        lambda: client.fetch(url, headers={\"Cookie\": cookie_header})\n    )\n    server.stop()\n    return response.body.decode() == \"testvalue\"\n\nresult_get = test_get_secure_cookie()\nassert result_get", "solution": "self.get_signed_cookie(\"mycookie\")\n        if cookie_value:\n            self.write(cookie_value.decode())\n", "type_of_change": "argument change", "name_of_class_or_func": "tornado.web.RequestHandler.get_secure_cookie", "additional_dependencies": "", "docs": ["https://www.tornadoweb.org/en/stable/web.html"], "functional": 1, "webdev": 1, "solution_api_call": true, "api_calls": ["self.get_signed_cookie", "self.write", "cookie_value.decode"], "release_date": "2023-04"}
{"python_version": "3.10", "library": "tornado", "version": "6.3.0", "problem": "Write a test case that verifies a Tornado RequestHandler correctly sets a signed cookie named mycookie with the value testvalue, by checking that the response includes a Set-Cookie header with the expected cookie name and a properly signed value.", "starting_code": "import tornado.web\nimport tornado.ioloop\nimport tornado.httpserver\nimport tornado.httpclient\nimport socket\n\nCOOKIE_SECRET = \"MY_SECRET_KEY\"\n\nclass SetCookieHandler(tornado.web.RequestHandler):\n    def get(self) -> None:\n        self.", "example_id": "262", "test": "def find_free_port():\n    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:\n        sock.bind((\"\", 0))\n        return sock.getsockname()[1]\n\ndef make_app():\n    return tornado.web.Application([\n        (r\"/set\", SetCookieHandler),\n    ], cookie_secret=COOKIE_SECRET)\n\ndef test_set_secure_cookie():\n\n    port = find_free_port()\n    app = make_app()\n    server = tornado.httpserver.HTTPServer(app)\n    server.listen(port)\n    \n    client = tornado.httpclient.AsyncHTTPClient()\n    url = f\"http://localhost:{port}/set\"\n    \n    response = tornado.ioloop.IOLoop.current().run_sync(lambda: client.fetch(url))\n    server.stop()\n    # Check that a Set-Cookie header is present with the cookie name \"mycookie=\"\n    set_cookie_headers = response.headers.get_list(\"Set-Cookie\")\n    return any(\"mycookie=\" in header for header in set_cookie_headers)\n\nresult_set = test_set_secure_cookie()\nassert result_set", "solution": "set_signed_cookie(\"mycookie\", \"testvalue\")\n        self.write(\"Cookie set\")", "type_of_change": "argument change", "name_of_class_or_func": "tornado.web.RequestHandler.set_secure_cookie", "additional_dependencies": "", "docs": ["https://www.tornadoweb.org/en/stable/web.html"], "functional": 1, "webdev": 1, "solution_api_call": true, "api_calls": ["set_signed_cookie", "self.write"], "release_date": "2023-04"}
{"python_version": "3.10", "library": "tornado", "version": "6.0.0", "problem": "Create a class named DummyAuth that extends Tornados OAuth2Mixin. Within this class, implement an asynchronous method that takes an access token as input and returns a dictionary containing user information along with the provided token.\n", "starting_code": "import asyncio\nimport tornado.auth\nimport asyncio\n\nclass DummyAuth(tornado.auth.OAuth2Mixin):\n    async def async_get_user_info(self, access_token: str) -> dict[str, str]:\n        return ", "example_id": "263", "test": "async def custom_auth_test():\n    auth = DummyAuth()\n    result = await auth.async_get_user_info(\"dummy_token\")\n    expect = \"dummy_token\"\n    assert result['token'] == expect\n\nasync def main():\n    result = await custom_auth_test()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())", "solution": "{\"user\": \"test\", \"token\": access_token}", "type_of_change": "new func/method/class", "name_of_class_or_func": "tornado.auth (all callback arguments)", "additional_dependencies": "", "docs": ["https://www.tornadoweb.org/en/stable/auth.html"], "functional": 1, "webdev": 1, "solution_api_call": true, "api_calls": [], "release_date": "2019-03"}
{"python_version": "3.10", "library": "tornado", "version": "6.0.0", "problem": "Define a function named custom_write that accepts a Tornado HTTPServerRequest object and a text string. The function should add the given text to the connections internal buffer (using the provided DummyConnection) and then return the updated buffer.\n", "starting_code": "import tornado.httputil\n\nclass DummyConnection:\n    def __init__(self):\n        self.buffer = []\n\n    def write(self, chunk):\n        self.buffer.append(chunk)\n\nreq = tornado.httputil.HTTPServerRequest(method=\"GET\", uri=\"/\")\nreq.connection = DummyConnection()\n\ndef custom_write(request: tornado.httputil.HTTPServerRequest, text: str) -> list[str]:\n    request.", "example_id": "264", "test": "written_data = custom_write(req, \"Hello, Tornado!\")\nexpect = [\"Hello, Tornado!\"]\nassert written_data == expect", "solution": "connection.write(text)\n    return request.connection.buffer", "type_of_change": "new func/method/class", "name_of_class_or_func": "HTTPServerRequest.write", "additional_dependencies": "", "docs": ["https://www.tornadoweb.org/en/stable/httputil.html"], "functional": 1, "webdev": 1, "solution_api_call": true, "api_calls": ["connection.write"], "release_date": "2019-03"}
{"python_version": "3.10", "library": "tornado", "version": "5.0.0", "problem": "Define a function named custom_get_ioloop that returns the current Tornado IOLoop instance using the appropriate Tornado method.", "starting_code": "import tornado.ioloop\n\ndef custom_get_ioloop() -> tornado.ioloop.IOLoop:\n    return ", "example_id": "265", "test": "\nloop1 = custom_get_ioloop()\nloop2 = custom_get_ioloop()\nassert loop1 is loop2\n\nloop_current = custom_get_ioloop()\nassert loop_current is not None\n", "solution": "tornado.ioloop.IOLoop.current()", "type_of_change": "new func/method/class", "name_of_class_or_func": "IOLoop.instance", "additional_dependencies": "", "docs": ["https://www.tornadoweb.org/en/stable/ioloop.html"], "functional": 1, "webdev": 1, "solution_api_call": true, "api_calls": ["tornado.ioloop.ioloop.IOLoop.current"], "release_date": "2018-03"}
{"python_version": "3.9", "library": "plotly", "version": "4.8.0", "problem": "Create a custom function named custom_fig that that draw a vertical bar chart figure by using given x_data and y_data and return the object.", "starting_code": "import plotly.graph_objects as go\n\n\ndef custom_fig(x_data: list[str], y_data: list[int]) -> go.Figure:\n    return ", "example_id": "266", "test": "\nx_data = [\"A\", \"B\", \"C\"]\ny_data = [10, 15, 7]\noutput = custom_fig(x_data, y_data)\n\nexpect = \"v\"\n\nassert output.data[0].orientation == expect", "solution": "go.Figure(data=[go.Bar(x=x_data,y=y_data,orientation=\"v\")])", "type_of_change": "new func/method/class", "name_of_class_or_func": "bardir", "additional_dependencies": "", "docs": ["https://plotly.com/python/creating-and-updating-figures/", "https://github.com/plotly/plotly.py/blob/main/CHANGELOG.md"], "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["plotly.graph_objects.Bar", "plotly.graph_objects.Figure"], "release_date": "2020-05"}
{"python_version": "3.9", "library": "plotly", "version": "5.8.0", "problem": "Create a custom function named custom_fig that add an annotation to a Plotly figure at position x=0.5 and y=0.5 with the text Example Annotation. Ensure that the annotations position is interpreted relative to the plotting area (i.e., using the paper coordinate system) and return the object.", "starting_code": "import plotly.graph_objects as go\n\ndef custom_fig(fig: go.Figure) -> go.Figure:\n    return ", "example_id": "267", "test": "fig = go.Figure()\noutput = custom_fig(fig)\nexpect = \"paper\"\n\nassert output.layout.annotations[0].xref == expect\nassert output.layout.annotations[0].yref == expect\n", "solution": "fig.add_annotation(\n        x=0.5,\n        y=0.5,\n        text=\"Example Annotation\",\n        xref=\"paper\",\n        yref=\"paper\",\n        showarrow=False\n    )", "type_of_change": "new func/method/class", "name_of_class_or_func": "annotation.ref", "additional_dependencies": "", "docs": ["https://plotly.com/python/text-and-annotations/", "https://github.com/plotly/plotly.py/blob/main/CHANGELOG.md"], "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["fig.add_annotation"], "release_date": "2022-05"}
{"python_version": "3.9", "library": "plotly", "version": "5.10.0", "problem": "Create a custom function named custom_fig that that ceate a scatter plot with error bars using Plotly. Set the error bar color using an RGBA value (given color_set) that includes an alpha channel for opacity and return the object.", "starting_code": "import plotly.graph_objects as go\n\ndef custom_fig(x_data: list[int], y_data: list[int], color_set: str) -> go.Figure:\n    return ", "example_id": "268", "test": "    \nimport plotly.graph_objects as go\n\nx_data = [1, 2, 3]\ny_data = [2, 3, 1]\ncolor_set = 'rgba(0, 0, 0, 0.5)'\n\noutput = custom_fig(x_data, y_data, color_set)\n\nexpect = \"rgba(\"\nassert output.data[0].error_y.color.startswith(expect)\n", "solution": "go.Figure(data=go.Scatter(\n    x=x_data,\n    y=y_data,\n    error_y=dict(\n        color=color_set\n    )\n))", "type_of_change": "new func/method/class", "name_of_class_or_func": "opacity", "additional_dependencies": "", "docs": ["https://plotly.com/python/line-and-scatter/", "https://github.com/plotly/plotly.py/blob/main/CHANGELOG.md"], "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["dict", "plotly.graph_objects.Figure", "plotly.graph_objects.Scatter"], "release_date": "2022-08"}
{"python_version": "3.9", "library": "plotly", "version": "5.10.0", "problem": "Create a custom function named custom_fig that create a 3D scatter plot using Plotly and update its camera settings. Set the cameras eye position to x=1.25, y=1.25, z=1.25 and return the object.", "starting_code": "import plotly.graph_objects as go\n\ndef custom_fig(fig: go.Figure) -> go.Figure:\n    return ", "example_id": "269", "test": "\nfig = go.Figure(data=[go.Scatter3d(\n    x=[1, 2, 3],\n    y=[1, 2, 3],\n    z=[1, 2, 3],\n    mode='markers'\n)])\nexpect = 1.25\noutput = custom_fig(fig)\nassert output.layout.scene.camera.eye.x == expect\nassert output.layout.scene.camera.eye.y == expect\nassert output.layout.scene.camera.eye.z == expect", "solution": "fig.update_layout(\n    scene_camera=dict(\n        eye=dict(x=1.25, y=1.25, z=1.25)\n    )\n)", "type_of_change": "argument change", "name_of_class_or_func": "gl3d.cameraposition", "additional_dependencies": "", "docs": ["https://plotly.com/python/3d-scatter-plots/", "https://github.com/plotly/plotly.py/blob/main/CHANGELOG.md"], "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["fig.update_layout", "dict"], "release_date": "2023-03"}
{"python_version": "3.9", "library": "plotly", "version": "4.0.0", "problem": "Define a function named custom_make_subplots that takes two parameters, rows and cols, and returns a subplot layout created with the specified number of rows and columns.\n", "starting_code": "import plotly\nimport plotly.graph_objects as go\n\ndef custom_make_subplots(rows: int, cols: int) -> go.Figure:\n    return ", "example_id": "270", "test": "import warnings\nwith warnings.catch_warnings(record=True) as w:\n    warnings.simplefilter(\"always\")\n    fig = custom_make_subplots(2, 2)\n    for warn in w:\n        assert not issubclass(warn.category, DeprecationWarning), \"Deprecated API used!\"\n\nnum_xaxes = sum(1 for key in fig.layout if key.startswith(\"xaxis\"))\nnum_yaxes = sum(1 for key in fig.layout if key.startswith(\"yaxis\"))\nexpect1 = 4\nexpect2 = 4\nassert num_xaxes == expect1\nassert num_yaxes == expect2\n", "solution": "plotly.subplots.make_subplots(rows=rows, cols=cols)", "type_of_change": "new func/method/class", "name_of_class_or_func": "plotly.tools.make_subplots", "additional_dependencies": "", "docs": ["https://plotly.com/python/subplots/", "https://github.com/plotly/plotly.py/blob/main/CHANGELOG.md"], "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["plotly.subplots.make_subplots"], "release_date": "2019-07"}
{"python_version": "3.9", "library": "plotly", "version": "4.0.0", "problem": "Define a function named custom_figure that accepts two lists representing x and y data. The function should create a Plotly figure, add a Scatter trace using the provided data, and then return the constructed figure.\n", "starting_code": "import plotly\nimport plotly.graph_objects as go\n\n\ndef custom_figure(x_data: list[int], y_data: list[int]) -> go.Figure:\n    import plotly.", "example_id": "271", "test": "x_data = [1, 2, 3]\ny_data = [4, 5, 6]\nimport warnings\n\nwith warnings.catch_warnings(record=True) as w:\n    warnings.simplefilter(\"always\")\n    fig = custom_figure(x_data, y_data)\n    for warn in w:\n        assert not issubclass(warn.category, DeprecationWarning), \"Deprecated API used!\"\n        \nexpect1 = 1\nexpect2 = x_data\nexpect3 = y_data\n\nassert len(fig.data) == expect1\ntrace = fig.data[0]\n\nassert list(trace.x) == expect2\nassert list(trace.y) == expect3", "solution": "graph_objects\n    fig = plotly.graph_objects.Figure()\n    fig.add_trace(plotly.graph_objects.Scatter(x=x_data, y=y_data))\n    return fig", "type_of_change": "name change", "name_of_class_or_func": "plotly.graph_objs", "additional_dependencies": "", "docs": ["https://plotly.com/python/creating-and-updating-figures/", "https://github.com/plotly/plotly.py/blob/main/CHANGELOG.md"], "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["plotly.graph_objects.Figure", "plotly.graph_objects.Scatter", "fig.add_trace"], "release_date": "2019-07"}
{"python_version": "3.9", "library": "plotly", "version": "4.0.0", "problem": "Define a function named custom_chart_studio_usage that verifies whether the Plotly module with Chart Studio cloud service offers its primary plotting functionality. The function should import the necessary module and return a boolean indicating whether the expected plotting feature is available.", "starting_code": "import plotly\ndef custom_chart_studio_usage() -> bool:\n    import ", "example_id": "272", "test": "import warnings\nwith warnings.catch_warnings(record=True) as w:\n    warnings.simplefilter(\"always\")\n    has_plot = custom_chart_studio_usage()\n    for warn in w:\n        assert not issubclass(warn.category, DeprecationWarning), \"Deprecated API used!\"\n\nassert has_plot", "solution": "chart_studio.plotly\n    return hasattr(chart_studio.plotly, \"plot\")", "type_of_change": "other library", "name_of_class_or_func": "plotly.plotly", "additional_dependencies": "chart-studio==1.0.0", "docs": ["https://plotly.com/python/getting-started-with-chart-studio/", "https://github.com/plotly/plotly.py/blob/main/CHANGELOG.md"], "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["hasattr"], "release_date": "2019-07"}
{"python_version": "3.9", "library": "plotly", "version": "4.0.0", "problem": "Define a function named custom_api_usage that, using Chart Studio cloud service, retrieves and returns the identifier of the module responsible for API functionalities by accessing its name attribute.\n", "starting_code": "import plotly\ndef custom_api_usage() -> str:\n    import ", "example_id": "273", "test": "import warnings\n\nwith warnings.catch_warnings(record=True) as w:\n    warnings.simplefilter(\"always\")\n    module_name = custom_api_usage()\n    for warn in w:\n        assert not issubclass(warn.category, DeprecationWarning), \"Deprecated API used!\"\n\nexpect = \"chart_studio.api\"\nassert module_name == expect", "solution": "chart_studio.api\n    return chart_studio.api.__name__", "type_of_change": "other library", "name_of_class_or_func": "plotly.api", "additional_dependencies": "chart-studio==1.0.0", "docs": ["https://plotly.com/python/getting-started-with-chart-studio/", "https://github.com/plotly/plotly.py/blob/main/CHANGELOG.md"], "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": [], "release_date": "2019-07"}
{"python_version": "3.9", "library": "plotly", "version": "3.0.0", "problem": "Define a function named custom_scatter that accepts a color value as an argument and uses Plotlys graph objects to create a figure containing a scatter plot with a single point at coordinates (0, 0). The marker for this point should use the provided color. Finally, the function should return the created figure.", "starting_code": "import plotly.graph_objs as go\n\ndef custom_scatter(custom_color: str) -> go.Figure:\n    return ", "example_id": "274", "test": "color = 'rgb(255,45,15)'\nimport warnings\n\nwith warnings.catch_warnings(record=True) as w:\n    warnings.simplefilter(\"always\")\n    fig = custom_scatter(color)\n    for warn in w:\n        assert not issubclass(warn.category, DeprecationWarning), \"Deprecated API used!\"\n\nscatter_trace = fig.data[0]\nmarker_color = scatter_trace.marker.color\nexpect = color\nassert marker_color == expect", "solution": "go.Figure(data=[go.Scatter(x=[0],y=[0],marker=go.scatter.Marker(color=custom_color)) ])", "type_of_change": "argument change", "name_of_class_or_func": "plotly.graph_objs.Scatter()", "additional_dependencies": "", "docs": ["https://plotly.com/python/line-and-scatter/", "https://github.com/plotly/plotly.py/blob/main/CHANGELOG.md"], "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["plotly.graph_objs.Scatter", "plotly.graph_objs.scatter.Marker", "plotly.graph_objs.Figure"], "release_date": "2018-07"}
{"python_version": "3.7", "library": "librosa", "version": "0.6.0", "extra_dependencies": "numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2", "problem": "Complete the function to compute the dynamic time warp between arrays X and Y. ", "starting_code": "import numpy as np\nimport librosa\nfrom scipy.spatial.distance import cdist\n\ndef compute_dtw(X: np.ndarray, Y: np.ndarray) -> np.ndarray:\n    \n", "example_id": "275", "test": "\nX = np.array([[1, 3, 3, 8, 1]])\nY = np.array([[2, 0, 0, 8, 7, 2]])\n\ngt_D = np.array([[1., 2., 3., 10., 16., 17.],\n [2., 4., 5., 8., 12., 13.],\n [3., 5., 7., 10., 12., 13.],\n [9., 11., 13., 7., 8., 14.],\n [10, 10., 11., 14., 13., 9.]])\nassert np.array_equal(gt_D, compute_dtw(X, Y))", "solution": "\n\n    dist_matrix = cdist(X.T, Y.T, metric='euclidean')\n    return librosa.dtw(C=dist_matrix, metric='invalid')[0]", "type_of_change": "name change", "name_of_class_or_func": "librosa.dtw", "additional_dependencies": "numpy==1.16.0 scipy==1.1.0", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["librosa.dtw", "scipy.spatial.distance.cdist"], "release_date": "2018-02", "docs": ["https://librosa.org/doc/main/generated/librosa.sequence.dtw.html", "https://librosa.org/doc/main/changelog.html"]}
{"python_version": "3.7", "library": "librosa", "version": "0.7.0", "extra_dependencies": "numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2", "problem": "Complete the function to compute the dynamic time warp between arrays X and Y. ", "starting_code": "import numpy as np\nimport librosa\nfrom scipy.spatial.distance import cdist\n\ndef compute_dtw(X: np.ndarray, Y: np.ndarray) -> np.ndarray:\n    \n", "example_id": "276", "test": "\nX = np.array([[1, 3, 3, 8, 1]])\nY = np.array([[2, 0, 0, 8, 7, 2]])\n\ngt_D = np.array([[1., 2., 3., 10., 16., 17.],\n [2., 4., 5., 8., 12., 13.],\n [3., 5., 7., 10., 12., 13.],\n [9., 11., 13., 7., 8., 14.],\n [10, 10., 11., 14., 13., 9.]])\nassert np.array_equal(gt_D, compute_dtw(X, Y))", "solution": "\n\n    dist_matrix = cdist(X.T, Y.T, metric='euclidean')\n    return librosa.sequence.dtw(C=dist_matrix, metric='invalid')[0]", "type_of_change": "name change", "name_of_class_or_func": "librosa.sequence.dtw", "additional_dependencies": "numpy==1.16.0 scipy==1.1.0", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["librosa.sequence.dtw", "scipy.spatial.distance.cdist"], "release_date": "2019-07", "docs": ["https://librosa.org/doc/main/generated/librosa.sequence.dtw.html", "https://librosa.org/doc/main/changelog.html"]}
{"python_version": "3.7", "library": "librosa", "version": "0.6.0", "extra_dependencies": "numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2", "problem": "Complete the function to compute the root mean square value for each frame. ", "starting_code": "import librosa\nimport numpy as np\n\ndef compute_rms(y: np.ndarray) -> np.float32:\n    ", "example_id": "277", "test": "\nduration = 2.0 \nfrequency = 440 \nsr = 22050 \n\nt = np.linspace(0, duration, int(sr * duration), endpoint=False)\n\ny = 0.5 * np.sin(2 * np.pi * frequency * t)\nexpect = np.array([\n    [0.35406065, 0.35311503, 0.35384659, 0.35345521, 0.35343952, 0.35385957,\n     0.35310695, 0.3540624,  0.35306794, 0.35393072, 0.35334823, 0.35355084,\n     0.3537631,  0.35317223, 0.35403977, 0.35304415, 0.35399675, 0.35325105,\n     0.35366224, 0.35365654, 0.35325578, 0.35399382, 0.35304479, 0.35404153,\n     0.35316836, 0.3537684,  0.35354501, 0.35335359, 0.35392676, 0.35306983,\n     0.35406292, 0.35310412, 0.35386422, 0.35343384, 0.35346095, 0.35384179,\n     0.35311807, 0.35405989, 0.35306143, 0.35394511, 0.35332838, 0.3535727,\n     0.35374297, 0.35318719, 0.35403259, 0.35304234, 0.35400722, 0.35323367,\n     0.3536835,  0.35363504, 0.35327386, 0.35398233, 0.35304777, 0.35404758,\n     0.35315429, 0.35378803, 0.35352315, 0.35337391, 0.3539115,  0.35307747,\n     0.35406426, 0.35309403, 0.35388128, 0.35341269, 0.35348254, 0.35382349,\n     0.35312999, 0.35405645, 0.35305581, 0.35395879, 0.35330893, 0.35359453,\n     0.35372251, 0.35320281, 0.35402454, 0.35304147, 0.35401687, 0.35321688,\n     0.35370452, 0.35361339, 0.35329244, 0.35397005, 0.35305168, 0.35405273,\n     0.35314094, 0.35398808, 0.35359185]\n])\n\nassertion_value = np.allclose(expect, compute_rms(y))\nassert assertion_value", "solution": "\n\n    return librosa.feature.rmse(y=y)", "type_of_change": "name change", "name_of_class_or_func": "librosa.feature.rmse", "additional_dependencies": "numpy==1.16.0 scipy==1.1.0", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["librosa.feature.rmse"], "release_date": "2018-02", "docs": ["https://librosa.org/doc/main/generated/librosa.feature.rms.html", "https://librosa.org/doc/main/changelog.html"]}
{"python_version": "3.7", "library": "librosa", "version": "0.7.0", "extra_dependencies": "numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2", "problem": "Complete the function to compute the root mean square value for each frame. ", "starting_code": "import librosa\nimport numpy as np\n\ndef compute_rms(y: np.ndarray) -> np.float32:\n    ", "example_id": "278", "test": "\nduration = 2.0 \nfrequency = 440 \nsr = 22050 \n\nt = np.linspace(0, duration, int(sr * duration), endpoint=False)\n\ny = 0.5 * np.sin(2 * np.pi * frequency * t)\nexpect = np.array([\n    [0.35406065, 0.35311503, 0.35384659, 0.35345521, 0.35343952, 0.35385957,\n     0.35310695, 0.3540624,  0.35306794, 0.35393072, 0.35334823, 0.35355084,\n     0.3537631,  0.35317223, 0.35403977, 0.35304415, 0.35399675, 0.35325105,\n     0.35366224, 0.35365654, 0.35325578, 0.35399382, 0.35304479, 0.35404153,\n     0.35316836, 0.3537684,  0.35354501, 0.35335359, 0.35392676, 0.35306983,\n     0.35406292, 0.35310412, 0.35386422, 0.35343384, 0.35346095, 0.35384179,\n     0.35311807, 0.35405989, 0.35306143, 0.35394511, 0.35332838, 0.3535727,\n     0.35374297, 0.35318719, 0.35403259, 0.35304234, 0.35400722, 0.35323367,\n     0.3536835,  0.35363504, 0.35327386, 0.35398233, 0.35304777, 0.35404758,\n     0.35315429, 0.35378803, 0.35352315, 0.35337391, 0.3539115,  0.35307747,\n     0.35406426, 0.35309403, 0.35388128, 0.35341269, 0.35348254, 0.35382349,\n     0.35312999, 0.35405645, 0.35305581, 0.35395879, 0.35330893, 0.35359453,\n     0.35372251, 0.35320281, 0.35402454, 0.35304147, 0.35401687, 0.35321688,\n     0.35370452, 0.35361339, 0.35329244, 0.35397005, 0.35305168, 0.35405273,\n     0.35314094, 0.35398808, 0.35359185]\n])\n\nassertion_value = np.allclose(expect, compute_rms(y))\nassert assertion_value", "solution": "\n\n    return librosa.feature.rms(y=y)", "type_of_change": "name change", "name_of_class_or_func": "librosa.feature.rms", "additional_dependencies": "numpy==1.16.0 scipy==1.1.0", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["librosa.feature.rms"], "release_date": "2019-07", "docs": ["https://librosa.org/doc/main/generated/librosa.feature.rms.html", "https://librosa.org/doc/main/changelog.html"]}
{"python_version": "3.7", "library": "librosa", "version": "0.6.0", "extra_dependencies": "numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2", "problem": "Complete the function to fill the off diagonal with a value of 0 with the constraint region being a Sakoe-Chiba band of radius 0.25. ", "starting_code": "import librosa\nimport numpy as np\n\ndef compute_fill_diagonal(mut_x: np.ndarray, radius: float) -> np.ndarray:\n    ", "example_id": "279", "test": "\nmut_x = np.ones((8, 12))\nradius = 0.25\n\nassertion_value = np.array_equal(librosa.fill_off_diagonal(mut_x,  radius), compute_fill_diagonal(mut_x, radius))\nassert assertion_value", "solution": "\n\n    return librosa.fill_off_diagonal(mut_x,  radius)", "type_of_change": "name change", "name_of_class_or_func": "librosa.fill_off_diagonal", "additional_dependencies": "numpy==1.16.0 scipy==1.1.0", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["librosa.fill_off_diagonal"], "release_date": "2018-02", "docs": ["https://librosa.org/doc/main/changelog.html"]}
{"python_version": "3.7", "library": "librosa", "version": "0.7.0", "extra_dependencies": "numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2", "problem": "Complete the function to fill the off diagonal with a value of 0 with the constraint region being a Sakoe-Chiba band of radius 0.25. ", "starting_code": "import librosa\nimport numpy as np\n\ndef compute_fill_diagonal(mut_x: np.ndarray, radius: float) -> np.ndarray:\n    ", "example_id": "280", "test": "\nmut_x = np.ones((8, 12))\nradius = 0.25\nassertion_value = np.array_equal(librosa.util.fill_off_diagonal(mut_x,  radius), compute_fill_diagonal(mut_x, radius))\nassert assertion_value", "solution": "\n\n    return librosa.util.fill_off_diagonal(mut_x,  radius)", "type_of_change": "name change", "name_of_class_or_func": "librosa.util.fill_off_diagonal", "additional_dependencies": "numpy==1.16.0 scipy==1.1.0", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["librosa.util.fill_off_diagonal"], "release_date": "2019-07", "docs": ["https://librosa.org/doc/main/changelog.html"]}
{"python_version": "3.7", "library": "librosa", "version": "0.6.0", "extra_dependencies": "numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2", "problem": "Complete the function to extract melspectrogram from waveform y. After it is computed, determine if it is of type float64. Return both values as a tuple.", "starting_code": "import librosa\nimport numpy as np\nfrom typing import Tuple\n\ndef compute_extraction(y: np.ndarray, sr: int) -> Tuple[np.ndarray, bool]:\n    ", "example_id": "281", "test": "\nduration = 2.0 \nfrequency = 440\nsr = 22050 \n\nt = np.linspace(0, duration, int(sr * duration), endpoint=False)\n\ny = 0.5 * np.sin(2 * np.pi * frequency * t)\ny = y.astype(np.float32)\n\nsol=librosa.feature.melspectrogram(y=y, sr=sr) \nM_from_y, float64_bool = compute_extraction(y, sr)\nassert np.array_equal(sol, M_from_y)\nassert float64_bool \n\n", "solution": "\n\n    M_from_y = librosa.feature.melspectrogram(y=y, sr=sr) \n    return M_from_y, M_from_y.dtype == np.float64", "type_of_change": "behaviour", "name_of_class_or_func": "librosa.feature.melspectrogram", "additional_dependencies": "numpy==1.16.0 scipy==1.1.0", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["librosa.feature.melspectrogram"], "release_date": "2018-02", "docs": ["https://librosa.org/doc/main/generated/librosa.feature.melspectrogram.html", "https://librosa.org/doc/main/changelog.html"]}
{"python_version": "3.7", "library": "librosa", "version": "0.7.0", "extra_dependencies": "numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2", "problem": "Complete the function to extract melspectrogram from waveform y. After it is computed, determine if it is of type float32.", "starting_code": "import librosa\nimport numpy as np\nfrom typing import Tuple\n\ndef compute_extraction(y: np.ndarray, sr: int) -> Tuple[np.ndarray, bool]:\n    ", "example_id": "282", "test": "\nduration = 2.0 \nfrequency = 440 \nsr = 22050 \nt = np.linspace(0, duration, int(sr * duration), endpoint=False)\n\ny = 0.5 * np.sin(2 * np.pi * frequency * t)\ny = y.astype(np.float32)\n\nsol=librosa.feature.melspectrogram(y=y, sr=sr) \nM_from_y, float32_bool = compute_extraction(y, sr)\nassert np.array_equal(sol, M_from_y)\nassert float32_bool\n", "solution": "\n\n    M_from_y = librosa.feature.melspectrogram(y=y, sr=sr) \n    return M_from_y, M_from_y.dtype == np.float32\n", "type_of_change": "behaviour", "name_of_class_or_func": "librosa.feature.melspectrogram", "additional_dependencies": "numpy==1.16.0 scipy==1.1.0", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["librosa.feature.melspectrogram"], "release_date": "2019-07", "docs": ["https://librosa.org/doc/main/generated/librosa.feature.melspectrogram.html", "https://librosa.org/doc/main/changelog.html"]}
{"python_version": "3.7", "library": "librosa", "version": "0.6.0", "extra_dependencies": "numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2", "problem": "Complete the function to iterate over an audio file using a stream and calculate the STFT on each mono channel.", "starting_code": "import librosa\nimport numpy as np\nimport soundfile as sf \n\n\n# Save the stream in variable stream. Save each stream block with the array stream_blocks\ndef compute_stream(filename, y, sr, n_fft, hop_length):\n    stream_blocks = []", "example_id": "283", "test": "\n\nfilename = librosa.util.example_audio_file()\ny, sr = librosa.load(filename)\n\nn_fft = 4096\nhop_length = n_fft // 2\n\nstream, stream_blocks = compute_stream(filename, y, sr, n_fft, hop_length)\nsol_stream = sf.blocks(filename, blocksize=n_fft + 15 * hop_length, overlap=n_fft - hop_length, fill_value=0)\nsol_blocks = []\nfor c, block in enumerate(sol_stream):\n    y = librosa.to_mono(block.T)\n    D = librosa.stft(y, n_fft=n_fft, hop_length=hop_length, center=False)\n    sol_blocks.append(D)\nfor i in range(0, len(stream_blocks)):\n    assert np.array_equal(sol_blocks[i], stream_blocks[i])", "solution": "\n\n    stream = sf.blocks(filename, blocksize=n_fft + 15 * hop_length, overlap=n_fft - hop_length,  fill_value=0)\n\n    for c, block in enumerate(stream):\n        y = librosa.to_mono(block.T)\n        D = librosa.stft(y, n_fft=n_fft, hop_length=hop_length, center=False)\n        stream_blocks.append(D)\n\n    return stream, stream_blocks", "type_of_change": "new feature", "name_of_class_or_func": "soundfile.blocks", "additional_dependencies": "scikit-learn==0.21.0 numpy==1.16.0 scipy==1.1.0 soundfile==0.10.2", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["stream_blocks.append", "soundfile.blocks", "enumerate", "librosa.to_mono", "librosa.stft"], "release_date": "2018-02", "docs": ["https://python-soundfile.readthedocs.io/en/0.11.0/", "https://librosa.org/doc/main/changelog.html"]}
{"python_version": "3.7", "library": "librosa", "version": "0.7.0", "extra_dependencies": "numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2", "problem": "Complete the function to complete the function to iterate over an audio file using a stream and calculate the STFT on each mono channel. Frame_length is given by n_fft. Save the stream and each stream block.", "starting_code": "import librosa\nimport numpy as np\n\n# Save the stream in variable stream. Save each stream block with the array stream_blocks\ndef compute_stream(y, sr, n_fft, hop_length):\n    stream_blocks = []", "example_id": "284", "test": "\nfilename = librosa.util.example_audio_file()\ny, sr = librosa.load(filename)\n\nn_fft = 4096\nhop_length = n_fft // 2\nstream, stream_blocks = compute_stream(y, sr, n_fft, hop_length)\nsol_stream =  librosa.stream(filename, block_length=16,\n                        frame_length=n_fft,\n                        hop_length=hop_length,\n                        mono=True,\n                        fill_value=0)\nfor c, y_block in enumerate(sol_stream):\n    assertion_value = np.array_equal(librosa.stft(y_block, n_fft=n_fft, hop_length=hop_length, center=False), stream_blocks[c])\n    assert  assertion_value", "solution": "\n\n\n    stream =  librosa.stream(filename, block_length=16,\n                        frame_length=n_fft,\n                        hop_length=hop_length,\n                        mono=True,\n                        fill_value=0)\n\n    for c, y_block in enumerate(stream):\n        stream_blocks.append(librosa.stft(y_block, n_fft=n_fft, hop_length=hop_length, center=False))\n    return stream, stream_blocks", "type_of_change": "new feature", "name_of_class_or_func": "librosa.stream", "additional_dependencies": "scikit-learn==0.21.0 numpy==1.16.0 scipy==1.1.0", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["librosa.stream", "librosa.stft", "stream_blocks.append", "enumerate"], "release_date": "2019-07", "docs": ["https://librosa.org/doc/main/generated/librosa.stream.html", "https://librosa.org/doc/main/changelog.html"]}
{"python_version": "3.7", "library": "librosa", "version": "0.6.0", "extra_dependencies": "numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2", "problem": "Compute an approximate magnitude spectrogram inversion using the Griffin-Lim algorithm.", "starting_code": "import librosa\nimport numpy as np\nfrom librosa import istft, stft\nfrom typing import Union, Optional\n\nDTypeLike = Union[np.dtype, type]\n\ndef compute_griffinlim(y: np.ndarray, sr: int, S: np.ndarray, random_state: int, n_iter: int, hop_length: Optional[int], win_length: Optional[int], window: str, center: bool, dtype: DTypeLike, length: Optional[int], pad_mode: str, n_fft: int) -> np.ndarray:\n    \"\"\"\n    Compute waveform from a linear scale magnitude spectrogram using the Griffin-Lim transformation.\n\n    Parameters:\n    y: Audio timeseries.\n    sr: Sampling rate.\n    S: short-time Fourier transform magnitude matrix.\n    random_state: Random state for the random number generator.\n    n_iter: Number of iterations.\n    hop_length: Hop length.\n    win_length: Window length.\n    window: Window function.\n    center: If True, the signal y is padded so that frame t is centered at y[t * hop_length]. If False, then frame t begins at y[t * hop_length].\n    dtype: Data type of the output.\n    length: Length of the output signal.\n    pad_mode: Padding mode.\n    n_fft: FFT size.\n\n    Returns:\n        The Griffin-Lim waveform.        \n    \"\"\"\n    rng = np.random.RandomState(seed=random_state)\n", "example_id": "285", "test": "\nfilename = librosa.util.example_audio_file()\ny, sr = librosa.load(filename)\nmomentum = 0.99\nS = np.abs(librosa.stft(y))\nrandom_state = 0\nrng = np.random.RandomState(seed=random_state)\nn_iter=32\nhop_length=None\nwin_length=None\nwindow='hann'\ncenter=True\ndtype=np.float32\nlength=None\npad_mode='reflect'\nn_fft = 2 * (S.shape[0] - 1)\n\nrng = np.random.RandomState(seed=random_state)\nsol = compute_griffinlim(y, sr, S, random_state, n_iter, hop_length, win_length, window, center, dtype, length, pad_mode, n_fft)\n\nrng = np.random.RandomState(seed=random_state)\nangles = np.exp(2j * np.pi * rng.rand(*S.shape))\n\nrebuilt = 0.\n\nfor _ in range(n_iter):\n    tprev = rebuilt\n\n    inverse = istft(S * angles, hop_length=hop_length, win_length=win_length,\n    window=window, center=center, dtype=dtype, length=length)\n\n    rebuilt = stft(inverse, n_fft=n_fft, hop_length=hop_length,\n    win_length=win_length, window=window, center=center,\n    pad_mode=pad_mode)\n\n    angles[:] = rebuilt - (momentum / (1 + momentum)) * tprev\n    angles[:] /= np.abs(angles) + 1e-16\n\ntest_sol = istft(S * angles, hop_length=hop_length, win_length=win_length,window=window, center=center, dtype=dtype, length=length)\nassert np.allclose(test_sol, sol)", "solution": "\n\n    angles = np.exp(2j * np.pi * rng.rand(*S.shape))\n    \n    rebuilt = 0.\n    \n    for _ in range(n_iter):\n        tprev = rebuilt\n    \n        inverse = istft(S * angles, hop_length=hop_length, win_length=win_length,\n        window=window, center=center, dtype=dtype, length=length)\n    \n        rebuilt = stft(inverse, n_fft=n_fft, hop_length=hop_length,\n        win_length=win_length, window=window, center=center,\n        pad_mode=pad_mode)\n    \n        angles[:] = rebuilt - (momentum / (1 + momentum)) * tprev\n        angles[:] /= np.abs(angles) + 1e-16\n    return istft(S * angles, hop_length=hop_length, win_length=win_length,window=window, center=center, dtype=dtype, length=length)", "type_of_change": "new feature", "name_of_class_or_func": "librosa.griffinlim", "additional_dependencies": "scikit-learn==0.21.0 numpy==1.16.0 scipy==1.1.0 soundfile==0.10.2", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["istft", "rng.rand", "range", "stft", "numpy.exp", "numpy.abs"], "release_date": "2018-02", "docs": ["https://librosa.org/doc/main/generated/librosa.griffinlim.html", "https://librosa.org/doc/main/changelog.html"]}
{"python_version": "3.7", "library": "librosa", "version": "0.7.0", "extra_dependencies": "numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2", "problem": "Complete the function to compute an approximate magnitude spectrogram inversion using the Griffin-Lim algorithm.", "starting_code": "import librosa\nimport numpy as np\nfrom librosa import istft, stft\nfrom typing import Union, Optional\n\nDTypeLike = Union[np.dtype, type]\n\ndef compute_griffinlim(y: np.ndarray, sr: int, S: np.ndarray, random_state: int, n_iter: int, hop_length: Optional[int], win_length: Optional[int], window: str, center: bool, dtype: DTypeLike, length: Optional[int], pad_mode: str, n_fft: int) -> np.ndarray:\n    \"\"\"\n    Compute waveform from a linear scale magnitude spectrogram using the Griffin-Lim transformation.\n\n    Parameters:\n        y: Audio timeseries.\n        sr: Sampling rate.\n        S: short-time Fourier transform magnitude matrix.\n        random_state: Random state for the random number generator.\n        n_iter: Number of iterations.\n        hop_length: Hop length.\n        win_length: Window length.\n        window: Window function.\n        center: If True, the signal y is padded so that frame t is centered at y[t * hop_length]. If False, then frame t begins at y[t * hop_length].\n        dtype: Data type of the output.\n        length: Length of the output signal.\n        pad_mode: Padding mode.\n        n_fft: FFT size.\n\n    Returns:\n        The Griffin-Lim waveform.        \n    \"\"\"    \n    rng = np.random.RandomState(seed=random_state)\n", "example_id": "286", "test": "\nfilename = librosa.util.example_audio_file()\ny, sr = librosa.load(filename)\nmomentum = 0.99\nS = np.abs(librosa.stft(y))\nrandom_state = 0\nrng = np.random.RandomState(seed=random_state)\nn_iter=32\nhop_length=None\nwin_length=None\nwindow='hann'\ncenter=True\ndtype=np.float32\nlength=None\npad_mode='reflect'\nn_fft = 2 * (S.shape[0] - 1)\n\nsol = compute_griffinlim(y, sr, S, random_state, n_iter, hop_length, win_length, window, center, dtype, length, pad_mode, n_fft)\n\nrng = np.random.RandomState(seed=random_state)\ntest_sol = librosa.griffinlim(S, n_iter, hop_length, win_length, window, center, dtype, length, pad_mode, momentum, random_state)\nassert np.allclose(test_sol, sol)", "solution": "\n\n    return librosa.griffinlim(S, n_iter, hop_length, win_length, window, center, dtype, length, pad_mode, momentum, random_state)", "type_of_change": "new feature", "name_of_class_or_func": "librosa.griffinlim", "additional_dependencies": "numpy==1.16.0 scipy==1.1.0 soundfile==0.10.2", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["librosa.griffinlim"], "release_date": "2019-07", "docs": ["https://librosa.org/doc/main/generated/librosa.griffinlim.html", "https://librosa.org/doc/main/changelog.html"]}
{"python_version": "3.7", "library": "librosa", "version": "0.6.0", "extra_dependencies": "numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2", "problem": "Complete the function to compute Linear Prediction coefficents of input array y.", "starting_code": "import librosa\nimport numpy as np\n\ndef compute_lpc_coef(y: np.ndarray, sr: int, order: int) -> np.ndarray:\n    \"\"\"\n    Compute the Linear Prediction Coefficients of an audio signal.\n\n    Parameters:\n        y: The audio signal.\n        sr: The sampling rate of the audio signal in Hertz.\n        order: Order of the linear filter.\n\n    Returns:\n        LP prediction error coefficients, i.e. filter denominator polynomial.\n    \"\"\"\n    ", "example_id": "287", "test": "\nfilename = librosa.util.example_audio_file()\ny, sr = librosa.load(filename)\norder=2\n\nsol = compute_lpc_coef(y, sr, order)\ndtype = y.dtype.type\nar_coeffs = np.zeros(order+1, dtype=dtype)\nar_coeffs[0] = dtype(1)\nar_coeffs_prev = np.zeros(order+1, dtype=dtype)\nar_coeffs_prev[0] = dtype(1)\nfwd_pred_error = y[1:]\nbwd_pred_error = y[:-1]\nden = np.dot(fwd_pred_error, fwd_pred_error) \\\n      + np.dot(bwd_pred_error, bwd_pred_error)\nfor i in range(order):\n    if den <= 0:\n        raise FloatingPointError('numerical error, input ill-conditioned?')\n    reflect_coeff = dtype(-2) * np.dot(bwd_pred_error, fwd_pred_error) / dtype(den)\n    ar_coeffs_prev, ar_coeffs = ar_coeffs, ar_coeffs_prev\n    for j in range(1, i + 2):\n        ar_coeffs[j] = ar_coeffs_prev[j] + reflect_coeff * ar_coeffs_prev[i - j + 1]\n    fwd_pred_error_tmp = fwd_pred_error\n    fwd_pred_error = fwd_pred_error + reflect_coeff * bwd_pred_error\n    bwd_pred_error = bwd_pred_error + reflect_coeff * fwd_pred_error_tmp\n    q = dtype(1) - reflect_coeff**2\n    den = q*den - bwd_pred_error[-1]**2 - fwd_pred_error[0]**2\n    fwd_pred_error = fwd_pred_error[1:]\n    bwd_pred_error = bwd_pred_error[:-1]\n\ntest_sol = ar_coeffs\nassert np.array_equal(test_sol, sol)", "solution": "\n    \n    dtype = y.dtype.type\n    ar_coeffs = np.zeros(order+1, dtype=dtype)\n    ar_coeffs[0] = dtype(1)\n    ar_coeffs_prev = np.zeros(order+1, dtype=dtype)\n    ar_coeffs_prev[0] = dtype(1)\n    fwd_pred_error = y[1:]\n    bwd_pred_error = y[:-1]\n    den = np.dot(fwd_pred_error, fwd_pred_error) \\\n          + np.dot(bwd_pred_error, bwd_pred_error)\n    for i in range(order):\n        if den <= 0:\n            raise FloatingPointError('numerical error, input ill-conditioned?')\n        reflect_coeff = dtype(-2) * np.dot(bwd_pred_error, fwd_pred_error) / dtype(den)\n        ar_coeffs_prev, ar_coeffs = ar_coeffs, ar_coeffs_prev\n        for j in range(1, i + 2):\n            ar_coeffs[j] = ar_coeffs_prev[j] + reflect_coeff * ar_coeffs_prev[i - j + 1]\n        fwd_pred_error_tmp = fwd_pred_error\n        fwd_pred_error = fwd_pred_error + reflect_coeff * bwd_pred_error\n        bwd_pred_error = bwd_pred_error + reflect_coeff * fwd_pred_error_tmp\n        q = dtype(1) - reflect_coeff**2\n        den = q*den - bwd_pred_error[-1]**2 - fwd_pred_error[0]**2\n        fwd_pred_error = fwd_pred_error[1:]\n        bwd_pred_error = bwd_pred_error[:-1]\n    return ar_coeffs", "type_of_change": "new feature", "name_of_class_or_func": "librosa.lpc", "additional_dependencies": "numpy==1.16.0 scipy==1.1.0 soundfile==0.10.2", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["numpy.zeros", "dtype", "range", "numpy.dot", "FloatingPointError"], "release_date": "2018-02", "docs": ["https://librosa.org/doc/main/generated/librosa.lpc.html", "https://librosa.org/doc/main/changelog.html"]}
{"python_version": "3.7", "library": "librosa", "version": "0.7.0", "extra_dependencies": "numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2", "problem": "Complete the function to compute Linear Prediction coefficents of input array y.", "starting_code": "import librosa\nimport numpy as np\n\ndef compute_lpc_coef(y: np.ndarray, sr: int, order: int) -> np.ndarray:\n    \"\"\"\n    Compute the Linear Prediction Coefficients of an audio signal.\n\n    Parameters:\n        y: The audio signal.\n        sr: The sampling rate of the audio signal in Hertz.\n        order: Order of the linear filter.\n\n    Returns:\n        LP prediction error coefficients, i.e. filter denominator polynomial.\n    \"\"\"\n", "example_id": "288", "test": "\nfilename = librosa.util.example_audio_file()\ny, sr = librosa.load(filename)\norder=2\n\nsol = compute_lpc_coef(y, sr, order)\ntest_sol = librosa.lpc(y, order)\nassert np.array_equal(test_sol, sol)", "solution": "\n\n    return librosa.lpc(y, order)", "type_of_change": "new feature", "name_of_class_or_func": "librosa.lpc", "additional_dependencies": "numpy==1.16.0 scipy==1.1.0 soundfile==0.10.2", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["librosa.lpc"], "release_date": "2019-07", "docs": ["https://librosa.org/doc/main/generated/librosa.lpc.html", "https://librosa.org/doc/main/changelog.html"]}
{"python_version": "3.7", "library": "librosa", "version": "0.6.0", "extra_dependencies": "numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2", "problem": "Complete the function to compute local onset autocorrelation in order to create a fourier tempogram.", "starting_code": "import librosa\nimport numpy as np\nfrom librosa.core.spectrum import stft\n\ndef compute_fourier_tempogram(oenv: np.ndarray, sr: int, hop_length: int) -> np.ndarray:\n    \"\"\"\n    Compute the Fourier tempogram: the short-time Fourier transform of the onset strength envelope.\n\n    Parameters:\n       oenv: The onset strength envelope.\n       sr: The sampling rate of the audio signal in Hertz.\n       hop_length: The number of samples between successive frames.\n\n    Returns:\n       The computed Fourier tempogram.\n    \"\"\"", "example_id": "289", "test": "\nfilename = librosa.util.example_audio_file()\ny, sr = librosa.load(filename)\nhop_length = 512\noenv = librosa.onset.onset_strength(y=y, sr=sr, hop_length=hop_length)\n\nsol = compute_fourier_tempogram(oenv, sr, hop_length)\ntest_sol = stft(oenv, n_fft=384, hop_length=1, center=True, window=\"hann\")\nassert np.array_equal(test_sol, sol)", "solution": "\n\n    return stft(oenv, n_fft=384, hop_length=1, center=True, window=\"hann\")", "type_of_change": "new feature", "name_of_class_or_func": "librosa.feature.fourier_tempogram", "additional_dependencies": "scikit-learn==0.21.0 numpy==1.16.0 scipy==1.1.0 soundfile==0.10.2", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["librosa.core.spectrum.stft"], "release_date": "2018-02", "docs": ["https://librosa.org/doc/main/generated/librosa.feature.fourier_tempogram.html", "https://librosa.org/doc/main/changelog.html"]}
{"python_version": "3.7", "library": "librosa", "version": "0.7.0", "extra_dependencies": "numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2", "problem": "Complete the function to compute local onset autocorrelation using fourier_tempogram.", "starting_code": "import librosa\nimport numpy as np\n\ndef compute_fourier_tempogram(oenv: np.ndarray, sr: int, hop_length: int) -> np.ndarray:\n    \"\"\"\n    Compute the Fourier tempogram: the short-time Fourier transform of the onset strength envelope.\n\n    Parameters:\n       oenv: The onset strength envelope.\n       sr: The sampling rate of the audio signal in Hertz.\n       hop_length: The number of samples between successive frames.\n\n    Returns:\n       The computed Fourier tempogram.\n    \"\"\"\n    ", "example_id": "290", "test": "\nfilename = librosa.util.example_audio_file()\ny, sr = librosa.load(filename)\nhop_length = 512\noenv = librosa.onset.onset_strength(y=y, sr=sr, hop_length=hop_length)\n\nsol = compute_fourier_tempogram(oenv, sr, hop_length)\ntest_sol = librosa.feature.fourier_tempogram(onset_envelope=oenv, sr=sr, hop_length=hop_length)\nassert np.array_equal(test_sol, sol)", "solution": "\n\n    return librosa.feature.fourier_tempogram(onset_envelope=oenv, sr=sr, hop_length=hop_length)", "type_of_change": "new feature", "name_of_class_or_func": "librosa.feature.fourier_tempogram", "additional_dependencies": "numpy==1.16.0 scipy==1.1.0 soundfile==0.10.2", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["librosa.feature.fourier_tempogram"], "release_date": "2019-07", "docs": ["https://librosa.org/doc/main/generated/librosa.feature.fourier_tempogram.html", "https://librosa.org/doc/main/changelog.html"]}
{"python_version": "3.7", "library": "librosa", "version": "0.6.0", "extra_dependencies": "numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2", "problem": "Complete the function to compute the predominant local pulse (PLP) estimation of y.", "starting_code": "import librosa\nimport numpy as np\nfrom librosa.core.spectrum import stft, istft\nfrom typing import Optional\n\n\ndef compute_plp(\n    y: np.ndarray,\n    sr: int,\n    hop_length: int,\n    win_length: int,\n    tempo_min: Optional[float],\n    tempo_max: Optional[float],\n    onset_env: np.ndarray\n) -> np.ndarray:\n    \"\"\"\n    Compute the Predominant Local Pulse (PLP) of an audio signal.\n    \n    Parameters:\n        y: The audio signal.\n        sr: The sampling rate of the audio signal in Hertz.\n        hop_length: The number of samples between successive frames.\n        win_length: The length (in samples) of the analysis window.\n        tempo_min: The minimum tempo (in BPM) for consideration.\n        tempo_max: The maximum tempo (in BPM) for consideration.\n        onset_env: The onset envelope of the audio signal.\n        \n    Returns:\n        The computed PLP (Predominant Local Pulse) values.\n    \"\"\"", "example_id": "291", "test": "filename = librosa.util.example_audio_file()\ny, sr = librosa.load(filename)\nhop_length=512\nwin_length=384\ntempo_min = None\ntempo_max = None\nonset_env = librosa.onset.onset_strength(y=y, sr=sr, hop_length=hop_length)\n\n\nsol = compute_plp(y, sr, hop_length, win_length, tempo_min, tempo_max, onset_env)\n\nftgram = stft(onset_env, n_fft=win_length, hop_length=1, center=True, window=\"hann\")\n\ntempo_frequencies = np.fft.rfftfreq(n=win_length, d=(sr * 60 / float(hop_length)))\n\nftmag = np.abs(ftgram)\npeak_values = ftmag.max(axis=0, keepdims=True)\nftgram[ftmag < peak_values] = 0\n\nftgram[:] /= peak_values\n\npulse = istft(ftgram, hop_length=1, length=len(onset_env))\n\nnp.clip(pulse, 0, None, pulse)\ntest_sol = librosa.util.normalize(pulse)\nassert np.array_equal(test_sol, sol)", "solution": "\n\n\n    ftgram = stft(onset_env, n_fft=win_length, hop_length=1, center=True, window=\"hann\")\n    \n    tempo_frequencies = np.fft.rfftfreq(n=win_length, d=(sr * 60 / float(hop_length)))\n\n    ftmag = np.abs(ftgram)\n    peak_values = ftmag.max(axis=0, keepdims=True)\n    ftgram[ftmag < peak_values] = 0\n\n    ftgram[:] /= peak_values\n\n    pulse = istft(ftgram, hop_length=1, length=len(onset_env))\n\n    np.clip(pulse, 0, None, pulse)\n    return librosa.util.normalize(pulse)", "type_of_change": "new feature", "name_of_class_or_func": "librosa.beat.plp", "additional_dependencies": "scikit-learn==0.21.0 numpy==1.16.0 scipy==1.1.0 soundfile==0.10.2", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["istft", "float", "numpy.fft.rfftfreq", "numpy.clip", "stft", "len", "ftmag.max", "numpy.abs", "librosa.util.normalize"], "release_date": "2018-02", "docs": ["https://librosa.org/doc/main/generated/librosa.beat.plp.html", "https://librosa.org/doc/main/changelog.html"]}
{"python_version": "3.7", "library": "librosa", "version": "0.7.0", "extra_dependencies": "numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2", "problem": "Complete the function to compute the predominant local pulse (PLP) estimation of y.", "starting_code": "import librosa\nimport numpy as np\nfrom librosa.core.spectrum import stft, istft\nfrom typing import Optional\n\n\ndef compute_plp(\n    y: np.ndarray,\n    sr: int,\n    hop_length: int,\n    win_length: int,\n    tempo_min: Optional[float],\n    tempo_max: Optional[float],\n    onset_env: np.ndarray\n) -> np.ndarray:\n    \"\"\"\n    Compute the Predominant Local Pulse (PLP) of an audio signal.\n    \n    Parameters:\n        y: The audio signal.\n        sr: The sampling rate of the audio signal in Hertz.\n        hop_length: The number of samples between successive frames.\n        win_length: The length (in samples) of the analysis window.\n        tempo_min: The minimum tempo (in BPM) for consideration.\n        tempo_max: The maximum tempo (in BPM) for consideration.\n        onset_env: The onset envelope of the audio signal.\n        \n    Returns:\n        The computed PLP (Predominant Local Pulse) values.\n    \"\"\"", "example_id": "292", "test": "filename = librosa.util.example_audio_file()\ny, sr = librosa.load(filename)\nhop_length=512\nwin_length=384\ntempo_min = None\ntempo_max = None\nonset_env = librosa.onset.onset_strength(y=y, sr=sr, hop_length=hop_length)\n\n\nsol = compute_plp(y, sr, hop_length, win_length, tempo_min, tempo_max, onset_env)\n\nftgram = stft(onset_env, n_fft=win_length, hop_length=1, center=True, window=\"hann\")\n\ntempo_frequencies = np.fft.rfftfreq(n=win_length, d=(sr * 60 / float(hop_length)))\n\nftmag = np.abs(ftgram)\npeak_values = ftmag.max(axis=0, keepdims=True)\nftgram[ftmag < peak_values] = 0\n\nftgram[:] /= peak_values\n\npulse = istft(ftgram, hop_length=1, length=len(onset_env))\n\nnp.clip(pulse, 0, None, pulse)\ntest_sol = librosa.util.normalize(pulse)\nassert np.array_equal(test_sol, sol)", "solution": "\n\n    return librosa.beat.plp(onset_envelope=onset_env, sr=sr, tempo_min=tempo_min, tempo_max=tempo_max)", "type_of_change": "new feature", "name_of_class_or_func": "librosa.beat.plp", "additional_dependencies": "numpy==1.16.0 scipy==1.1.0 soundfile==0.10.2", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["librosa.beat.plp"], "release_date": "2019-07", "docs": ["https://librosa.org/doc/main/generated/librosa.beat.plp.html", "https://librosa.org/doc/main/changelog.html"]}
{"python_version": "3.7", "library": "librosa", "version": "0.6.0", "extra_dependencies": "numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2", "problem": "Complete the function to return an array of time values to match the time axis from a feature matrix.", "starting_code": "import librosa\nimport numpy as np\n\ndef compute_times_like(y: np.ndarray, sr: int, hop_length: int, D: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute the times vector of a spectrogram.\n\n    Parameters:\n        y: The audio signal.\n        sr: The sampling rate of the audio signal in Hertz.\n        hop_length: The number of samples between successive frames.\n        D: The spectrogram.\n\n    Returns:\n        The computed times vector.\n    \"\"\"\n    ", "example_id": "293", "test": "\nfilename = librosa.util.example_audio_file()\ny, sr = librosa.load(filename)\nD = librosa.stft(y)\nhop_length = 512 \n\nsol = compute_times_like(y, sr, hop_length, D)\n    \nif np.isscalar(D):\n    frames = np.arange(D) # type: ignore\nelse:\n    frames = np.arange(D.shape[-1]) # type: ignore\noffset = 0\nsamples = (np.asanyarray(frames) * hop_length + offset).astype(int)\n\ntest_sol = np.asanyarray(samples) / float(sr)\nassert np.array_equal(test_sol, sol)", "solution": "\n\n    if np.isscalar(D):\n        frames = np.arange(D) # type: ignore\n    else:\n        frames = np.arange(D.shape[-1]) # type: ignore\n    offset = 0\n    samples = (np.asanyarray(frames) * hop_length + offset).astype(int)\n\n    return np.asanyarray(samples) / float(sr)", "type_of_change": "new feature", "name_of_class_or_func": "librosa.times_like", "additional_dependencies": "scikit-learn==0.21.0 numpy==1.16.0 scipy==1.1.0 soundfile==0.10.2", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["numpy.asanyarray", "numpy.arange", "float", "numpy.isscalar", "astype"], "release_date": "2018-02", "docs": ["https://librosa.org/doc/main/generated/librosa.times_like.html", "https://librosa.org/doc/main/changelog.html"]}
{"python_version": "3.7", "library": "librosa", "version": "0.7.0", "extra_dependencies": "numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2", "problem": "Complete the function to return an array of time values to match the time axis from a feature matrix.", "starting_code": "import librosa\nimport numpy as np\n\ndef compute_times_like(y: np.ndarray, sr: int, hop_length: int, D: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute the times vector of a spectrogram.\n\n    Parameters:\n        y: The audio signal.\n        sr: The sampling rate of the audio signal in Hertz.\n        hop_length: The number of samples between successive frames.\n        D: The spectrogram.\n\n    Returns:\n        The computed times vector.\n    \"\"\"\n    ", "example_id": "294", "test": "\nfilename = librosa.util.example_audio_file()\ny, sr = librosa.load(filename)\nD = librosa.stft(y)\nhop_length = 512 \n\nsol = compute_times_like(y, sr, hop_length, D)\n    \nif np.isscalar(D):\n    frames = np.arange(D) # type: ignore\nelse:\n    frames = np.arange(D.shape[-1]) # type: ignore\noffset = 0\nsamples = (np.asanyarray(frames) * hop_length + offset).astype(int)\n\ntest_sol = np.asanyarray(samples) / float(sr)\nassert np.array_equal(test_sol, sol)", "solution": "\n\n    return librosa.times_like(D, sr=sr)", "type_of_change": "new feature", "name_of_class_or_func": "librosa.times_like", "additional_dependencies": "numpy==1.16.0 scipy==1.1.0 soundfile==0.10.2", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["librosa.times_like"], "release_date": "2019-07", "docs": ["https://librosa.org/doc/main/generated/librosa.times_like.html", "https://librosa.org/doc/main/changelog.html"]}
{"python_version": "3.7", "library": "librosa", "version": "0.6.0", "extra_dependencies": "numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2", "problem": "Complete the function to return an array of sample indices to match the time axis from a feature matrix.", "starting_code": "import librosa\nimport numpy as np\n\ndef compute_samples_like(y: np.ndarray, sr: int, D: np.ndarray, hop_length: int) -> np.ndarray:\n    \"\"\"\n    Compute the samples vector of a spectrogram.\n\n    Parameters:\n        y: The audio signal.\n        sr: The sampling rate of the audio signal in Hertz.\n        D: The spectrogram.\n    \n    Returns:\n        The computed samples vector.\n    \"\"\"\n    \n    ", "example_id": "295", "test": "\nfilename = librosa.util.example_audio_file()\ny, sr = librosa.load(filename)\nD = librosa.stft(y)\nhop_length = 512 \n\nsol = compute_samples_like(y, sr, D, hop_length)\n\nif np.isscalar(D):\n    frames = np.arange(D) # type: ignore\nelse:\n    frames = np.arange(D.shape[-1]) # type: ignore\noffset = 0\ntest_sol = (np.asanyarray(frames) * hop_length + offset).astype(int)\n\nassert np.array_equal(test_sol, sol)", "solution": "\n    \n\n    if np.isscalar(D):\n        frames = np.arange(D) # type: ignore\n    else:\n        frames = np.arange(D.shape[-1]) # type: ignore\n    offset = 0\n    return (np.asanyarray(frames) * hop_length + offset).astype(int)\n ", "type_of_change": "new feature", "name_of_class_or_func": "librosa.samples_like", "additional_dependencies": "scikit-learn==0.21.0 numpy==1.16.0 scipy==1.1.0 soundfile==0.10.2", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["numpy.asanyarray", "numpy.arange", "numpy.isscalar", "astype", "return"], "release_date": "2018-02", "docs": ["https://librosa.org/doc/main/generated/librosa.samples_like.html", "https://librosa.org/doc/main/changelog.html"]}
{"python_version": "3.7", "library": "librosa", "version": "0.7.0", "extra_dependencies": "numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2", "problem": "Complete the function to return an array of sample indices to match the time axis from a feature matrix.", "starting_code": "import librosa\nimport numpy as np\n\ndef compute_samples_like(y: np.ndarray, sr: int, D: np.ndarray, hop_length: int) -> np.ndarray:\n    \"\"\"\n    Compute the samples vector of a spectrogram.\n\n    Parameters:\n        y: The audio signal.\n        sr: The sampling rate of the audio signal in Hertz.\n        D: The spectrogram.\n    \n    Returns:\n        The computed samples vector.\n    \"\"\"\n    ", "example_id": "296", "test": "\nfilename = librosa.util.example_audio_file()\ny, sr = librosa.load(filename)\nD = librosa.stft(y)\nhop_length = 512 \n\nsol = compute_samples_like(y, sr, D, hop_length)\n\nif np.isscalar(D):\n    frames = np.arange(D) # type: ignore\nelse:\n    frames = np.arange(D.shape[-1]) # type: ignore\noffset = 0\ntest_sol = (np.asanyarray(frames) * hop_length + offset).astype(int)\n\nassert np.array_equal(test_sol, sol)", "solution": "\n\n    return librosa.samples_like(D)", "type_of_change": "new feature", "name_of_class_or_func": "librosa.samples_like", "additional_dependencies": "numpy==1.16.0 scipy==1.1.0 soundfile==0.10.2", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["librosa.samples_like"], "release_date": "2019-07", "docs": ["https://librosa.org/doc/main/generated/librosa.samples_like.html", "https://librosa.org/doc/main/changelog.html"]}
{"python_version": "3.7", "library": "librosa", "version": "0.6.0", "extra_dependencies": "numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2", "problem": "Complete the function to construct a pure tone (cosine) signal at a given frequency.", "starting_code": "import librosa\nimport numpy as np\n\ndef compute_tone(frequency: int, sr: int, length: int) -> np.ndarray:\n    \"\"\"\n    Constructs a pure tone (cosine) signal at a given frequency.\n\n    Parameters:\n        frequency: The frequency of the tone in Hz.\n        sr: The sampling rate of the signal in Hz.\n        length: The length of the signal in samples.\n\n    Returns:\n        np.ndarray: The pure tone signal.\n    \"\"\"\n    ", "example_id": "297", "test": "\nfrequency = 440\nsr = 22050\nlength = sr\n\nsol = compute_tone(frequency, sr, length)\nphi = -np.pi * 0.5\ntest_sol = np.cos(2 * np.pi * frequency * np.arange(length) / sr + phi)\nassert np.array_equal(test_sol, sol)", "solution": "\n\n    phi = -np.pi * 0.5\n    return np.cos(2 * np.pi * frequency * np.arange(length) / sr + phi)", "type_of_change": "new feature", "name_of_class_or_func": "librosa.tone", "additional_dependencies": "scikit-learn==0.21.0 numpy==1.16.0 scipy==1.1.0 soundfile==0.10.2", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["numpy.arange", "numpy.cos"], "release_date": "2018-02", "docs": ["https://librosa.org/doc/main/generated/librosa.tone.html", "https://librosa.org/doc/main/changelog.html"]}
{"python_version": "3.7", "library": "librosa", "version": "0.7.0", "extra_dependencies": "numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2", "problem": "Complete the function to construct a pure tone (cosine) signal at a given frequency.", "starting_code": "import librosa\nimport numpy as np\n\ndef compute_tone(frequency: int, sr: int, length: int) -> np.ndarray:\n    \"\"\"\n    Constructs a pure tone (cosine) signal at a given frequency.\n\n    Parameters:\n        frequency: The frequency of the tone in Hz.\n        sr: The sampling rate of the signal in Hz.\n        length: The length of the signal in samples.\n\n    Returns:\n        np.ndarray: The pure tone signal.\n    \"\"\"\n    ", "example_id": "298", "test": "\nfrequency = 440\nsr = 22050\nlength = sr\n\nsol = compute_tone(frequency, sr, length)\ntest_sol = librosa.tone(frequency, sr=sr, length=length)\nassert np.array_equal(test_sol, sol)", "solution": "\n\n    return librosa.tone(frequency, sr=sr, length=length)", "type_of_change": "new feature", "name_of_class_or_func": "librosa.tone", "additional_dependencies": "numpy==1.16.0 scipy==1.1.0 soundfile==0.10.2", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["librosa.tone"], "release_date": "2019-07", "docs": ["https://librosa.org/doc/main/generated/librosa.tone.html", "https://librosa.org/doc/main/changelog.html"]}
{"python_version": "3.7", "library": "librosa", "version": "0.6.0", "extra_dependencies": "numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2", "problem": "Complete the function to construct a chirp or sine-sweep signal. The chirp sweeps from frequency fmin to fmax (in Hz).", "starting_code": "import librosa\nimport numpy as np\n\ndef compute_chirp(fmin: int, fmax: int, duration: int, sr: int, linear: bool) -> np.ndarray:\n    \"\"\"\n    Constructs a chirp or sine-sweep signal. The chirp sweeps from frequency fmin to fmax (in Hz).\n\n    Parameters:\n        fmin: The minimum frequency of the chirp in Hz.\n        fmax: The maximum frequency of the chirp in Hz.\n        duration: The duration of the chirp in seconds.\n        sr: The sampling rate of the signal in Hz.\n\n    Returns:\n        np.ndarray: The chirp signal.\n    \"\"\"\n    ", "example_id": "299", "test": "\nimport scipy\n\nfmin = 110\nfmax = 110*64\nduration = 1\nsr = 22050\nlinear = True\n\nsol  = compute_chirp(fmin, fmax, duration, sr, linear)\nperiod = 1.0 / sr\nphi = -np.pi * 0.5\nmethod = \"linear\" if linear else \"logarithmic\"\ntest_sol = scipy.signal.chirp(\n np.arange(int(duration * sr)) / sr,\n fmin,\n duration,\n fmax,\n method=method,\n phi=phi / np.pi * 180, # scipy.signal.chirp uses degrees for phase offset\n)\nassert np.array_equal(test_sol, sol)", "solution": "\n\n    import scipy\n    period = 1.0 / sr\n    phi = -np.pi * 0.5\n\n    method = \"linear\" if linear else \"logarithmic\"\n\n    return scipy.signal.chirp(np.arange(int(duration * sr)) / sr, fmin, duration, fmax, method=method, phi=phi / np.pi * 180, )", "type_of_change": "new feature", "name_of_class_or_func": "librosa.chirp", "additional_dependencies": "scikit-learn==0.21.0 numpy==1.16.0 scipy==1.1.0 soundfile==0.10.2", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["numpy.arange", "int", "scipy.signal.chirp"], "release_date": "2018-02", "docs": ["https://librosa.org/doc/main/generated/librosa.chirp.html", "https://librosa.org/doc/main/changelog.html"]}
{"python_version": "3.7", "library": "librosa", "version": "0.7.0", "extra_dependencies": "numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2", "problem": "Complete the function to construct a chirp or sine-sweep signal. The chirp sweeps from frequency fmin to fmax (in Hz).", "starting_code": "import librosa\nimport numpy as np\n\ndef compute_chirp(fmin: int, fmax: int, duration: int, sr: int, linear: bool) -> np.ndarray:\n    \"\"\"\n    Constructs a chirp or sine-sweep signal. The chirp sweeps from frequency fmin to fmax (in Hz).\n\n    Parameters:\n        fmin: The minimum frequency of the chirp in Hz.\n        fmax: The maximum frequency of the chirp in Hz.\n        duration: The duration of the chirp in seconds.\n        sr: The sampling rate of the signal in Hz.\n\n    Returns:\n        np.ndarray: The chirp signal.\n    \"\"\"    \n    ", "example_id": "300", "test": "\nfmin = 110\nfmax = 110*64\nduration = 1\nsr = 22050\nlinear = True\n\nsol  = compute_chirp(fmin, fmax, duration, sr, linear)\n\ntest_sol = librosa.chirp(fmin=fmin, fmax=fmax, duration=duration, sr=sr)\nassert np.array_equal(test_sol, sol)", "solution": "\n\n    return librosa.chirp(fmin=fmin, fmax=fmax, duration=duration, sr=sr)", "type_of_change": "new feature", "name_of_class_or_func": "librosa.chirp", "additional_dependencies": "numpy==1.16.0 scipy==1.1.0 soundfile==0.10.2", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["librosa.chirp"], "release_date": "2019-07", "docs": ["https://librosa.org/doc/main/generated/librosa.chirp.html", "https://librosa.org/doc/main/changelog.html"]}
{"python_version": "3.7", "library": "librosa", "version": "0.7.0", "extra_dependencies": "numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2", "problem": "Complete the function to shear a matrix by a given factor. ", "starting_code": "import librosa\nimport numpy as np\n\ndef compute_shear(E: np.ndarray, factor: int, axis: int) -> np.ndarray:\n    \n", "example_id": "301", "test": "\nE = np.eye(3)\nfactor=-1\naxis=-1\n\nsol = compute_shear(E, factor, axis)\ngt = np.array([[1., 1., 1.],\n [0., 0., 0.],\n [0., 0., 0.]])\nassert np.array_equal(gt, sol)", "solution": "\n\n    E_shear = np.empty_like(E)\n    for i in range(E.shape[1]):\n        E_shear[:, i] = np.roll(E[:, i], factor * i)\n    return E_shear\n    ", "type_of_change": "new feature", "name_of_class_or_func": "librosa.util.shear", "additional_dependencies": "numpy==1.16.0 scipy==1.1.0 soundfile==0.10.2", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["numpy.roll", "range", "numpy.empty_like"], "release_date": "2018-02", "docs": ["https://librosa.org/doc/main/generated/librosa.util.shear.html", "https://librosa.org/doc/main/changelog.html"]}
{"python_version": "3.7", "library": "librosa", "version": "0.7.1", "extra_dependencies": "numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2", "problem": "Complete the function to shear a matrix by a given factor. ", "starting_code": "import librosa\nimport numpy as np\n\ndef compute_shear(E: np.ndarray, factor: int, axis: int) -> np.ndarray:\n    \n", "example_id": "302", "test": "\nE = np.eye(3)\nfactor=-1\naxis=-1\n\nsol = compute_shear(E, factor, axis)\ngt = np.array([[1., 1., 1.],\n [0., 0., 0.],\n [0., 0., 0.]])\nassert np.array_equal(gt, sol)", "solution": "\n\n    return librosa.util.shear(E, factor=factor, axis=axis)", "type_of_change": "new feature", "name_of_class_or_func": "librosa.util.shear", "additional_dependencies": "numpy==1.16.0 scipy==1.1.0 soundfile==0.10.2", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["librosa.util.shear"], "release_date": "2019-07", "docs": ["https://librosa.org/doc/main/generated/librosa.util.shear.html", "https://librosa.org/doc/main/changelog.html"]}
{"python_version": "3.7", "library": "librosa", "version": "0.7.0", "extra_dependencies": "numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2", "problem": "Complete the function to locate the local minimas of an array. ", "starting_code": "import librosa\nimport numpy as np\n\ndef compute_localmin(x: np.ndarray, axis: int) -> np.ndarray:\n    ", "example_id": "303", "test": "\naxis=0\nx = np.array([[1,0,1], [2, -1, 0], [2, 1, 3]])\n\nsol = compute_localmin(x, axis)\ngt = np.array([[False, False, False],\n [False, True, True],\n [False, False, False]])\n\nassert np.array_equal(gt, sol)", "solution": "\n\n    return librosa.util.localmax(-x, axis=axis)", "type_of_change": "new feature", "name_of_class_or_func": "librosa.util.localmin", "additional_dependencies": "numpy==1.16.0 scipy==1.1.0 soundfile==0.10.2", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["librosa.util.localmax"], "release_date": "2019-07", "docs": ["https://librosa.org/doc/main/generated/librosa.util.localmin.html", "https://librosa.org/doc/main/changelog.html"]}
{"python_version": "3.7", "library": "librosa", "version": "0.8.0", "extra_dependencies": "numba==0.46 llvmlite==0.30 joblib==0.14 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2", "problem": "Complete the function to locate the local minimas of an array. ", "starting_code": "import librosa\nimport numpy as np\n\ndef compute_localmin(x: np.ndarray, axis: int) -> np.ndarray:\n    ", "example_id": "304", "test": "\naxis=0\nx = np.array([[1,0,1], [2, -1, 0], [2, 1, 3]])\n\nsol = compute_localmin(x, axis)\n\ngt = np.array([[False, False, False],\n [False, True, True],\n [False, False, False]])\n\nassert np.array_equal(gt, sol)", "solution": "\n\n    return librosa.util.localmin(x, axis=axis)", "type_of_change": "new feature", "name_of_class_or_func": "librosa.util.localmin", "additional_dependencies": "numpy==1.16.0 scipy==1.1.0 soundfile==0.10.2", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["librosa.util.localmin"], "release_date": "2020-07", "docs": ["https://librosa.org/doc/main/generated/librosa.util.localmin.html", "https://librosa.org/doc/main/changelog.html"]}
{"python_version": "3.7", "library": "librosa", "version": "0.7.0", "extra_dependencies": "numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2", "problem": "Complete the function to calculate the fundamental frequency (F0) estimation using the YIN algorithm.", "starting_code": "import librosa\nimport numpy as np\nimport scipy\nfrom typing import Optional\n\ndef compute_yin(sr: int, fmin: int, fmax: int, duration: float, period: float, phi: float, method: str, y: np.ndarray, frame_length: int, center: bool, pad_mode: str, win_length: Optional[int], hop_length: Optional[int], trough_threshold: float) -> np.ndarray:\n    \"\"\"\n    Calculates the fundamental frequency (F0) estimation using the YIN algorithm.\n\n    Parameters:\n        sr: The sampling rate of the audio signal in Hertz.\n        fmin: The minimum frequency to consider in Hz.\n        fmax: The maximum frequency to consider in Hz.\n        duration: The duration of the audio signal in seconds.\n        period: The period of the fundamental frequency in seconds.\n        phi: The phase of the fundamental frequency in radians.\n        method: Interpolation method.\n        y: The audio signal.\n        frame_length: The length of the frame in samples.\n        center: If True, the signal y is padded so that frame t is centered at y[t * hop_length].\n        pad_mode: Padding mode.\n        win_length: Window length.\n        hop_length: Hop length.\n        trough_threshold: Absolute threshold for peak estimation.\n\n    Returns:\n        The estimated fundamental frequency in Hz.\n    \"\"\"\n    \n    \n\n\n    ", "example_id": "305", "test": "sr=22050\nfmin = 440\nfmax = 880\nduration = 5.0\nperiod = 1.0 / sr\nphi = -np.pi * 0.5\nmethod = \"linear\" \ny = scipy.signal.chirp(\n np.arange(int(duration * sr)) / sr,\n fmin,\n duration,\n fmax,\n method=method,\n phi=phi / np.pi * 180, # scipy.signal.chirp uses degrees for phase offset\n)\nframe_length = 2048\ncenter = True\npad_mode = 'reflect'\nwin_length = None\nhop_length = None\ntrough_threshold = 0.1\n\nsol = compute_yin(sr, fmin, fmax, duration, period, phi, method, y, frame_length, center, pad_mode, win_length, hop_length, trough_threshold)\nif win_length is None:\n win_length = frame_length // 2\n\nif hop_length is None:\n hop_length = frame_length // 4\n\nif center:\n y = np.pad(y, frame_length // 2, mode=pad_mode)\n\ny_frames = librosa.util.frame(y, frame_length=frame_length, hop_length=hop_length)\n\nmin_period = max(int(np.floor(sr / fmax)), 1)\nmax_period = min(int(np.ceil(sr / fmin)), frame_length - win_length - 1)\n\na = np.fft.rfft(y_frames, frame_length, axis=0)\nb = np.fft.rfft(y_frames[win_length::-1, :], frame_length, axis=0)\nacf_frames = np.fft.irfft(a * b, frame_length, axis=0)[win_length:]\nacf_frames[np.abs(acf_frames) < 1e-6] = 0\n\nenergy_frames = np.cumsum(y_frames ** 2, axis=0)\nenergy_frames = energy_frames[win_length:, :] - energy_frames[:-win_length, :]\nenergy_frames[np.abs(energy_frames) < 1e-6] = 0\n\nyin_frames = energy_frames[0, :] + energy_frames - 2 * acf_frames\n\nyin_numerator = yin_frames[min_period : max_period + 1, :]\ntau_range = np.arange(1, max_period + 1)[:, None]\ncumulative_mean = np.cumsum(yin_frames[1 : max_period + 1, :], axis=0) / tau_range\nyin_denominator = cumulative_mean[min_period - 1 : max_period, :]\nyin_frames = yin_numerator / (yin_denominator + librosa.util.tiny(yin_denominator))\n\nparabolic_shifts = np.zeros_like(yin_frames)\nparabola_a = (yin_frames[:-2, :] + yin_frames[2:, :] - 2 * yin_frames[1:-1, :]) / 2\nparabola_b = (yin_frames[2:, :] - yin_frames[:-2, :]) / 2\nparabolic_shifts[1:-1, :] = -parabola_b / (2 * parabola_a + librosa.util.tiny(parabola_a))\nparabolic_shifts[np.abs(parabolic_shifts) > 1] = 0\n\nis_trough = librosa.util.localmax(-yin_frames, axis=0)\nis_trough[0, :] = yin_frames[0, :] < yin_frames[1, :]\n\nis_threshold_trough = np.logical_and(is_trough, yin_frames < trough_threshold)\n\nglobal_min = np.argmin(yin_frames, axis=0)\nyin_period = np.argmax(is_threshold_trough, axis=0)\nno_trough_below_threshold = np.all(~is_threshold_trough, axis=0)\nyin_period[no_trough_below_threshold] = global_min[no_trough_below_threshold]\n\nyin_period = (\n min_period\n + yin_period\n + parabolic_shifts[yin_period, range(yin_frames.shape[1])]\n)\n\ntest_sol = sr / yin_period\nassert np.allclose(test_sol, sol)", "solution": "\n    # Set the default window length if it is not already specified.\n    if win_length is None:\n        win_length = frame_length // 2\n\n\n    # Set the default hop if it is not already specified.\n    if hop_length is None:\n        hop_length = frame_length // 4\n\n    # Pad the time series so that frames are centered\n    if center:\n        y = np.pad(y, frame_length // 2, mode=pad_mode)\n\n    # Frame audio.\n    y_frames = librosa.util.frame(y, frame_length=frame_length, hop_length=hop_length)\n\n    # Calculate minimum and maximum periods\n    min_period = max(int(np.floor(sr / fmax)), 1)\n    max_period = min(int(np.ceil(sr / fmin)), frame_length - win_length - 1)\n\n    # Calculate cumulative mean normalized difference function.\n    # Autocorrelation.\n    a = np.fft.rfft(y_frames, frame_length, axis=0)\n    b = np.fft.rfft(y_frames[win_length::-1, :], frame_length, axis=0)\n    acf_frames = np.fft.irfft(a * b, frame_length, axis=0)[win_length:]\n    acf_frames[np.abs(acf_frames) < 1e-6] = 0\n\n    # Energy terms.\n    energy_frames = np.cumsum(y_frames ** 2, axis=0)\n    energy_frames = energy_frames[win_length:, :] - energy_frames[:-win_length, :]\n    energy_frames[np.abs(energy_frames) < 1e-6] = 0\n\n    # Difference function.\n    yin_frames = energy_frames[0, :] + energy_frames - 2 * acf_frames\n\n    # Cumulative mean normalized difference function.\n    yin_numerator = yin_frames[min_period : max_period + 1, :]\n    tau_range = np.arange(1, max_period + 1)[:, None]\n    cumulative_mean = np.cumsum(yin_frames[1 : max_period + 1, :], axis=0) / tau_range\n    yin_denominator = cumulative_mean[min_period - 1 : max_period, :]\n    yin_frames = yin_numerator / (yin_denominator + librosa.util.tiny(yin_denominator))\n\n    parabolic_shifts = np.zeros_like(yin_frames)\n    parabola_a = (yin_frames[:-2, :] + yin_frames[2:, :] - 2 * yin_frames[1:-1, :]) / 2\n    parabola_b = (yin_frames[2:, :] - yin_frames[:-2, :]) / 2\n    parabolic_shifts[1:-1, :] = -parabola_b / (2 * parabola_a + librosa.util.tiny(parabola_a))\n    parabolic_shifts[np.abs(parabolic_shifts) > 1] = 0\n\n    # Find local minima.\n    is_trough = librosa.util.localmax(-yin_frames, axis=0)\n    is_trough[0, :] = yin_frames[0, :] < yin_frames[1, :]\n\n    # Find minima below peak threshold.\n    is_threshold_trough = np.logical_and(is_trough, yin_frames < trough_threshold)\n\n    # Absolute threshold.\n    # \"The solution we propose is to set an absolute threshold and choose the\n    # smallest value of tau that gives a minimum of d' deeper than\n    # this threshold. If none is found, the global minimum is chosen instead.\"\n    global_min = np.argmin(yin_frames, axis=0)\n    yin_period = np.argmax(is_threshold_trough, axis=0)\n    no_trough_below_threshold = np.all(~is_threshold_trough, axis=0)\n    yin_period[no_trough_below_threshold] = global_min[no_trough_below_threshold]\n\n    # Refine peak by parabolic interpolation.\n    yin_period = (\n     min_period\n     + yin_period\n     + parabolic_shifts[yin_period, range(yin_frames.shape[1])]\n    )\n\n    # Convert period to fundamental frequency.\n    return sr / yin_period", "type_of_change": "new feature", "name_of_class_or_func": "librosa.yin", "additional_dependencies": "numpy==1.16.0 scipy==1.1.0 soundfile==0.10.2", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["librosa.util.frame", "numpy.ceil", "numpy.logical_and", "numpy.fft.rfft", "numpy.abs", "numpy.floor", "numpy.argmin", "numpy.pad", "numpy.fft.irfft", "numpy.arange", "librosa.util.tiny", "numpy.cumsum", "numpy.zeros_like", "int", "min", "numpy.argmax", "numpy.all", "librosa.util.localmax", "range", "max"], "release_date": "2019-07", "docs": ["https://librosa.org/doc/main/generated/librosa.yin.html", "https://librosa.org/doc/main/changelog.html"]}
{"python_version": "3.7", "library": "librosa", "version": "0.8.0", "extra_dependencies": "numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2", "problem": "Complete the function to calculate the fundamental frequency (F0) estimation using the YIN algorithm.", "starting_code": "import librosa\nimport numpy as np\nimport scipy\nfrom typing import Optional\n\ndef compute_yin(sr: int, fmin: int, fmax: int, duration: float, period: float, phi: float, method: str, y: np.ndarray, frame_length: int, center: bool, pad_mode: str, win_length: Optional[int], hop_length: Optional[int], trough_threshold: float) -> np.ndarray:\n    \"\"\"\n    Calculates the fundamental frequency (F0) estimation using the YIN algorithm.\n\n    Parameters:\n        sr: The sampling rate of the audio signal in Hertz.\n        fmin: The minimum frequency to consider in Hz.\n        fmax: The maximum frequency to consider in Hz.\n        duration: The duration of the audio signal in seconds.\n        period: The period of the fundamental frequency in seconds.\n        phi: The phase of the fundamental frequency in radians.\n        method: Interpolation method.\n        y: The audio signal.\n        frame_length: The length of the frame in samples.\n        center: If True, the signal y is padded so that frame t is centered at y[t * hop_length].\n        pad_mode: Padding mode.\n        win_length: Window length.\n        hop_length: Hop length.\n        trough_threshold: Absolute threshold for peak estimation.\n\n    Returns:\n        The estimated fundamental frequency in Hz.\n    \"\"\"\n", "example_id": "306", "test": "sr=22050\nfmin = 440\nfmax = 880\nduration = 5.0\nperiod = 1.0 / sr\nphi = -np.pi * 0.5\nmethod = \"linear\" \ny = scipy.signal.chirp(\n np.arange(int(duration * sr)) / sr,\n fmin,\n duration,\n fmax,\n method=method,\n phi=phi / np.pi * 180, # scipy.signal.chirp uses degrees for phase offset\n)\nframe_length = 2048\ncenter = True\npad_mode = 'reflect'\nwin_length = None\nhop_length = None\ntrough_threshold = 0.1\n\nsol = compute_yin(sr, fmin, fmax, duration, period, phi, method, y, frame_length, center, pad_mode, win_length, hop_length, trough_threshold)\nif win_length is None:\n win_length = frame_length // 2\n\nif hop_length is None:\n hop_length = frame_length // 4\n\nif center:\n y = np.pad(y, frame_length // 2, mode=pad_mode)\n\ny_frames = librosa.util.frame(y, frame_length=frame_length, hop_length=hop_length)\n\nmin_period = max(int(np.floor(sr / fmax)), 1)\nmax_period = min(int(np.ceil(sr / fmin)), frame_length - win_length - 1)\n\na = np.fft.rfft(y_frames, frame_length, axis=0)\nb = np.fft.rfft(y_frames[win_length::-1, :], frame_length, axis=0)\nacf_frames = np.fft.irfft(a * b, frame_length, axis=0)[win_length:]\nacf_frames[np.abs(acf_frames) < 1e-6] = 0\n\nenergy_frames = np.cumsum(y_frames ** 2, axis=0)\nenergy_frames = energy_frames[win_length:, :] - energy_frames[:-win_length, :]\nenergy_frames[np.abs(energy_frames) < 1e-6] = 0\n\nyin_frames = energy_frames[0, :] + energy_frames - 2 * acf_frames\n\nyin_numerator = yin_frames[min_period : max_period + 1, :]\ntau_range = np.arange(1, max_period + 1)[:, None]\ncumulative_mean = np.cumsum(yin_frames[1 : max_period + 1, :], axis=0) / tau_range\nyin_denominator = cumulative_mean[min_period - 1 : max_period, :]\nyin_frames = yin_numerator / (yin_denominator + librosa.util.tiny(yin_denominator))\n\nparabolic_shifts = np.zeros_like(yin_frames)\nparabola_a = (yin_frames[:-2, :] + yin_frames[2:, :] - 2 * yin_frames[1:-1, :]) / 2\nparabola_b = (yin_frames[2:, :] - yin_frames[:-2, :]) / 2\nparabolic_shifts[1:-1, :] = -parabola_b / (2 * parabola_a + librosa.util.tiny(parabola_a))\nparabolic_shifts[np.abs(parabolic_shifts) > 1] = 0\n\nis_trough = librosa.util.localmax(-yin_frames, axis=0)\nis_trough[0, :] = yin_frames[0, :] < yin_frames[1, :]\n\nis_threshold_trough = np.logical_and(is_trough, yin_frames < trough_threshold)\n\nglobal_min = np.argmin(yin_frames, axis=0)\nyin_period = np.argmax(is_threshold_trough, axis=0)\nno_trough_below_threshold = np.all(~is_threshold_trough, axis=0)\nyin_period[no_trough_below_threshold] = global_min[no_trough_below_threshold]\n\nyin_period = (\n min_period\n + yin_period\n + parabolic_shifts[yin_period, range(yin_frames.shape[1])]\n)\n\ntest_sol = sr / yin_period\nassert np.allclose(test_sol, sol)", "solution": "\n\n    return librosa.yin(y, fmin=fmin, fmax=fmax, sr=sr)", "type_of_change": "new feature", "name_of_class_or_func": "librosa.yin", "additional_dependencies": "numpy==1.16.0 scipy==1.1.0 soundfile==0.10.2", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["librosa.yin"], "release_date": "2020-07", "docs": ["https://librosa.org/doc/main/generated/librosa.yin.html", "https://librosa.org/doc/main/changelog.html"]}
{"python_version": "3.7", "library": "librosa", "version": "0.7.0", "extra_dependencies": "numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2", "problem": "Complete the function to calculate the fundamental frequency estimation using probabilistic YIN.", "starting_code": "import librosa\nimport numpy as np\nimport scipy\nfrom typing import Union, Optional, Tuple\n\nDTypeLike = Union[np.dtype, type]\n\ndef compute_pyin(freq: int, sr: int, y: np.ndarray, fmin: int, fmax: int, frame_length: int, center: bool, pad_mode: str, win_length: Optional[int], hop_length: Optional[int], n_thresholds: int, beta_parameters: Tuple[int], boltzmann_parameter: int, resolution: float, max_transition_rate: float, switch_prob: float, no_trough_prob: float, fill_na: DTypeLike) -> np.ndarray:\n    \"\"\"\n    Calculates the fundamental frequency estimation using probabilistic YIN.\n\n    Parameters:\n        freq: The frequency of the fundamental frequency in Hz.\n        sr: The sampling rate of the audio signal in Hertz.\n        y: The audio signal.\n        fmin: The minimum frequency to consider in Hz.\n        fmax: The maximum frequency to consider in Hz.\n        frame_length: The length of the frame in samples.\n        center: If True, the signal y is padded so that frame t is centered at y[t * hop_length].\n        pad_mode: Padding mode.\n        win_length: Window length.\n        hop_length: Hop length.\n        n_thresholds: Number of thresholds.\n        beta_parameters: Beta parameters.\n        boltzmann_parameter: Boltzmann parameter.\n        resolution: Resolution.\n        max_transition_rate: Maximum transition rate.\n        switch_prob: Switch probability.\n        no_trough_prob: No trough probability.\n        fill_na: Fill NA value.\n\n    Returns:\n        Time series of fundamental frequencies in Hertz.\n    \"\"\"\n    ", "example_id": "307", "test": "\nfreq=110\nsr=22050\ny = librosa.tone(freq, duration=1.0)\nfmin = 110\nfmax = 880\nframe_length = 2048\ncenter = False\npad_mode = 'reflect'\nwin_length = None\nhop_length = None\n#trough_threshold = 0.1\n\nn_thresholds=100\nbeta_parameters=(2, 18)\nboltzmann_parameter=2\nresolution=0.1\nmax_transition_rate=35.92\nswitch_prob=0.01\nno_trough_prob=0.01\nfill_na=np.nan\n\nsol = compute_pyin(freq, sr, y, fmin, fmax, frame_length, center, pad_mode, win_length, hop_length, n_thresholds, beta_parameters, boltzmann_parameter, resolution, max_transition_rate, switch_prob, no_trough_prob, fill_na)\n\nif win_length is None:\n    win_length = frame_length // 2\n\nif hop_length is None:\n    hop_length = frame_length // 4\n\nif center:\n    y = np.pad(y, frame_length // 2, mode=pad_mode)\n\ny_frames = librosa.util.frame(y, frame_length=frame_length, hop_length=hop_length)\n\nmin_period = max(int(np.floor(sr / fmax)), 1)\nmax_period = min(int(np.ceil(sr / fmin)), frame_length - win_length - 1)\n\na = np.fft.rfft(y_frames, frame_length, axis=0)\nb = np.fft.rfft(y_frames[win_length::-1, :], frame_length, axis=0)\nacf_frames = np.fft.irfft(a * b, frame_length, axis=0)[win_length:]\nacf_frames[np.abs(acf_frames) < 1e-6] = 0\n\nenergy_frames = np.cumsum(y_frames ** 2, axis=0)\nenergy_frames = energy_frames[win_length:, :] - energy_frames[:-win_length, :]\nenergy_frames[np.abs(energy_frames) < 1e-6] = 0\n\nyin_frames = energy_frames[0, :] + energy_frames - 2 * acf_frames\n\nyin_numerator = yin_frames[min_period : max_period + 1, :]\ntau_range = np.arange(1, max_period + 1)[:, None]\ncumulative_mean = np.cumsum(yin_frames[1 : max_period + 1, :], axis=0) / tau_range\nyin_denominator = cumulative_mean[min_period - 1 : max_period, :]\nyin_frames = yin_numerator / (yin_denominator + librosa.util.tiny(yin_denominator))\n\nparabolic_shifts = np.zeros_like(yin_frames)\nparabola_a = (yin_frames[:-2, :] + yin_frames[2:, :] - 2 * yin_frames[1:-1, :]) / 2\nparabola_b = (yin_frames[2:, :] - yin_frames[:-2, :]) / 2\nparabolic_shifts[1:-1, :] = -parabola_b / (2 * parabola_a + librosa.util.tiny(parabola_a))\nparabolic_shifts[np.abs(parabolic_shifts) > 1] = 0\n\nthresholds = np.linspace(0, 1, n_thresholds + 1)\nbeta_cdf = scipy.stats.beta.cdf(thresholds, beta_parameters[0], beta_parameters[1])\nbeta_probs = np.diff(beta_cdf)\n\nyin_probs = np.zeros_like(yin_frames)\nfor i, yin_frame in enumerate(yin_frames.T):\n    is_trough = librosa.util.localmax(-yin_frame, axis=0)\n    is_trough[0] = yin_frame[0] < yin_frame[1]\n    (trough_index,) = np.nonzero(is_trough)\n\n    if len(trough_index) == 0:\n        continue\n    trough_heights = yin_frame[trough_index]\n    trough_thresholds = trough_heights[:, None] < thresholds[None, 1:]\n\n    trough_positions = np.cumsum(trough_thresholds, axis=0) - 1\n    n_troughs = np.count_nonzero(trough_thresholds, axis=0)\n    trough_prior = scipy.stats.boltzmann.pmf(\n        trough_positions, boltzmann_parameter, n_troughs\n    )\n    trough_prior[~trough_thresholds] = 0\n\n    probs = np.sum(trough_prior * beta_probs, axis=1)\n    global_min = np.argmin(trough_heights)\n    n_thresholds_below_min = np.count_nonzero(~trough_thresholds[global_min, :])\n    probs[global_min] += no_trough_prob * np.sum(\n        beta_probs[:n_thresholds_below_min]\n    )\n\n    yin_probs[trough_index, i] = probs\n\nyin_period, frame_index = np.nonzero(yin_probs)\n\nperiod_candidates = min_period + yin_period\nperiod_candidates = period_candidates + parabolic_shifts[yin_period, frame_index]\nf0_candidates = sr / period_candidates\n\nn_bins_per_semitone = int(np.ceil(1.0 / resolution))\nn_pitch_bins = int(np.floor(12 * n_bins_per_semitone * np.log2(fmax / fmin))) + 1\n\n\nmax_semitones_per_frame = round(max_transition_rate * 12 * hop_length / sr)\ntransition_width = max_semitones_per_frame * n_bins_per_semitone + 1\n\ntransition = librosa.sequence.transition_local(\n    n_pitch_bins, transition_width, window=\"triangle\", wrap=False\n)\ntransition = np.block(\n    [\n        [(1 - switch_prob) * transition, switch_prob * transition],\n        [switch_prob * transition, (1 - switch_prob) * transition],\n    ]\n)\n\nbin_index = 12 * n_bins_per_semitone * np.log2(f0_candidates / fmin)\nbin_index = np.clip(np.round(bin_index), 0, n_pitch_bins).astype(int)\n\nobservation_probs = np.zeros((2 * n_pitch_bins, yin_frames.shape[1]))\nobservation_probs[bin_index, frame_index] = yin_probs[yin_period, frame_index]\nvoiced_prob = np.clip(np.sum(observation_probs[:n_pitch_bins, :], axis=0), 0, 1)\nobservation_probs[n_pitch_bins:, :] = (1 - voiced_prob[None, :]) / n_pitch_bins\n\np_init = np.zeros(2 * n_pitch_bins)\np_init[n_pitch_bins:] = 1 / n_pitch_bins\n\nstates = librosa.sequence.viterbi(observation_probs, transition, p_init=p_init)\n\nfreqs = fmin * 2 ** (np.arange(n_pitch_bins) / (12 * n_bins_per_semitone))\nf0 = freqs[states % n_pitch_bins]\nvoiced_flag = states < n_pitch_bins\nif fill_na is not None:\n    f0[~voiced_flag] = fill_na\ntest_sol = f0\nassert np.allclose(test_sol, sol)\nassert np.allclose(np.log2(sol), np.log2(freq), rtol=0, atol=1e-2)", "solution": "\n\n\n    if win_length is None:\n        win_length = frame_length // 2\n    \n    if hop_length is None:\n        hop_length = frame_length // 4\n    \n    if center:\n        y = np.pad(y, frame_length // 2, mode=pad_mode)\n    \n    y_frames = librosa.util.frame(y, frame_length=frame_length, hop_length=hop_length)\n    \n    min_period = max(int(np.floor(sr / fmax)), 1)\n    max_period = min(int(np.ceil(sr / fmin)), frame_length - win_length - 1)\n    \n    a = np.fft.rfft(y_frames, frame_length, axis=0)\n    b = np.fft.rfft(y_frames[win_length::-1, :], frame_length, axis=0)\n    acf_frames = np.fft.irfft(a * b, frame_length, axis=0)[win_length:]\n    acf_frames[np.abs(acf_frames) < 1e-6] = 0\n    \n    energy_frames = np.cumsum(y_frames ** 2, axis=0)\n    energy_frames = energy_frames[win_length:, :] - energy_frames[:-win_length, :]\n    energy_frames[np.abs(energy_frames) < 1e-6] = 0\n    \n    yin_frames = energy_frames[0, :] + energy_frames - 2 * acf_frames\n    \n    yin_numerator = yin_frames[min_period : max_period + 1, :]\n    tau_range = np.arange(1, max_period + 1)[:, None]\n    cumulative_mean = np.cumsum(yin_frames[1 : max_period + 1, :], axis=0) / tau_range\n    yin_denominator = cumulative_mean[min_period - 1 : max_period, :]\n    yin_frames = yin_numerator / (yin_denominator + librosa.util.tiny(yin_denominator))\n    \n    parabolic_shifts = np.zeros_like(yin_frames)\n    parabola_a = (yin_frames[:-2, :] + yin_frames[2:, :] - 2 * yin_frames[1:-1, :]) / 2\n    parabola_b = (yin_frames[2:, :] - yin_frames[:-2, :]) / 2\n    parabolic_shifts[1:-1, :] = -parabola_b / (2 * parabola_a + librosa.util.tiny(parabola_a))\n    parabolic_shifts[np.abs(parabolic_shifts) > 1] = 0\n    \n    thresholds = np.linspace(0, 1, n_thresholds + 1)\n    beta_cdf = scipy.stats.beta.cdf(thresholds, beta_parameters[0], beta_parameters[1])\n    beta_probs = np.diff(beta_cdf)\n    \n    yin_probs = np.zeros_like(yin_frames)\n    for i, yin_frame in enumerate(yin_frames.T):\n        is_trough = librosa.util.localmax(-yin_frame, axis=0)\n        is_trough[0] = yin_frame[0] < yin_frame[1]\n        (trough_index,) = np.nonzero(is_trough)\n    \n        if len(trough_index) == 0:\n            continue\n        \n        trough_heights = yin_frame[trough_index]\n        trough_thresholds = trough_heights[:, None] < thresholds[None, 1:]\n    \n        trough_positions = np.cumsum(trough_thresholds, axis=0) - 1\n        n_troughs = np.count_nonzero(trough_thresholds, axis=0)\n        trough_prior = scipy.stats.boltzmann.pmf(\n            trough_positions, boltzmann_parameter, n_troughs\n        )\n        trough_prior[~trough_thresholds] = 0\n    \n        probs = np.sum(trough_prior * beta_probs, axis=1)\n        global_min = np.argmin(trough_heights)\n        n_thresholds_below_min = np.count_nonzero(~trough_thresholds[global_min, :])\n        probs[global_min] += no_trough_prob * np.sum(\n            beta_probs[:n_thresholds_below_min]\n        )\n    \n        yin_probs[trough_index, i] = probs\n    \n    yin_period, frame_index = np.nonzero(yin_probs)\n    \n\n    period_candidates = min_period + yin_period\n    period_candidates = period_candidates + parabolic_shifts[yin_period, frame_index]\n    f0_candidates = sr / period_candidates\n    \n    n_bins_per_semitone = int(np.ceil(1.0 / resolution))\n    n_pitch_bins = int(np.floor(12 * n_bins_per_semitone * np.log2(fmax / fmin))) + 1\n    \n\n    max_semitones_per_frame = round(max_transition_rate * 12 * hop_length / sr)\n    transition_width = max_semitones_per_frame * n_bins_per_semitone + 1\n\n    transition = librosa.sequence.transition_local(\n        n_pitch_bins, transition_width, window=\"triangle\", wrap=False\n    )\n\n    transition = np.block(\n        [\n            [(1 - switch_prob) * transition, switch_prob * transition],\n            [switch_prob * transition, (1 - switch_prob) * transition],\n        ]\n    )\n    \n    bin_index = 12 * n_bins_per_semitone * np.log2(f0_candidates / fmin)\n    bin_index = np.clip(np.round(bin_index), 0, n_pitch_bins).astype(int)\n    \n    observation_probs = np.zeros((2 * n_pitch_bins, yin_frames.shape[1]))\n    observation_probs[bin_index, frame_index] = yin_probs[yin_period, frame_index]\n    voiced_prob = np.clip(np.sum(observation_probs[:n_pitch_bins, :], axis=0), 0, 1)\n    observation_probs[n_pitch_bins:, :] = (1 - voiced_prob[None, :]) / n_pitch_bins\n    \n    p_init = np.zeros(2 * n_pitch_bins)\n    p_init[n_pitch_bins:] = 1 / n_pitch_bins\n    \n    states = librosa.sequence.viterbi(observation_probs, transition, p_init=p_init)\n    \n    freqs = fmin * 2 ** (np.arange(n_pitch_bins) / (12 * n_bins_per_semitone))\n    f0 = freqs[states % n_pitch_bins]\n    voiced_flag = states < n_pitch_bins\n    if fill_na is not None:\n        f0[~voiced_flag] = fill_na\n    \n    return f0", "type_of_change": "new feature", "name_of_class_or_func": "librosa.pyin", "additional_dependencies": "numpy==1.16.0 scipy==1.1.0 soundfile==0.10.2", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["numpy.block", "librosa.util.frame", "scipy.stats.beta.cdf", "numpy.sum", "numpy.ceil", "numpy.nonzero", "numpy.fft.rfft", "numpy.abs", "numpy.floor", "numpy.argmin", "numpy.round", "numpy.clip", "numpy.pad", "enumerate", "numpy.fft.irfft", "astype", "round", "scipy.stats.boltzmann.pmf", "numpy.linspace", "numpy.arange", "numpy.zeros", "librosa.util.tiny", "len", "numpy.cumsum", "numpy.diff", "numpy.zeros_like", "int", "librosa.sequence.viterbi", "min", "numpy.log2", "numpy.count_nonzero", "librosa.util.localmax", "librosa.sequence.transition_local", "max"], "release_date": "2019-07", "docs": ["https://librosa.org/doc/latest/generated/librosa.pyin.html", "https://librosa.org/doc/main/changelog.html"]}
{"python_version": "3.7", "library": "librosa", "version": "0.8.0", "extra_dependencies": "numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2", "problem": "Complete the function to calculate the fundamental frequency estimation using probabilistic YIN.", "starting_code": "import librosa\nimport numpy as np\nimport scipy\nfrom typing import Union, Optional, Tuple\n\nDTypeLike = Union[np.dtype, type]\n\ndef compute_pyin(freq: int, sr: int, y: int, fmin: int, fmax: int, frame_length: int, center: bool, pad_mode: str, win_length: Optional[int], hop_length: Optional[int], n_thresholds: int, beta_parameters: Tuple[int], boltzmann_parameter: int, resolution: float, max_transition_rate: float, switch_prob: float, no_trough_prob: float, fill_na: DTypeLike) -> np.ndarray:\n    \"\"\"\n    Calculates the fundamental frequency estimation using probabilistic YIN.\n\n    Parameters:\n        freq: The frequency of the fundamental frequency in Hz.\n        sr: The sampling rate of the audio signal in Hertz.\n        y: The audio signal.\n        fmin: The minimum frequency to consider in Hz.\n        fmax: The maximum frequency to consider in Hz.\n        frame_length: The length of the frame in samples.\n        center: If True, the signal y is padded so that frame t is centered at y[t * hop_length].\n        pad_mode: Padding mode.\n        win_length: Window length.\n        hop_length: Hop length.\n        n_thresholds: Number of thresholds.\n        beta_parameters: Beta parameters.\n        boltzmann_parameter: Boltzmann parameter.\n        resolution: Resolution.\n        max_transition_rate: Maximum transition rate.\n        switch_prob: Switch probability.\n        no_trough_prob: No trough probability.\n        fill_na: Fill NA value.\n\n    Returns:\n        Time series of fundamental frequencies in Hertz.\n    \"\"\"    ", "example_id": "308", "test": "\nfreq=110\nsr=22050\ny = librosa.tone(freq, duration=1.0)\nfmin = 110\nfmax = 880\nframe_length = 2048\ncenter = False\npad_mode = 'reflect'\nwin_length = None\nhop_length = None\n#trough_threshold = 0.1\n\nn_thresholds=100\nbeta_parameters=(2, 18)\nboltzmann_parameter=2\nresolution=0.1\nmax_transition_rate=35.92\nswitch_prob=0.01\nno_trough_prob=0.01\nfill_na=np.nan\n\nsol = compute_pyin(freq, sr, y, fmin, fmax, frame_length, center, pad_mode, win_length, hop_length, n_thresholds, beta_parameters, boltzmann_parameter, resolution, max_transition_rate, switch_prob, no_trough_prob, fill_na)\n\nif win_length is None:\n    win_length = frame_length // 2\n\nif hop_length is None:\n    hop_length = frame_length // 4\n\nif center:\n    y = np.pad(y, frame_length // 2, mode=pad_mode)\n\ny_frames = librosa.util.frame(y, frame_length=frame_length, hop_length=hop_length)\n\nmin_period = max(int(np.floor(sr / fmax)), 1)\nmax_period = min(int(np.ceil(sr / fmin)), frame_length - win_length - 1)\n\na = np.fft.rfft(y_frames, frame_length, axis=0)\nb = np.fft.rfft(y_frames[win_length::-1, :], frame_length, axis=0)\nacf_frames = np.fft.irfft(a * b, frame_length, axis=0)[win_length:]\nacf_frames[np.abs(acf_frames) < 1e-6] = 0\n\nenergy_frames = np.cumsum(y_frames ** 2, axis=0)\nenergy_frames = energy_frames[win_length:, :] - energy_frames[:-win_length, :]\nenergy_frames[np.abs(energy_frames) < 1e-6] = 0\n\nyin_frames = energy_frames[0, :] + energy_frames - 2 * acf_frames\n\nyin_numerator = yin_frames[min_period : max_period + 1, :]\ntau_range = np.arange(1, max_period + 1)[:, None]\ncumulative_mean = np.cumsum(yin_frames[1 : max_period + 1, :], axis=0) / tau_range\nyin_denominator = cumulative_mean[min_period - 1 : max_period, :]\nyin_frames = yin_numerator / (yin_denominator + librosa.util.tiny(yin_denominator))\n\nparabolic_shifts = np.zeros_like(yin_frames)\nparabola_a = (yin_frames[:-2, :] + yin_frames[2:, :] - 2 * yin_frames[1:-1, :]) / 2\nparabola_b = (yin_frames[2:, :] - yin_frames[:-2, :]) / 2\nparabolic_shifts[1:-1, :] = -parabola_b / (2 * parabola_a + librosa.util.tiny(parabola_a))\nparabolic_shifts[np.abs(parabolic_shifts) > 1] = 0\n\nthresholds = np.linspace(0, 1, n_thresholds + 1)\nbeta_cdf = scipy.stats.beta.cdf(thresholds, beta_parameters[0], beta_parameters[1])\nbeta_probs = np.diff(beta_cdf)\n\nyin_probs = np.zeros_like(yin_frames)\nfor i, yin_frame in enumerate(yin_frames.T):\n    is_trough = librosa.util.localmax(-yin_frame, axis=0)\n    is_trough[0] = yin_frame[0] < yin_frame[1]\n    (trough_index,) = np.nonzero(is_trough)\n\n    if len(trough_index) == 0:\n        continue\n    trough_heights = yin_frame[trough_index]\n    trough_thresholds = trough_heights[:, None] < thresholds[None, 1:]\n\n    trough_positions = np.cumsum(trough_thresholds, axis=0) - 1\n    n_troughs = np.count_nonzero(trough_thresholds, axis=0)\n    trough_prior = scipy.stats.boltzmann.pmf(\n        trough_positions, boltzmann_parameter, n_troughs\n    )\n    trough_prior[~trough_thresholds] = 0\n\n    probs = np.sum(trough_prior * beta_probs, axis=1)\n    global_min = np.argmin(trough_heights)\n    n_thresholds_below_min = np.count_nonzero(~trough_thresholds[global_min, :])\n    probs[global_min] += no_trough_prob * np.sum(\n        beta_probs[:n_thresholds_below_min]\n    )\n\n    yin_probs[trough_index, i] = probs\n\nyin_period, frame_index = np.nonzero(yin_probs)\n\nperiod_candidates = min_period + yin_period\nperiod_candidates = period_candidates + parabolic_shifts[yin_period, frame_index]\nf0_candidates = sr / period_candidates\n\nn_bins_per_semitone = int(np.ceil(1.0 / resolution))\nn_pitch_bins = int(np.floor(12 * n_bins_per_semitone * np.log2(fmax / fmin))) + 1\n\n\nmax_semitones_per_frame = round(max_transition_rate * 12 * hop_length / sr)\ntransition_width = max_semitones_per_frame * n_bins_per_semitone + 1\n\ntransition = librosa.sequence.transition_local(\n    n_pitch_bins, transition_width, window=\"triangle\", wrap=False\n)\ntransition = np.block(\n    [\n        [(1 - switch_prob) * transition, switch_prob * transition],\n        [switch_prob * transition, (1 - switch_prob) * transition],\n    ]\n)\n\nbin_index = 12 * n_bins_per_semitone * np.log2(f0_candidates / fmin)\nbin_index = np.clip(np.round(bin_index), 0, n_pitch_bins).astype(int)\n\nobservation_probs = np.zeros((2 * n_pitch_bins, yin_frames.shape[1]))\nobservation_probs[bin_index, frame_index] = yin_probs[yin_period, frame_index]\nvoiced_prob = np.clip(np.sum(observation_probs[:n_pitch_bins, :], axis=0), 0, 1)\nobservation_probs[n_pitch_bins:, :] = (1 - voiced_prob[None, :]) / n_pitch_bins\n\np_init = np.zeros(2 * n_pitch_bins)\np_init[n_pitch_bins:] = 1 / n_pitch_bins\n\nstates = librosa.sequence.viterbi(observation_probs, transition, p_init=p_init)\n\nfreqs = fmin * 2 ** (np.arange(n_pitch_bins) / (12 * n_bins_per_semitone))\nf0 = freqs[states % n_pitch_bins]\nvoiced_flag = states < n_pitch_bins\nif fill_na is not None:\n    f0[~voiced_flag] = fill_na\ntest_sol = f0\nassert np.allclose(test_sol, sol)\nassert np.allclose(np.log2(sol), np.log2(freq), rtol=0, atol=1e-2)", "solution": "\n\n    return librosa.pyin(y, fmin=fmin, fmax=fmax, center=center)[0]", "type_of_change": "new feature", "name_of_class_or_func": "librosa.pyin", "additional_dependencies": "numpy==1.16.0 scipy==1.1.0 soundfile==0.10.2", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["librosa.pyin"], "release_date": "2020-07", "docs": ["https://librosa.org/doc/latest/generated/librosa.pyin.html", "https://librosa.org/doc/main/changelog.html"]}
{"python_version": "3.7", "library": "librosa", "version": "0.7.0", "extra_dependencies": "numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2", "problem": "Complete the function to compute the variable-Q transform of an audio signal.", "starting_code": "import librosa\nimport numpy as np\nimport scipy\nfrom typing import Union\n\nDTypeLike = Union[np.dtype, type]\n\n\ndef compute_vqt(y: np.ndarray, sr: int, hop_length: int, fmin: int, n_bins: int, gamma: int, bins_per_octave: int, tuning: float, filter_scale: int, norm: 1, sparsity: float, window: str, scale: bool, pad_mode: str, res_type: str, dtype: DTypeLike) -> np.ndarray:\n    ", "example_id": "309", "test": "\nfilename = librosa.util.example_audio_file()\ny, sr = librosa.load(filename)\nhop_length=512\nfmin=None\nn_bins=84\ngamma=None\nbins_per_octave=12\ntuning=0.0\nfilter_scale=1\nnorm=1\nsparsity=0.01\nwindow=\"hann\"\nscale=True\npad_mode=\"reflect\"\nres_type=None\ndtype=None\n\nsol = compute_vqt(y, sr, hop_length, fmin, n_bins, gamma, bins_per_octave, tuning, filter_scale, norm, sparsity, window, scale, pad_mode, res_type, dtype)\n\ndef dtype_r2c(d, default=np.complex64):\n    mapping = {\n        np.dtype(np.float32): np.complex64,\n        np.dtype(np.float64): np.complex128,\n        np.dtype(np.float): np.complex,\n    }\n\n    dt = np.dtype(d)\n    if dt.kind == \"c\":\n        return dt\n\n    return np.dtype(mapping.get(dt, default))\n\nn_octaves = int(np.ceil(float(n_bins) / bins_per_octave))\nn_filters = min(bins_per_octave, n_bins)\n\nlen_orig = len(y)\n\nalpha = 2.0 ** (1.0 / bins_per_octave) - 1\n\nif fmin is None:\n    fmin = librosa.note_to_hz(\"C1\")\n\nif tuning is None:\n    tuning = librosa.pitch.estimate_tuning(y=y, sr=sr, bins_per_octave=bins_per_octave)\n\nif gamma is None:\n    gamma = 24.7 * alpha / 0.108\n\nif dtype is None:\n    dtype = dtype_r2c(y.dtype)\n\nfmin = fmin * 2.0 ** (tuning / bins_per_octave)\n\nfreqs = librosa.time_frequency.cqt_frequencies(n_bins, fmin, bins_per_octave=bins_per_octave)[\n    -bins_per_octave:\n]\n\nfmin_t = np.min(freqs)\nfmax_t = np.max(freqs)\n\nQ = float(filter_scale) / alpha\nfilter_cutoff = (\n    fmax_t * (1 + 0.5 * librosa.filters.window_bandwidth(window) / Q) + 0.5 * gamma\n)\nnyquist = sr / 2.0\n\nauto_resample = False\nif not res_type:\n    auto_resample = True\n    if filter_cutoff < librosa.audio.BW_FASTEST * nyquist:\n        res_type = \"kaiser_fast\"\n    else:\n        res_type = \"kaiser_best\"\n\ndownsample_count1 = max(\n    0, int(np.ceil(np.log2(librosa.audio.BW_FASTEST * nyquist / filter_cutoff)) - 1) - 1\n)\n\ndef num_two_factors(x):\n    if x <= 0:\n        return 0\n    num_twos = 0\n    while x % 2 == 0:\n        num_twos += 1\n        x //= 2\n\n    return num_twos\nnum_twos=num_two_factors(hop_length)\ndownsample_count2 = max(0, num_twos - n_octaves + 1)\ndownsample_count = min(downsample_count1, downsample_count2)\n\n\nvqt_resp = []\n\nnum_twos=num_two_factors(hop_length)\nif num_twos < n_octaves - 1:\n    raise ParameterError(\n        \"hop_length must be a positive integer \"\n        \"multiple of 2^{0:d} for {1:d}-octave CQT/VQT\".format(\n            n_octaves - 1, n_octaves\n        )\n    )\n\nmy_y, my_sr, my_hop = y, sr, hop_length\ndef sparsify_rows(x, quantile=0.01, dtype=None):\n\n    if x.ndim == 1:\n        x = x.reshape((1, -1))\n\n    elif x.ndim > 2:\n        raise ParameterError(\n            \"Input must have 2 or fewer dimensions. \"\n            \"Provided x.shape={}.\".format(x.shape)\n        )\n\n    if not 0.0 <= quantile < 1:\n        raise ParameterError(\"Invalid quantile {:.2f}\".format(quantile))\n\n    if dtype is None:\n        dtype = x.dtype\n\n    x_sparse = scipy.sparse.lil_matrix(x.shape, dtype=dtype)\n\n    mags = np.abs(x)\n    norms = np.sum(mags, axis=1, keepdims=True)\n\n    mag_sort = np.sort(mags, axis=1)\n    cumulative_mag = np.cumsum(mag_sort / norms, axis=1)\n\n    threshold_idx = np.argmin(cumulative_mag < quantile, axis=1)\n\n    for i, j in enumerate(threshold_idx):\n        idx = np.where(mags[i] >= mag_sort[i, j])\n        x_sparse[i, idx] = x[i, idx]\n\n    return x_sparse.tocsr()\n\ndef cqt_filter_fft(\n    sr,\n    fmin,\n    n_bins,\n    bins_per_octave,\n    filter_scale,\n    norm,\n    sparsity,\n    hop_length=None,\n    window=\"hann\",\n    gamma=0.0,\n    dtype=np.complex,\n):\n    basis, lengths = librosa.filters.constant_q(\n        sr,\n        fmin=fmin,\n        n_bins=n_bins,\n        bins_per_octave=bins_per_octave,\n        filter_scale=filter_scale,\n        norm=norm,\n        pad_fft=True,\n        window=window,\n    )\n\n    n_fft = basis.shape[1]\n\n    if hop_length is not None and n_fft < 2.0 ** (1 + np.ceil(np.log2(hop_length))):\n        n_fft = int(2.0 ** (1 + np.ceil(np.log2(hop_length))))\n\n    basis *= lengths[:, np.newaxis] / float(n_fft)\n\n    fft = librosa.get_fftlib()\n    fft_basis = fft.fft(basis, n=n_fft, axis=1)[:, : (n_fft // 2) + 1]\n\n    fft_basis = sparsify_rows(fft_basis, quantile=sparsity, dtype=dtype)\n\n    return fft_basis, n_fft, lengths\n\n\ndef cqt_response(y, n_fft, hop_length, fft_basis, mode, dtype=None):\n    D = librosa.stft(\n        y, n_fft=n_fft, hop_length=hop_length, window=\"ones\", pad_mode=mode, dtype=dtype\n    )\n    return fft_basis.dot(D)\n\nfor i in range(n_octaves):\n    if i > 0:\n        if len(my_y) < 2:\n            raise ParameterError(\n                \"Input signal length={} is too short for \"\n                \"{:d}-octave CQT/VQT\".format(len_orig, n_octaves)\n            )\n\n        my_y = librosa.audio.resample(my_y, 2, 1, res_type=res_type, scale=True)\n\n        my_sr /= 2.0\n        my_hop //= 2\n\n    fft_basis, n_fft, _ = cqt_filter_fft(\n        my_sr,\n        fmin_t * 2.0 ** -i,\n        n_filters,\n        bins_per_octave,\n        filter_scale,\n        norm,\n        sparsity,\n        window=window,\n        gamma=gamma,\n        dtype=dtype,\n    )\n\n    fft_basis[:] *= np.sqrt(2 ** i)\n\n    vqt_resp.append(\n        cqt_response(my_y, n_fft, my_hop, fft_basis, pad_mode, dtype=dtype)\n    )\n\ndef trim_stack(cqt_resp, n_bins, dtype):\n    max_col = min(c_i.shape[-1] for c_i in cqt_resp)\n    cqt_out = np.empty((n_bins, max_col), dtype=dtype, order=\"F\")\n\n    end = n_bins\n    for c_i in cqt_resp:\n        n_oct = c_i.shape[0]\n        if end < n_oct:\n            cqt_out[:end] = c_i[-end:, :max_col]\n        else:\n            cqt_out[end - n_oct : end] = c_i[:, :max_col]\n\n        end -= n_oct\n\n    return cqt_out\n\nV = trim_stack(vqt_resp, n_bins, dtype)\n\nif scale:\n    lengths = librosa.filters.constant_q_lengths(\n        sr,\n        fmin,\n        n_bins=n_bins,\n        bins_per_octave=bins_per_octave,\n        window=window,\n        filter_scale=filter_scale,\n    )\n    V /= np.sqrt(lengths[:, np.newaxis])\n\ntest_sol = V\nassert np.allclose(test_sol, sol)", "solution": "\n   # How many octaves are we dealing with?\n    def dtype_r2c(d, default=np.complex64):\n        \"\"\"Find the complex numpy dtype corresponding to a real dtype.\n\n        This is used to maintain numerical precision and memory footprint\n        when constructing complex arrays from real-valued data\n        (e.g. in a Fourier transform).\n\n        A `float32` (single-precision) type maps to `complex64`,\n        while a `float64` (double-precision) maps to `complex128`.\n\n\n        Parameters\n        ----------\n        d : np.dtype\n            The real-valued dtype to convert to complex.\n            If ``d`` is a complex type already, it will be returned.\n\n        default : np.dtype, optional\n            The default complex target type, if ``d`` does not match a\n            known dtype\n\n        Returns\n        -------\n        d_c : np.dtype\n            The complex dtype\n\n        See Also\n        --------\n        dtype_c2r\n        numpy.dtype\n\n        \"\"\"\n        mapping = {\n            np.dtype(np.float32): np.complex64,\n            np.dtype(np.float64): np.complex128,\n            np.dtype(np.float): np.complex,\n        }\n\n        # If we're given a complex type already, return it\n        dt = np.dtype(d)\n        if dt.kind == \"c\":\n            return dt\n\n        # Otherwise, try to map the dtype.\n        # If no match is found, return the default.\n        return np.dtype(mapping.get(dt, default))\n\n    n_octaves = int(np.ceil(float(n_bins) / bins_per_octave))\n    n_filters = min(bins_per_octave, n_bins)\n\n    len_orig = len(y)\n\n    # Relative difference in frequency between any two consecutive bands\n    alpha = 2.0 ** (1.0 / bins_per_octave) - 1\n\n    if fmin is None:\n        # C1 by default\n        fmin = librosa.note_to_hz(\"C1\")\n\n    if tuning is None:\n        tuning = librosa.pitch.estimate_tuning(y=y, sr=sr, bins_per_octave=bins_per_octave)\n\n    if gamma is None:\n        gamma = 24.7 * alpha / 0.108\n\n    if dtype is None:\n        dtype = dtype_r2c(y.dtype)\n\n    # Apply tuning correction\n    fmin = fmin * 2.0 ** (tuning / bins_per_octave)\n\n    # First thing, get the freqs of the top octave\n    freqs = librosa.time_frequency.cqt_frequencies(n_bins, fmin, bins_per_octave=bins_per_octave)[\n        -bins_per_octave:\n    ]\n\n    fmin_t = np.min(freqs)\n    fmax_t = np.max(freqs)\n\n    # Determine required resampling quality\n    Q = float(filter_scale) / alpha\n    filter_cutoff = (\n        fmax_t * (1 + 0.5 * librosa.filters.window_bandwidth(window) / Q) + 0.5 * gamma\n    )\n    nyquist = sr / 2.0\n\n    auto_resample = False\n    if not res_type:\n        auto_resample = True\n        if filter_cutoff < librosa.audio.BW_FASTEST * nyquist:\n            res_type = \"kaiser_fast\"\n        else:\n            res_type = \"kaiser_best\"\n\n    downsample_count1 = max(\n        0, int(np.ceil(np.log2(librosa.audio.BW_FASTEST * nyquist / filter_cutoff)) - 1) - 1\n    )\n\n    def num_two_factors(x):\n        if x <= 0:\n            return 0\n        num_twos = 0\n        while x % 2 == 0:\n            num_twos += 1\n            x //= 2\n\n        return num_twos\n    num_twos=num_two_factors(hop_length)\n    downsample_count2 = max(0, num_twos - n_octaves + 1)\n    downsample_count = min(downsample_count1, downsample_count2)\n\n\n    vqt_resp = []\n\n    # Make sure our hop is long enough to support the bottom octave\n\n    num_twos=num_two_factors(hop_length)\n\n\n    #num_twos = __num_two_factors(hop_length)\n    if num_twos < n_octaves - 1:\n        raise ParameterError(\n            \"hop_length must be a positive integer \"\n            \"multiple of 2^{0:d} for {1:d}-octave CQT/VQT\".format(\n                n_octaves - 1, n_octaves\n            )\n        )\n\n    # Now do the recursive bit\n    my_y, my_sr, my_hop = y, sr, hop_length\n    def sparsify_rows(x, quantile=0.01, dtype=None):\n        \"\"\"Return a row-sparse matrix approximating the input\n\n        Parameters\n        ----------\n        x : np.ndarray [ndim <= 2]\n            The input matrix to sparsify.\n\n        quantile : float in [0, 1.0)\n            Percentage of magnitude to discard in each row of ``x``\n\n        dtype : np.dtype, optional\n            The dtype of the output array.\n            If not provided, then ``x.dtype`` will be used.\n\n        Returns\n        -------\n        x_sparse : ``scipy.sparse.csr_matrix`` [shape=x.shape]\n            Row-sparsified approximation of ``x``\n\n            If ``x.ndim == 1``, then ``x`` is interpreted as a row vector,\n            and ``x_sparse.shape == (1, len(x))``.\n\n        Raises\n        ------\n        ParameterError\n            If ``x.ndim > 2``\n\n            If ``quantile`` lies outside ``[0, 1.0)``\n        \"\"\"\n\n        if x.ndim == 1:\n            x = x.reshape((1, -1))\n\n        elif x.ndim > 2:\n            raise ParameterError(\n                \"Input must have 2 or fewer dimensions. \"\n                \"Provided x.shape={}.\".format(x.shape)\n            )\n\n        if not 0.0 <= quantile < 1:\n            raise ParameterError(\"Invalid quantile {:.2f}\".format(quantile))\n\n        if dtype is None:\n            dtype = x.dtype\n\n        x_sparse = scipy.sparse.lil_matrix(x.shape, dtype=dtype)\n\n        mags = np.abs(x)\n        norms = np.sum(mags, axis=1, keepdims=True)\n\n        mag_sort = np.sort(mags, axis=1)\n        cumulative_mag = np.cumsum(mag_sort / norms, axis=1)\n\n        threshold_idx = np.argmin(cumulative_mag < quantile, axis=1)\n\n        for i, j in enumerate(threshold_idx):\n            idx = np.where(mags[i] >= mag_sort[i, j])\n            x_sparse[i, idx] = x[i, idx]\n\n        return x_sparse.tocsr()\n\n    def cqt_filter_fft(\n        sr,\n        fmin,\n        n_bins,\n        bins_per_octave,\n        filter_scale,\n        norm,\n        sparsity,\n        hop_length=None,\n        window=\"hann\",\n        gamma=0.0,\n        dtype=np.complex,\n    ):\n        \"\"\"Generate the frequency domain constant-Q filter basis.\"\"\"\n\n        basis, lengths = librosa.filters.constant_q(\n            sr,\n            fmin=fmin,\n            n_bins=n_bins,\n            bins_per_octave=bins_per_octave,\n            filter_scale=filter_scale,\n            norm=norm,\n            pad_fft=True,\n            window=window,\n        )\n\n        # Filters are padded up to the nearest integral power of 2\n        n_fft = basis.shape[1]\n\n        if hop_length is not None and n_fft < 2.0 ** (1 + np.ceil(np.log2(hop_length))):\n\n            n_fft = int(2.0 ** (1 + np.ceil(np.log2(hop_length))))\n\n        # re-normalize bases with respect to the FFT window length\n        basis *= lengths[:, np.newaxis] / float(n_fft)\n\n        # FFT and retain only the non-negative frequencies\n        fft = librosa.get_fftlib()\n        fft_basis = fft.fft(basis, n=n_fft, axis=1)[:, : (n_fft // 2) + 1]\n\n        # sparsify the basis\n        fft_basis = sparsify_rows(fft_basis, quantile=sparsity, dtype=dtype)\n\n        return fft_basis, n_fft, lengths\n\n\n    def cqt_response(y, n_fft, hop_length, fft_basis, mode, dtype=None):\n        \"\"\"Compute the filter response with a target STFT hop.\"\"\"\n\n        # Compute the STFT matrix\n        D = librosa.stft(\n            y, n_fft=n_fft, hop_length=hop_length, window=\"ones\", pad_mode=mode, dtype=dtype\n        )\n\n        # And filter response energy\n        return fft_basis.dot(D)\n\n    # Iterate down the octaves\n    for i in range(n_octaves):\n        # Resample (except first time)\n        if i > 0:\n            if len(my_y) < 2:\n                raise ParameterError(\n                    \"Input signal length={} is too short for \"\n                    \"{:d}-octave CQT/VQT\".format(len_orig, n_octaves)\n                )\n\n            my_y = librosa.audio.resample(my_y, 2, 1, res_type=res_type, scale=True)\n\n            my_sr /= 2.0\n            my_hop //= 2\n\n        fft_basis, n_fft, _ = cqt_filter_fft(\n            my_sr,\n            fmin_t * 2.0 ** -i,\n            n_filters,\n            bins_per_octave,\n            filter_scale,\n            norm,\n            sparsity,\n            window=window,\n            gamma=gamma,\n            dtype=dtype,\n        )\n\n        # Re-scale the filters to compensate for downsampling\n        fft_basis[:] *= np.sqrt(2 ** i)\n\n        # Compute the vqt filter response and append to the stack\n        vqt_resp.append(\n            cqt_response(my_y, n_fft, my_hop, fft_basis, pad_mode, dtype=dtype)\n        )\n\n    def trim_stack(cqt_resp, n_bins, dtype):\n        \"\"\"Helper function to trim and stack a collection of CQT responses\"\"\"\n\n        max_col = min(c_i.shape[-1] for c_i in cqt_resp)\n        cqt_out = np.empty((n_bins, max_col), dtype=dtype, order=\"F\")\n\n        # Copy per-octave data into output array\n        end = n_bins\n        for c_i in cqt_resp:\n            # By default, take the whole octave\n            n_oct = c_i.shape[0]\n            # If the whole octave is more than we can fit,\n            # take the highest bins from c_i\n            if end < n_oct:\n                cqt_out[:end] = c_i[-end:, :max_col]\n            else:\n                cqt_out[end - n_oct : end] = c_i[:, :max_col]\n\n            end -= n_oct\n\n        return cqt_out\n\n    V = trim_stack(vqt_resp, n_bins, dtype)\n\n    if scale:\n        lengths = librosa.filters.constant_q_lengths(\n            sr,\n            fmin,\n            n_bins=n_bins,\n            bins_per_octave=bins_per_octave,\n            window=window,\n            filter_scale=filter_scale,\n        )\n        V /= np.sqrt(lengths[:, np.newaxis])\n    return V", "type_of_change": "new feature", "name_of_class_or_func": "librosa.vqt", "additional_dependencies": "numpy==1.16.0 scipy==1.1.0 soundfile==0.10.2", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["librosa.audio.resample", "numpy.sort", "float", "numpy.sum", "numpy.ceil", "__num_two_factors", "dtype_r2c", "trim_stack", "numpy.abs", "fft.fft", "cqt_filter_fft", "numpy.argmin", "numpy.empty", "librosa.filters.window_bandwidth", "enumerate", "librosa.pitch.estimate_tuning", "librosa.stft", "mapping.get", "x_sparse.tocsr", "Resample", "sparsify_rows", "len", "numpy.cumsum", "x.reshape", "num_two_factors", "numpy.where", "numpy.max", "int", "numpy.min", "numpy.dtype", "librosa.time_frequency.cqt_frequencies", "fft_basis.dot", "cqt_response", "vqt_resp.append", "min", "numpy.sqrt", "scipy.sparse.lil_matrix", "numpy.log2", "data", "ParameterError", "librosa.filters.constant_q", "range", "librosa.filters.constant_q_lengths", "format", "librosa.get_fftlib", "librosa.note_to_hz", "max"], "release_date": "2019-07", "docs": ["https://librosa.org/doc/main/generated/librosa.vqt.html", "https://librosa.org/doc/main/changelog.html"]}
{"python_version": "3.7", "library": "librosa", "version": "0.8.0", "extra_dependencies": "numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2", "problem": "Complete the function to compute the variable-Q transform of an audio signal.", "starting_code": "import librosa\nimport numpy as np\nimport scipy\nfrom typing import Union, Optional\n\nDTypeLike = Union[np.dtype, type]\n\n\ndef compute_vqt(y: np.ndarray, sr: int) -> np.ndarray:\n    ", "example_id": "310", "test": "\nfilename = librosa.util.example_audio_file()\ny, sr = librosa.load(filename)\n\nsol = compute_vqt(y, sr)\ntest_sol = librosa.vqt(y, sr=sr)\nassert np.allclose(test_sol, sol)", "solution": "\n\n    return librosa.vqt(y, sr=sr)", "type_of_change": "new feature", "name_of_class_or_func": "librosa.vqt", "additional_dependencies": "numpy==1.16.0 scipy==1.1.0 soundfile==0.10.2", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["librosa.vqt"], "release_date": "2020-07", "docs": ["https://librosa.org/doc/main/generated/librosa.vqt.html", "https://librosa.org/doc/main/changelog.html"]}
{"python_version": "3.7", "library": "librosa", "version": "0.7.0", "extra_dependencies": "numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2", "problem": "Complete the function to compute the approximate constant-Q magnitude spectrogram inversion using the fast Griffin-Lim algorithm.", "starting_code": "import librosa\nimport numpy as np\nimport scipy\nfrom typing import Union, Optional\n\nDTypeLike = Union[np.dtype, type]\n\ndef compute_griffinlim_cqt(y: np.ndarray, sr: int, C, n_iter: int, hop_length: int, fmin: int, bins_per_octave: int, tuning: float, filter_scale: 1, norm: int, sparsity: float, window: str, scale: bool, pad_mode: str, res_type: str, dtype: DTypeLike, length: Optional[int], momentum: float, init: Optional[str]) -> np.ndarray:\n    rng = np.random.RandomState(seed=0)", "example_id": "311", "test": "\nfilename = librosa.util.example_audio_file()\ny, sr = librosa.load(filename)\ny=y[:10000]\n\nC = np.abs(librosa.cqt(y=y, sr=sr, bins_per_octave=36, n_bins=7*36))\nn_iter=32\nhop_length=512\nfmin=None\nbins_per_octave=36\ntuning=0.0\nfilter_scale=1\nnorm=1\nsparsity=0.01\nwindow=\"hann\"\nscale=True\npad_mode=\"reflect\"\nres_type=\"kaiser_fast\"\ndtype=None\nlength=None\nmomentum=0.99\ninit=None\nrng = np.random.RandomState(seed=0)\nsol = compute_griffinlim_cqt(y, sr, C, n_iter, hop_length, fmin, bins_per_octave, tuning, filter_scale, norm, sparsity, window, scale, pad_mode, res_type, dtype, length, momentum, init)\n\nif fmin is None:\n    fmin = librosa.note_to_hz(\"C1\")\n\nangles = np.empty(C.shape, dtype=np.complex64)\nif init == \"random\":\n    angles[:] = np.exp(2j * np.pi * rng.rand(*C.shape))\nelif init is None:\n    angles[:] = 1.0\nrebuilt = 0.0\n\nfor _ in range(n_iter):\n    tprev = rebuilt\n\n    inverse = librosa.constantq.icqt(\n    C * angles,\n    sr=sr,\n    hop_length=hop_length,\n    bins_per_octave=bins_per_octave,\n    fmin=fmin,\n    tuning=tuning,\n    filter_scale=filter_scale,\n    window=window,\n    length=length,\n    res_type=res_type,\n    )\n\n    rebuilt = librosa.constantq.cqt(\n    inverse,\n    sr=sr,\n    bins_per_octave=bins_per_octave,\n    n_bins=C.shape[0],\n    hop_length=hop_length,\n    fmin=fmin,\n    tuning=tuning,\n    filter_scale=filter_scale,\n    window=window,\n    res_type=res_type,\n    )\n    angles[:] = rebuilt - (momentum / (1 + momentum)) * tprev\n    angles[:] /= np.abs(angles) + 1e-16\n\ntest_sol = librosa.constantq.icqt(\n C * angles,\n sr=sr,\n hop_length=hop_length,\n bins_per_octave=bins_per_octave,\n tuning=tuning,\n filter_scale=filter_scale,\n fmin=fmin,\n window=window,\n length=length,\n res_type=res_type,\n)\nassert np.allclose(test_sol, sol)", "solution": "\n\n    if fmin is None:\n        fmin = librosa.note_to_hz(\"C1\")\n    \n    angles = np.empty(C.shape, dtype=np.complex64)\n    if init == \"random\":\n        angles[:] = np.exp(2j * np.pi * rng.rand(*C.shape))\n    elif init is None:\n        angles[:] = 1.0\n    \n    rebuilt = 0.0\n    \n    for _ in range(n_iter):\n        tprev = rebuilt\n    \n        inverse = librosa.constantq.icqt(\n        C * angles,\n        sr=sr,\n        hop_length=hop_length,\n        bins_per_octave=bins_per_octave,\n        fmin=fmin,\n        tuning=tuning,\n        filter_scale=filter_scale,\n        window=window,\n        length=length,\n        res_type=res_type,\n        )\n    \n        rebuilt = librosa.constantq.cqt(\n        inverse,\n        sr=sr,\n        bins_per_octave=bins_per_octave,\n        n_bins=C.shape[0],\n        hop_length=hop_length,\n        fmin=fmin,\n        tuning=tuning,\n        filter_scale=filter_scale,\n        window=window,\n        res_type=res_type,\n        )\n    \n        angles[:] = rebuilt - (momentum / (1 + momentum)) * tprev\n        angles[:] /= np.abs(angles) + 1e-16\n    \n    return  librosa.constantq.icqt(\n     C * angles,\n     sr=sr,\n     hop_length=hop_length,\n     bins_per_octave=bins_per_octave,\n     tuning=tuning,\n     filter_scale=filter_scale,\n     fmin=fmin,\n     window=window,\n     length=length,\n     res_type=res_type,\n    )\n    ", "type_of_change": "new feature", "name_of_class_or_func": "librosa.griffinlim_cqt", "additional_dependencies": "numpy==1.16.0 scipy==1.1.0 soundfile==0.10.2", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["librosa.constantq.icqt", "rng.rand", "numpy.abs", "numpy.empty", "librosa.constantq.cqt", "range", "numpy.exp", "librosa.note_to_hz"], "release_date": "2019-07", "docs": ["https://librosa.org/doc/main/generated/librosa.griffinlim_cqt.html", "https://librosa.org/doc/main/changelog.html"]}
{"python_version": "3.7", "library": "librosa", "version": "0.8.0", "extra_dependencies": "numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2", "problem": "Complete the function to compute the approximate constant-Q magnitude spectrogram inversion using the fast Griffin-Lim algorithm.", "starting_code": "import librosa\nimport numpy as np\nimport scipy\nfrom typing import Union, Optional\n\nDTypeLike = Union[np.dtype, type]\n\ndef compute_griffinlim_cqt(y: np.ndarray, sr: int, C, n_iter: int, hop_length: int, fmin: int, bins_per_octave: int, tuning: float, filter_scale: 1, norm: int, sparsity: float, window: str, scale: bool, pad_mode: str, res_type: str, dtype: DTypeLike, length: Optional[int], momentum: float, init: Optional[str]) -> np.ndarray:\n    rng = np.random.RandomState(seed=0)", "example_id": "312", "test": "\nfilename = librosa.util.example_audio_file()\ny, sr = librosa.load(filename)\ny=y[:10000]\n\nC = np.abs(librosa.cqt(y=y, sr=sr, bins_per_octave=36, n_bins=7*36))\nn_iter=32\nhop_length=512\nfmin=None\nbins_per_octave=36\ntuning=0.0\nfilter_scale=1\nnorm=1\nsparsity=0.01\nwindow=\"hann\"\nscale=True\npad_mode=\"reflect\"\nres_type=\"kaiser_fast\"\ndtype=None\nlength=None\nmomentum=0.99\ninit=None\nrng = np.random.RandomState(seed=0)\nsol = compute_griffinlim_cqt(y, sr, C, n_iter, hop_length, fmin, bins_per_octave, tuning, filter_scale, norm, sparsity, window, scale, pad_mode, res_type, dtype, length, momentum, init)\n\nif fmin is None:\n    fmin = librosa.note_to_hz(\"C1\")\n\nangles = np.empty(C.shape, dtype=np.complex64)\nif init == \"random\":\n    angles[:] = np.exp(2j * np.pi * rng.rand(*C.shape))\nelif init is None:\n    angles[:] = 1.0\nrebuilt = 0.0\n\nfor _ in range(n_iter):\n    tprev = rebuilt\n\n    inverse = librosa.constantq.icqt(\n    C * angles,\n    sr=sr,\n    hop_length=hop_length,\n    bins_per_octave=bins_per_octave,\n    fmin=fmin,\n    tuning=tuning,\n    filter_scale=filter_scale,\n    window=window,\n    length=length,\n    res_type=res_type,\n    )\n\n    rebuilt = librosa.constantq.cqt(\n    inverse,\n    sr=sr,\n    bins_per_octave=bins_per_octave,\n    n_bins=C.shape[0],\n    hop_length=hop_length,\n    fmin=fmin,\n    tuning=tuning,\n    filter_scale=filter_scale,\n    window=window,\n    res_type=res_type,\n    )\n    angles[:] = rebuilt - (momentum / (1 + momentum)) * tprev\n    angles[:] /= np.abs(angles) + 1e-16\n\ntest_sol = librosa.constantq.icqt(\n C * angles,\n sr=sr,\n hop_length=hop_length,\n bins_per_octave=bins_per_octave,\n tuning=tuning,\n filter_scale=filter_scale,\n fmin=fmin,\n window=window,\n length=length,\n res_type=res_type,\n)\nassert np.allclose(test_sol, sol)", "solution": "\n\n    return librosa.griffinlim_cqt(C, sr=sr, bins_per_octave=bins_per_octave, init=init)", "type_of_change": "new feature", "name_of_class_or_func": "librosa.griffinlim_cqt", "additional_dependencies": "numpy==1.16.0 scipy==1.1.0 soundfile==0.10.2", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["librosa.griffinlim_cqt"], "release_date": "2020-07", "docs": ["https://librosa.org/doc/main/generated/librosa.griffinlim_cqt.html", "https://librosa.org/doc/main/changelog.html"]}
{"python_version": "3.7", "library": "librosa", "version": "0.6.0", "extra_dependencies": "numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2", "problem": "Complete the function to invert a mel power spectrogram to audio using Griffin-Lim.", "starting_code": "import librosa\nimport numpy as np\nimport scipy\nimport scipy.optimize\nfrom typing import Union, Optional\n\nDTypeLike = Union[np.dtype, type]\n\n\ndef compute_mel_to_audio(y: np.ndarray, sr: int, S: np.ndarray, M: np.ndarray, n_fft: int, hop_length: Optional[int], win_length: Optional[int], window: str, center: bool, pad_mode: str, power: float, n_iter: int, length: Optional[int], dtype: DTypeLike) -> np.ndarray:\n    np.random.seed(seed=0)", "example_id": "313", "test": "filename = librosa.util.example_audio_file()\ny, sr = librosa.load(filename)\ny=y[:10000]\n\nS = np.abs(librosa.stft(y))**2\nM = librosa.feature.melspectrogram(y=y, sr=sr, S=S)\nn_fft=2048\nhop_length=512\nwin_length=None\nwindow='hann'\ncenter=True\npad_mode='reflect'\npower=2.0\nn_iter=32\nlength=None\ndtype=np.float32\nnp.random.seed(seed=0)\nsol =  compute_mel_to_audio(y, sr, S, M, n_fft, hop_length, win_length, window, center, pad_mode, power, n_iter, length, dtype)\nnp.random.seed(seed=0)\n\ndef _nnls_obj(x, shape, A, B):\n    x = x.reshape(shape)\n\n    diff = np.dot(A, x) - B\n\n    value = 0.5 * np.sum(diff**2)\n\n    grad = np.dot(A.T, diff)\n\n    return value, grad.flatten()\n\n\ndef _nnls_lbfgs_block(A, B, x_init=None, **kwargs):\n    if x_init is None:\n        x_init = np.linalg.lstsq(A, B, rcond=None)[0]\n        np.clip(x_init, 0, None, out=x_init)\n\n    kwargs.setdefault('m', A.shape[1])\n\n    bounds = [(0, None)] * x_init.size\n    shape = x_init.shape\n\n    x, obj_value, diagnostics = scipy.optimize.fmin_l_bfgs_b(_nnls_obj, x_init,\n                                                             args=(shape, A, B),\n                                                             bounds=bounds,\n                                                             **kwargs)\n    return x.reshape(shape)\n\n\ndef nnls(A, B, **kwargs):\n    if B.ndim == 1:\n        return scipy.optimize.nnls(A, B)[0]\n\n    n_columns = int((2**8 * 2**10)// (A.shape[-1] * A.itemsize))\n\n    if B.shape[-1] <= n_columns:\n        return _nnls_lbfgs_block(A, B, **kwargs).astype(A.dtype)\n\n    x = np.linalg.lstsq(A, B, rcond=None)[0].astype(A.dtype)\n    np.clip(x, 0, None, out=x)\n    x_init = x\n\n    for bl_s in range(0, x.shape[-1], n_columns):\n        bl_t = min(bl_s + n_columns, B.shape[-1])\n        x[:, bl_s:bl_t] = _nnls_lbfgs_block(A, B[:, bl_s:bl_t],\n                                            x_init=x_init[:, bl_s:bl_t],\n                                            **kwargs)\n    return x\n\nrng = np.random.seed(seed=0)\ndef mel_to_stft(M, sr=22050, n_fft=2048, power=2.0, **kwargs):\n    mel_basis = librosa.filters.mel(sr, n_fft, n_mels=M.shape[0],\n                            **kwargs)\n    inverse = nnls(mel_basis, M)\n    return np.power(inverse, 1./power, out=inverse)\n\n\nstft = mel_to_stft(M, sr=sr, n_fft=n_fft, power=power)\ndef griffinlim(S, n_iter=32, hop_length=None, win_length=None, window='hann', \n               center=True, dtype=np.float32, length=None, pad_mode='reflect',\n               momentum=0.99, random_state=None):\n    rng = np.random\n    n_fft = 2 * (S.shape[0] - 1)\n\n    angles = np.exp(2j * np.pi * rng.rand(*S.shape))\n\n    rebuilt = 0.\n\n    for _ in range(n_iter):\n        tprev = rebuilt\n        inverse = librosa.istft(S * angles, hop_length=hop_length, win_length=win_length,\n                         window=window, center=center, dtype=dtype, length=length)\n        rebuilt = librosa.stft(inverse, n_fft=n_fft, hop_length=hop_length,win_length=win_length, window=window, center=center, pad_mode=pad_mode)\n\n        angles[:] = rebuilt - (momentum / (1 + momentum)) * tprev\n        angles[:] /= np.abs(angles) + 1e-16\n    return librosa.istft(S * angles, hop_length=hop_length, win_length=win_length,\n                 window=window, center=center, dtype=dtype, length=length)\ntest_sol = griffinlim(stft, n_iter=n_iter, hop_length=hop_length, win_length=win_length,\n                      window=window, center=center, dtype=dtype, length=length,\n                      pad_mode=pad_mode)\nassert np.allclose(test_sol, sol)", "solution": "\n\n    def _nnls_obj(x, shape, A, B):\n        x = x.reshape(shape)\n\n        diff = np.dot(A, x) - B\n\n        value = 0.5 * np.sum(diff**2)\n\n        grad = np.dot(A.T, diff)\n\n        return value, grad.flatten()\n\n\n    def _nnls_lbfgs_block(A, B, x_init=None, **kwargs):\n        if x_init is None:\n            x_init = np.linalg.lstsq(A, B, rcond=None)[0]\n            np.clip(x_init, 0, None, out=x_init)\n\n        kwargs.setdefault('m', A.shape[1])\n\n        bounds = [(0, None)] * x_init.size\n        shape = x_init.shape\n\n        x, obj_value, diagnostics = scipy.optimize.fmin_l_bfgs_b(_nnls_obj, x_init,\n                                                                 args=(shape, A, B),\n                                                                 bounds=bounds,\n                                                                 **kwargs)\n        return x.reshape(shape)\n\n\n    def nnls(A, B, **kwargs):\n        if B.ndim == 1:\n            return scipy.optimize.nnls(A, B)[0]\n\n        n_columns = int((2**8 * 2**10)// (A.shape[-1] * A.itemsize))\n\n        if B.shape[-1] <= n_columns:\n            return _nnls_lbfgs_block(A, B, **kwargs).astype(A.dtype)\n\n        x = np.linalg.lstsq(A, B, rcond=None)[0].astype(A.dtype)\n        np.clip(x, 0, None, out=x)\n        x_init = x\n\n        for bl_s in range(0, x.shape[-1], n_columns):\n            bl_t = min(bl_s + n_columns, B.shape[-1])\n            x[:, bl_s:bl_t] = _nnls_lbfgs_block(A, B[:, bl_s:bl_t],\n                                                x_init=x_init[:, bl_s:bl_t],\n                                                **kwargs)\n        return x\n\n    rng = np.random.seed(seed=0)\n    def mel_to_stft(M, sr=22050, n_fft=2048, power=2.0, **kwargs):\n        mel_basis = librosa.filters.mel(sr, n_fft, n_mels=M.shape[0],\n                                **kwargs)\n\n        inverse = nnls(mel_basis, M)\n        return np.power(inverse, 1./power, out=inverse)\n\n\n    stft = mel_to_stft(M, sr=sr, n_fft=n_fft, power=power)\n    def griffinlim(S, n_iter=32, hop_length=None, win_length=None, window='hann', \n                   center=True, dtype=np.float32, length=None, pad_mode='reflect',\n                   momentum=0.99, random_state=None):\n        rng = np.random\n        n_fft = 2 * (S.shape[0] - 1)\n\n        angles = np.exp(2j * np.pi * rng.rand(*S.shape))\n\n        rebuilt = 0.\n\n        for _ in range(n_iter):\n            tprev = rebuilt\n            inverse = librosa.istft(S * angles, hop_length=hop_length, win_length=win_length,\n                             window=window, center=center, dtype=dtype, length=length)\n            rebuilt = librosa.stft(inverse, n_fft=n_fft, hop_length=hop_length,win_length=win_length, window=window, center=center, pad_mode=pad_mode)\n\n            angles[:] = rebuilt - (momentum / (1 + momentum)) * tprev\n            angles[:] /= np.abs(angles) + 1e-16\n        return librosa.istft(S * angles, hop_length=hop_length, win_length=win_length,\n                     window=window, center=center, dtype=dtype, length=length)\n    return griffinlim(stft, n_iter=n_iter, hop_length=hop_length, win_length=win_length,\n                          window=window, center=center, dtype=dtype, length=length,\n                          pad_mode=pad_mode)", "type_of_change": "new feature", "name_of_class_or_func": "librosa.feature.inverse.mel_to_audio", "additional_dependencies": "numpy==1.16.0 scipy==1.1.0 soundfile==0.10.2", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["librosa.filters.mel", "numpy.linalg.lstsq", "numpy.sum", "grad.flatten", "scipy.optimize.optimize.nnls", "numpy.dot", "mel_to_stft", "numpy.power", "numpy.exp", "numpy.abs", "numpy.clip", "numpy.random.seed", "astype", "librosa.istft", "librosa.stft", "_nnls_lbfgs_block", "scipy.optimize.optimize.fmin_l_bfgs_b", "x.reshape", "nnls", "int", "rng.rand", "_nnls_obj", "min", "kwargs.setdefault", "range", "griffinlim"], "release_date": "2018-02", "docs": ["https://librosa.org/doc/main/generated/librosa.feature.inverse.mel_to_audio.html", "https://librosa.org/doc/main/changelog.html"]}
{"python_version": "3.7", "library": "librosa", "version": "0.7.0", "extra_dependencies": "numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2", "problem": "Complete the function to invert a mel power spectrogram to audio using Griffin-Lim.", "starting_code": "import librosa\nimport numpy as np\nimport scipy\nimport scipy.optimize\nfrom typing import Union, Optional\n\nDTypeLike = Union[np.dtype, type]\n\n\ndef compute_mel_to_audio(y: np.ndarray, sr: int, S: np.ndarray, M: np.ndarray, n_fft: int, hop_length: Optional[int], win_length: Optional[int], window: str, center: bool, pad_mode: str, power: float, n_iter: int, length: Optional[int], dtype: DTypeLike) -> np.ndarray:\n    np.random.seed(seed=0)", "example_id": "314", "test": "filename = librosa.util.example_audio_file()\ny, sr = librosa.load(filename)\ny=y[:10000]\n\nS = np.abs(librosa.stft(y))**2\nM = librosa.feature.melspectrogram(y=y, sr=sr, S=S)\nn_fft=2048\nhop_length=512\nwin_length=None\nwindow='hann'\ncenter=True\npad_mode='reflect'\npower=2.0\nn_iter=32\nlength=None\ndtype=np.float32\nnp.random.seed(seed=0)\nsol =  compute_mel_to_audio(y, sr, S, M, n_fft, hop_length, win_length, window, center, pad_mode, power, n_iter, length, dtype)\nnp.random.seed(seed=0)\n\ndef _nnls_obj(x, shape, A, B):\n    x = x.reshape(shape)\n\n    diff = np.dot(A, x) - B\n\n    value = 0.5 * np.sum(diff**2)\n\n    grad = np.dot(A.T, diff)\n\n    return value, grad.flatten()\n\n\ndef _nnls_lbfgs_block(A, B, x_init=None, **kwargs):\n    if x_init is None:\n        x_init = np.linalg.lstsq(A, B, rcond=None)[0]\n        np.clip(x_init, 0, None, out=x_init)\n\n    kwargs.setdefault('m', A.shape[1])\n\n    bounds = [(0, None)] * x_init.size\n    shape = x_init.shape\n\n    x, obj_value, diagnostics = scipy.optimize.fmin_l_bfgs_b(_nnls_obj, x_init,\n                                                             args=(shape, A, B),\n                                                             bounds=bounds,\n                                                             **kwargs)\n    return x.reshape(shape)\n\n\ndef nnls(A, B, **kwargs):\n    if B.ndim == 1:\n        return scipy.optimize.nnls(A, B)[0]\n\n    n_columns = int((2**8 * 2**10)// (A.shape[-1] * A.itemsize))\n\n    if B.shape[-1] <= n_columns:\n        return _nnls_lbfgs_block(A, B, **kwargs).astype(A.dtype)\n\n    x = np.linalg.lstsq(A, B, rcond=None)[0].astype(A.dtype)\n    np.clip(x, 0, None, out=x)\n    x_init = x\n\n    for bl_s in range(0, x.shape[-1], n_columns):\n        bl_t = min(bl_s + n_columns, B.shape[-1])\n        x[:, bl_s:bl_t] = _nnls_lbfgs_block(A, B[:, bl_s:bl_t],\n                                            x_init=x_init[:, bl_s:bl_t],\n                                            **kwargs)\n    return x\n\nrng = np.random.seed(seed=0)\ndef mel_to_stft(M, sr=22050, n_fft=2048, power=2.0, **kwargs):\n    mel_basis = librosa.filters.mel(sr, n_fft, n_mels=M.shape[0],\n                            **kwargs)\n    inverse = nnls(mel_basis, M)\n    return np.power(inverse, 1./power, out=inverse)\n\n\nstft = mel_to_stft(M, sr=sr, n_fft=n_fft, power=power)\ndef griffinlim(S, n_iter=32, hop_length=None, win_length=None, window='hann', \n               center=True, dtype=np.float32, length=None, pad_mode='reflect',\n               momentum=0.99, random_state=None):\n    rng = np.random\n    n_fft = 2 * (S.shape[0] - 1)\n\n    angles = np.exp(2j * np.pi * rng.rand(*S.shape))\n\n    rebuilt = 0.\n\n    for _ in range(n_iter):\n        tprev = rebuilt\n        inverse = librosa.istft(S * angles, hop_length=hop_length, win_length=win_length,\n                         window=window, center=center, dtype=dtype, length=length)\n        rebuilt = librosa.stft(inverse, n_fft=n_fft, hop_length=hop_length,win_length=win_length, window=window, center=center, pad_mode=pad_mode)\n\n        angles[:] = rebuilt - (momentum / (1 + momentum)) * tprev\n        angles[:] /= np.abs(angles) + 1e-16\n    return librosa.istft(S * angles, hop_length=hop_length, win_length=win_length,\n                 window=window, center=center, dtype=dtype, length=length)\ntest_sol = griffinlim(stft, n_iter=n_iter, hop_length=hop_length, win_length=win_length,\n                      window=window, center=center, dtype=dtype, length=length,\n                      pad_mode=pad_mode)\nassert np.allclose(test_sol, sol)", "solution": "\n\n    return librosa.feature.inverse.mel_to_audio(M)", "type_of_change": "new feature", "name_of_class_or_func": "librosa.feature.inverse.mel_to_audio", "additional_dependencies": "numpy==1.16.0 scipy==1.1.0 soundfile==0.10.2", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["librosa.feature.inverse.mel_to_audio"], "release_date": "2019-07", "docs": ["https://librosa.org/doc/main/generated/librosa.feature.inverse.mel_to_audio.html", "https://librosa.org/doc/main/changelog.html"]}
{"python_version": "3.7", "library": "librosa", "version": "0.6.0", "extra_dependencies": "numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2", "problem": "Complete the function to invert Mel-frequency cepstral coefficients to approximate a Mel power spectrogram.", "starting_code": "import librosa\nimport numpy as np\nimport scipy\n\ndef compute_mfcc_to_mel(mfcc: np.ndarray, n_mels: int=128, dct_type: int=2, norm: str='ortho', ref: float=1.0) -> np.ndarray:\n    \"\"\"\n    Invert Mel-frequency cepstral coefficients to approximate a Mel power spectrogram.\n\n    Parameters:\n        mfcc (np.ndarray): Mel-frequency cepstral coefficients.\n        n_mels (int): Number of Mel bands to generate.\n        dct_type (int): Type of DCT to use.\n        norm (str): Normalization to use.\n        ref: Reference power for (inverse) decibel calculation\n\n    Returns:\n        An approximate Mel power spectrum recovered from mfcc.        \n    \"\"\"\n    np.random.seed(seed=0)", "example_id": "315", "test": "filename = librosa.util.example_audio_file()\ny, sr = librosa.load(filename)\nmfcc = librosa.feature.mfcc(y=y, sr=sr)\n\nsol =  compute_mfcc_to_mel(mfcc)\ndef mfcc_to_mel(mfcc, n_mels=128, dct_type=2, norm='ortho', ref=1.0):\n    logmel = scipy.fftpack.idct(mfcc, axis=0, type=dct_type, norm=norm, n=n_mels)\n    return librosa.db_to_power(logmel, ref=ref)\n\nnp.random.seed(seed=0)\ntest_sol = mfcc_to_mel(mfcc)\nassert np.allclose(test_sol, sol)", "solution": "\n\n    logmel = scipy.fftpack.idct(mfcc, axis=0, type=dct_type, norm=norm, n=n_mels)\n    return librosa.db_to_power(logmel, ref=ref)\n", "type_of_change": "new feature", "name_of_class_or_func": "librosa.feature.inverse.mfcc_to_mel", "additional_dependencies": "numpy==1.16.0 scipy==1.1.0 soundfile==0.10.2", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["scipy.fftpack.idct", "librosa.db_to_power"], "release_date": "2018-02", "docs": ["https://librosa.org/doc/main/generated/librosa.feature.inverse.mfcc_to_mel.html", "https://librosa.org/doc/main/changelog.html"]}
{"python_version": "3.7", "library": "librosa", "version": "0.7.0", "extra_dependencies": "numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2", "problem": "Complete the function to invert Mel-frequency cepstral coefficients to approximate a Mel power spectrogram.", "starting_code": "import librosa\nimport numpy as np\nimport scipy\n\ndef compute_mfcc_to_mel(mfcc: np.ndarray, n_mels: int=128, dct_type: int=2, norm: str='ortho', ref: float=1.0) -> np.ndarray:\n    \"\"\"\n    Invert Mel-frequency cepstral coefficients to approximate a Mel power spectrogram.\n\n    Parameters:\n        mfcc (np.ndarray): Mel-frequency cepstral coefficients.\n        n_mels (int): Number of Mel bands to generate.\n        dct_type (int): Type of DCT to use.\n        norm (str): Normalization to use.\n        ref: Reference power for (inverse) decibel calculation\n\n    Returns:\n        An approximate Mel power spectrum recovered from mfcc.        \n    \"\"\"    \n    np.random.seed(seed=0)", "example_id": "316", "test": "\nfilename = librosa.util.example_audio_file()\ny, sr = librosa.load(filename)\nmfcc = librosa.feature.mfcc(y=y, sr=sr)\n\nsol = compute_mfcc_to_mel(mfcc)\n\nnp.random.seed(seed=0)\ntest_sol = librosa.feature.inverse.mfcc_to_mel(mfcc)\nassert np.allclose(test_sol, sol)", "solution": "\n\n    return librosa.feature.inverse.mfcc_to_mel(mfcc)", "type_of_change": "new feature", "name_of_class_or_func": "librosa.feature.inverse.mfcc_to_mel", "additional_dependencies": "numpy==1.16.0 scipy==1.1.0 soundfile==0.10.2", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["librosa.feature.inverse.mfcc_to_mel"], "release_date": "2019-07", "docs": ["https://librosa.org/doc/main/generated/librosa.feature.inverse.mfcc_to_mel.html", "https://librosa.org/doc/main/changelog.html"]}
{"python_version": "3.7", "library": "pillow", "version": "7.0.0", "extra_dependencies": "numpy==1.16", "problem": "Implement the function to superimpose two images on top of each other using the Overlay algorithm.", "starting_code": "import numpy as np\nfrom PIL import Image, ImageChops\n\n\ndef imaging(img1: Image, img2: Image) -> Image:\n    ", "example_id": "317", "test": "def generate_random_image(width, height):\n    random_data = np.random.randint(0, 256, (height, width, 3), dtype=np.uint8)\n    return Image.fromarray(random_data)\n\ndef create(imIn1, imIn2, mode=None):\n    if imIn1.shape != imIn2.shape:\n        return None\n    return np.empty_like(imIn1, dtype=np.uint8)\n\ndef imaging_overlay(imIn1, imIn2):\n    imOut = create(imIn1, imIn2)\n    if imOut is None:\n        return None\n    \n    ysize, xsize, _ = imOut.shape\n    for y in range(ysize):\n        for x in range(xsize):\n            for c in range(3):  # Loop over RGB channels\n                in1, in2 = int(imIn1[y, x, c]), int(imIn2[y, x, c])\n                if in1 < 128:\n                    imOut[y, x, c] = np.clip((in1 * in2) // 127, 0, 255)\n                else:\n                    imOut[y, x, c] = np.clip(255 - (((255 - in1) * (255 - in2)) // 127), 0, 255)\n    \n    return imOut\n\nwidth, height = 256, 256\nimg1 = generate_random_image(width, height)\nimg2 = generate_random_image(width, height)\ngt = imaging_overlay(np.array(img1), np.array(img2))\nsol = imaging(img1, img2)\nassert np.allclose(np.array(gt), np.array(sol))", "solution": "\n\n    def create(imIn1, imIn2, mode=None):\n        if imIn1.shape != imIn2.shape:\n            return None\n        return np.empty_like(imIn1, dtype=np.uint8)\n\n    def imaging_overlay(imIn1, imIn2):\n        imOut = create(imIn1, imIn2)\n        if imOut is None:\n            return None\n        \n        ysize, xsize, _ = imOut.shape\n        for y in range(ysize):\n            for x in range(xsize):\n                for c in range(3):  # Loop over RGB channels\n                    in1, in2 = int(imIn1[y, x, c]), int(imIn2[y, x, c])\n                    if in1 < 128:\n                        imOut[y, x, c] = np.clip((in1 * in2) // 127, 0, 255)\n                    else:\n                        imOut[y, x, c] = np.clip(255 - (((255 - in1) * (255 - in2)) // 127), 0, 255)\n        \n        return imOut\n\n    return imaging_overlay(np.array(img1), np.array(img2))", "type_of_change": "new feature", "name_of_class_or_func": "PIL.ImageChops.overlay", "additional_dependencies": "numpy==1.16", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["create", "numpy.empty_like", "imaging_overlay", "numpy.clip", "range", "numpy.array", "int"], "release_date": "2020-01", "docs": ["https://pillow.readthedocs.io/en/stable/reference/ImageChops.html#PIL.ImageChops.overlay"]}
{"python_version": "3.7", "library": "pillow", "version": "7.0.0", "extra_dependencies": "numpy==1.16", "problem": "Implement the function to superimpose two images on top of each other using the Soft Light algorithm.", "starting_code": "import numpy as np\nfrom PIL import Image, ImageChops\n\ndef imaging(img1: Image, img2: Image) -> Image:\n    ", "example_id": "318", "test": "\ndef generate_random_image(width, height):\n    random_data = np.random.randint(0, 256, (height, width, 3), dtype=np.uint8)\n    return Image.fromarray(random_data)\n\ndef create(imIn1, imIn2, mode=None):\n    if imIn1.shape != imIn2.shape:\n        return None\n    return np.empty_like(imIn1, dtype=np.uint8)\n\nnp.random.seed(42)\nwidth, height = 8, 8\nimg1 = generate_random_image(width, height)\nimg2 = generate_random_image(width, height)\n\n\ngt = np.array([\n    [[131, 189, 237],\n     [ 88, 204, 105],\n     [222, 168, 112],\n     [  0,  38, 249],\n     [ 19, 153, 241],\n     [ 54,  68, 251],\n     [ 51, 156,  62],\n     [177, 203, 188]],\n\n    [[159, 214,  77],\n     [154,  36,   8],\n     [220, 210,  71],\n     [212, 244,  44],\n     [180,  35,  33],\n     [109, 209,  32],\n     [251,   8, 240],\n     [234, 106,  40]],\n\n    [[ 41,  89, 223],\n     [ 15, 203,  99],\n     [101, 139,  38],\n     [180, 203, 200],\n     [ 87, 251,  87],\n     [ 79, 117, 140],\n     [233,  99,  50],\n     [229, 132,   5]],\n\n    [[152,  29, 109],\n     [227, 109, 115],\n     [124, 217, 115],\n     [  7,  37,   8],\n     [ 14, 254,  73],\n     [ 15,   0,   0],\n     [123, 252, 140],\n     [239, 223, 171]],\n\n    [[217, 238,  47],\n     [198, 138, 252],\n     [ 26, 226,  17],\n     [190,  32,  19],\n     [128,  14,  18],\n     [  0, 173,  34],\n     [ 97,  59, 172],\n     [189,   0, 254]],\n\n    [[ 16, 193, 247],\n     [ 40, 187,  53],\n     [ 33, 179, 225],\n     [238, 212,  35],\n     [ 71, 102, 105],\n     [168,  38, 162],\n     [115, 132, 250],\n     [ 13, 191,   0]],\n\n    [[241, 246, 183],\n     [ 50,  45, 121],\n     [226,   0,  71],\n     [254, 176, 116],\n     [205, 166,  42],\n     [119,  69,  87],\n     [131, 126, 254],\n     [ 23, 110,  62]],\n\n    [[157, 196, 191],\n     [ 43, 255,  56],\n     [238,  15, 188],\n     [252, 226,  54],\n     [236,   0, 107],\n     [248, 209, 210],\n     [169,  94,   5],\n     [237, 116,  32]]\n])\nsol = imaging(img1, img2)\nassert np.allclose(np.array(gt), np.array(sol))", "solution": "\n\n    def create(imIn1, imIn2, mode=None):\n        if imIn1.shape != imIn2.shape:\n            return None\n        return np.empty_like(imIn1, dtype=np.uint8)\n\n    def imaging_softlight(imIn1, imIn2):\n        if imIn1.shape != imIn2.shape:\n            return None\n        \n        imOut = create(imIn1, imIn2)\n        ysize, xsize, _ = imOut.shape\n        for y in range(ysize):\n            for x in range(xsize):\n                for c in range(3):  # Loop over RGB channels\n                    in1, in2 = int(imIn1[y, x, c]), int(imIn2[y, x, c])\n                    imOut[y, x, c] = int((((255 - in1) * (in1 * in2)) // 65536) +  (in1 * (255 - ((255 - in1) * (255 - in2) // 255))) // 255)\n        return imOut\n    return imaging_softlight(np.array(img1), np.array(img2))", "type_of_change": "new feature", "name_of_class_or_func": "PIL.ImageChops.soft_light", "additional_dependencies": "numpy==1.16", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["create", "imaging_softlight", "numpy.empty_like", "range", "numpy.array", "int"], "release_date": "2020-01", "docs": ["https://pillow.readthedocs.io/en/stable/reference/ImageChops.html#PIL.ImageChops.soft_light"]}
{"python_version": "3.7", "library": "pillow", "version": "7.0.0", "extra_dependencies": "numpy==1.16", "problem": "Implement the function to superimpose two images on top of each other using the Hard Light algorithm. ", "starting_code": "import numpy as np\nfrom PIL import Image, ImageChops\n\n\ndef imaging(img1: Image, img2: Image) -> Image:\n    ", "example_id": "319", "test": "\ndef generate_random_image(width, height):\n    random_data = np.random.randint(0, 256, (height, width, 3), dtype=np.uint8)\n    return Image.fromarray(random_data)\n\nnp.random.seed(42)\n\nwidth, height = 8, 8\nimg1 = generate_random_image(width, height)\nimg2 = generate_random_image(width, height)\n\n\n\ndef create(imIn1, imIn2, mode=None):\n    if imIn1.shape != imIn2.shape:\n        return None\n    return np.empty_like(imIn1, dtype=np.uint8)\n\ndef imaging_hardlight(imIn1, imIn2):\n    imOut = create(imIn1, imIn2)\n    if imOut is None:\n        return None\n    \n    ysize, xsize, _ = imOut.shape\n    for y in range(ysize):\n        for x in range(xsize):\n            for c in range(3):  # Loop over RGB channels\n                in1, in2 = int(imIn2[y, x, c]), int(imIn1[y, x, c])\n                if in1 < 128:\n                    imOut[y, x, c] = np.clip((in1 * in2) // 127, 0, 255)\n                else:\n                    imOut[y, x, c] = np.clip(255 - (((255 - in1) * (255 - in2)) // 127), 0, 255)\n    \n    return imOut\n\ngt = np.array([\n    [[176,   0, 241],\n     [ 86, 216, 249],\n     [ 90,  36, 152],\n     [  1,   0, 250],\n     [130, 159, 150],\n     [ 96,  41, 252],\n     [ 22,  75,  59],\n     [ 11, 238, 151]],\n\n    [[ 79, 234, 119],\n     [156, 233,   1],\n     [225, 252,  50],\n     [207, 245,  79],\n     [248, 150,  33],\n     [106, 211,  22],\n     [252,   3, 247],\n     [ 24,  43, 192]],\n\n    [[ 29,  17, 225],\n     [ 66, 205, 131],\n     [ 45, 167,  13],\n     [173, 210,  74],\n     [ 65, 253,  71],\n     [ 76, 146,  75],\n     [235,  19,  13],\n     [157, 116,   0]],\n\n    [[154,  20, 225],\n     [248,  90,  42],\n     [ 65, 246,  69],\n     [ 87,  27, 240],\n     [  5, 254,  44],\n     [ 45, 227,   0],\n     [239, 253, 248],\n     [240, 227, 141]],\n\n    [[ 14,  82, 248],\n     [129, 110, 185],\n     [ 11,   7,  15],\n     [ 17,   0,  10],\n     [130, 186,  10],\n     [  0, 124, 207],\n     [ 45, 124, 133],\n     [222, 103, 122]],\n\n    [[ 17, 222, 248],\n     [ 40, 205,  48],\n     [216, 194, 232],\n     [246, 108,  18],\n     [125, 163,  46],\n     [178, 250, 231],\n     [176, 132,  13],\n     [ 12, 123, 171]],\n\n    [[243, 247, 213],\n     [  6,  23, 145],\n     [176,   1, 174],\n     [112, 207, 223],\n     [137, 170,  19],\n     [105, 113,  81],\n     [ 80,  81,  98],\n     [  3, 143,  36]],\n\n    [[137,  26, 197],\n     [107, 255,  51],\n     [181, 136,   1],\n     [151, 234,  46],\n     [254,   0, 198],\n     [211, 209,  80],\n     [ 55, 100,   5],\n     [239,  69,  24]]\n])\nsol = imaging(img1, img2)\nassert np.allclose(np.array(gt), np.array(sol))", "solution": "\n\n    def create(imIn1, imIn2, mode=None):\n        if imIn1.shape != imIn2.shape:\n            return None\n        return np.empty_like(imIn1, dtype=np.uint8)\n\n    def imaging_hardlight(imIn1, imIn2):\n        imOut = create(imIn1, imIn2)\n        if imOut is None:\n            return None\n        \n        ysize, xsize, _ = imOut.shape\n        for y in range(ysize):\n            for x in range(xsize):\n                for c in range(3):  # Loop over RGB channels\n                    in1, in2 = int(imIn2[y, x, c]), int(imIn1[y, x, c])\n                    if in1 < 128:\n                        imOut[y, x, c] = np.clip((in1 * in2) // 127, 0, 255)\n                    else:\n                        imOut[y, x, c] = np.clip(255 - (((255 - in1) * (255 - in2)) // 127), 0, 255)\n        \n        return imOut\n\n    return imaging_hardlight(np.array(img1), np.array(img2))", "type_of_change": "new feature", "name_of_class_or_func": "PIL.ImageChops.hard_light", "additional_dependencies": "numpy==1.16", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["create", "numpy.empty_like", "numpy.clip", "range", "imaging_hardlight", "numpy.array", "int"], "release_date": "2020-01", "docs": ["https://pillow.readthedocs.io/en/stable/reference/ImageChops.html#PIL.ImageChops.hard_light"]}
{"python_version": "3.7", "library": "pillow", "version": "7.1.0", "extra_dependencies": "numpy==1.16", "problem": "Implement the function to superimpose two images on top of each other using the Overlay algorithm.", "starting_code": "import numpy as np\nfrom PIL import Image, ImageChops\n\n\ndef imaging(img1: Image, img2: Image) -> Image:\n    ", "example_id": "320", "test": "\nimport numpy as np\nfrom PIL import Image, ImageChops\n\ndef generate_random_image(width, height):\n random_data = np.random.randint(0, 256, (height, width, 3), dtype=np.uint8)\n return Image.fromarray(random_data)\n\nnp.random.seed(42)\nwidth, height = 8, 8\nimg1 = generate_random_image(width, height)\nimg2 = generate_random_image(width, height)\n\ngt = np.array([\n    [[151, 185, 241],\n     [ 86, 216, 120],\n     [221, 161, 126],\n     [  1,   0, 250],\n     [ 20, 159, 242],\n     [ 57,  41, 252],\n     [ 22, 146,  59],\n     [172, 238, 186]],\n\n    [[149, 234,  83],\n     [156,  38,   1],\n     [225, 252,  50],\n     [213, 245,  46],\n     [236,  37,  33],\n     [106, 211,  22],\n     [252,   3, 247],\n     [235,  77,  43]],\n\n    [[ 29,  48, 225],\n     [ 17, 205, 108],\n     [ 69, 160,  13],\n     [180, 210, 199],\n     [ 65, 253,  71],\n     [ 76, 129, 125],\n     [235,  64,  13],\n     [229, 123,   0]],\n\n    [[154,  20, 125],\n     [248,  91,  90],\n     [104, 246,  92],\n     [  7,  27,   9],\n     [  5, 254,  44],\n     [ 15,   1,   0],\n     [145, 253, 170],\n     [240, 227, 168]],\n\n    [[217, 239,  51],\n     [197, 127, 253],\n     [ 11, 226,  15],\n     [186,   0,  10],\n     [130,  15,  10],\n     [  0, 169,  36],\n     [ 62,  63, 168],\n     [222,   1, 254]],\n\n    [[ 17, 222, 248],\n     [ 40, 205,  48],\n     [ 35, 194, 232],\n     [246, 211,  18],\n     [ 76, 114,  76],\n     [178,  41, 205],\n     [132, 133, 250],\n     [ 12, 188,   1]],\n\n    [[243, 247, 213],\n     [  6,  23, 134],\n     [226,   1,  77],\n     [254, 207, 135],\n     [205, 170,  19],\n     [108,  74,  81],\n     [115, 108, 254],\n     [  3, 122,  36]],\n\n    [[153, 193, 197],\n     [ 46, 255,  51],\n     [239,  16, 183],\n     [253, 234,  46],\n     [254,   0, 121],\n     [249, 209, 209],\n     [162,  97,   5],\n     [239,  93,  24]]\n])\n\nsol = imaging(img1, img2)\nassert np.allclose(np.array(gt), np.array(sol))", "solution": "\n    return ImageChops.overlay(img1, img2)", "type_of_change": "new feature", "name_of_class_or_func": "PIL.ImageChops.overlay", "additional_dependencies": "numpy==1.16", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["ImageChops.overlay"], "release_date": "2020-04", "docs": ["https://pillow.readthedocs.io/en/stable/reference/ImageChops.html#PIL.ImageChops.overlay"]}
{"python_version": "3.7", "library": "pillow", "version": "7.1.0", "extra_dependencies": "numpy==1.16", "problem": "Implement a function to superimpose two images on top of each other using the Soft Light algorithm.", "starting_code": "import numpy as np\nfrom PIL import Image, ImageChops\n\ndef imaging(img1: Image, img2: Image) -> Image:", "example_id": "321", "test": "\ndef generate_random_image(width, height):\n random_data = np.random.randint(0, 256, (height, width, 3), dtype=np.uint8)\n return Image.fromarray(random_data)\n\nnp.random.seed(42)\n\nwidth, height = 8, 8\nimg1 = generate_random_image(width, height)\nimg2 = generate_random_image(width, height)\n\ngt = np.array([\n    [[131, 189, 237],\n     [ 88, 204, 105],\n     [222, 168, 112],\n     [  0,  38, 249],\n     [ 19, 153, 241],\n     [ 54,  68, 251],\n     [ 51, 156,  62],\n     [177, 203, 188]],\n\n    [[159, 214,  77],\n     [154,  36,   8],\n     [220, 210,  71],\n     [212, 244,  44],\n     [180,  35,  33],\n     [109, 209,  32],\n     [251,   8, 240],\n     [234, 106,  40]],\n\n    [[ 41,  89, 223],\n     [ 15, 203,  99],\n     [101, 139,  38],\n     [180, 203, 200],\n     [ 87, 251,  87],\n     [ 79, 117, 140],\n     [233,  99,  50],\n     [229, 132,   5]],\n\n    [[152,  29, 109],\n     [227, 109, 115],\n     [124, 217, 115],\n     [  7,  37,   8],\n     [ 14, 254,  73],\n     [ 15,   0,   0],\n     [123, 252, 140],\n     [239, 223, 171]],\n\n    [[217, 238,  47],\n     [198, 138, 252],\n     [ 26, 226,  17],\n     [190,  32,  19],\n     [128,  14,  18],\n     [  0, 173,  34],\n     [ 97,  59, 172],\n     [189,   0, 254]],\n\n    [[ 16, 193, 247],\n     [ 40, 187,  53],\n     [ 33, 179, 225],\n     [238, 212,  35],\n     [ 71, 102, 105],\n     [168,  38, 162],\n     [115, 132, 250],\n     [ 13, 191,   0]],\n\n    [[241, 246, 183],\n     [ 50,  45, 121],\n     [226,   0,  71],\n     [254, 176, 116],\n     [205, 166,  42],\n     [119,  69,  87],\n     [131, 126, 254],\n     [ 23, 110,  62]],\n\n    [[157, 196, 191],\n     [ 43, 255,  56],\n     [238,  15, 188],\n     [252, 226,  54],\n     [236,   0, 107],\n     [248, 209, 210],\n     [169,  94,   5],\n     [237, 116,  32]]\n])\nsol = imaging(img1, img2)\nassert np.allclose(np.array(gt), np.array(sol))", "solution": "\n    return ImageChops.soft_light(img1, img2)", "type_of_change": "new feature", "name_of_class_or_func": "PIL.ImageChops.soft_light", "additional_dependencies": "numpy==1.16", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["ImageChops.soft_light"], "release_date": "2020-04", "docs": ["https://pillow.readthedocs.io/en/stable/reference/ImageChops.html#PIL.ImageChops.soft_light"]}
{"python_version": "3.7", "library": "pillow", "version": "7.1.0", "extra_dependencies": "numpy==1.16", "problem": "Implement the function to superimpose two images on top of each other using the Hard Light algorithm.", "starting_code": "import numpy as np\nfrom PIL import Image, ImageChops\n\ndef imaging(img1: Image, img2: Image) -> Image:", "example_id": "322", "test": "\ndef generate_random_image(width, height):\n random_data = np.random.randint(0, 256, (height, width, 3), dtype=np.uint8)\n return Image.fromarray(random_data)\n\nnp.random.seed(42)\n\nwidth, height = 8, 8\nimg1 = generate_random_image(width, height)\nimg2 = generate_random_image(width, height)\n\n\ngt = np.array([\n    [[176,   0, 241],\n     [ 86, 216, 249],\n     [ 90,  36, 152],\n     [  1,   0, 250],\n     [130, 159, 150],\n     [ 96,  41, 252],\n     [ 22,  75,  59],\n     [ 11, 238, 151]],\n\n    [[ 79, 234, 119],\n     [156, 233,   1],\n     [225, 252,  50],\n     [207, 245,  79],\n     [248, 150,  33],\n     [106, 211,  22],\n     [252,   3, 247],\n     [ 24,  43, 192]],\n\n    [[ 29,  17, 225],\n     [ 66, 205, 131],\n     [ 45, 167,  13],\n     [173, 210,  74],\n     [ 65, 253,  71],\n     [ 76, 146,  75],\n     [235,  19,  13],\n     [157, 116,   0]],\n\n    [[154,  20, 225],\n     [248,  90,  42],\n     [ 65, 246,  69],\n     [ 87,  27, 240],\n     [  5, 254,  44],\n     [ 45, 227,   0],\n     [239, 253, 248],\n     [240, 227, 141]],\n\n    [[ 14,  82, 248],\n     [129, 110, 185],\n     [ 11,   7,  15],\n     [ 17,   0,  10],\n     [130, 186,  10],\n     [  0, 124, 207],\n     [ 45, 124, 133],\n     [222, 103, 122]],\n\n    [[ 17, 222, 248],\n     [ 40, 205,  48],\n     [216, 194, 232],\n     [246, 108,  18],\n     [125, 163,  46],\n     [178, 250, 231],\n     [176, 132,  13],\n     [ 12, 123, 171]],\n\n    [[243, 247, 213],\n     [  6,  23, 145],\n     [176,   1, 174],\n     [112, 207, 223],\n     [137, 170,  19],\n     [105, 113,  81],\n     [ 80,  81,  98],\n     [  3, 143,  36]],\n\n    [[137,  26, 197],\n     [107, 255,  51],\n     [181, 136,   1],\n     [151, 234,  46],\n     [254,   0, 198],\n     [211, 209,  80],\n     [ 55, 100,   5],\n     [239,  69,  24]]\n])\nsol = imaging(img1, img2)\nassert np.allclose(np.array(gt), np.array(sol))", "solution": "\n    return ImageChops.hard_light(img1, img2)", "type_of_change": "new feature", "name_of_class_or_func": "PIL.ImageChops.hard_light", "additional_dependencies": "numpy==1.16", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["ImageChops.hard_light"], "release_date": "2020-04", "docs": ["https://pillow.readthedocs.io/en/stable/reference/ImageChops.html#PIL.ImageChops.hard_light"]}
{"python_version": "3.7", "library": "tqdm", "version": "4.28", "problem": "Iterate over an infinite iterable.", "starting_code": "from tqdm import tqdm\n\ndef infinite():\n    i = 0\n    while True:\n        yield i\n        i += 1\n        if i == 1000:\n          return\n\n# Define the total in sol_dict['total'] and use it.\nsol_dict = {\"total\":0}", "example_id": "323", "test": "assertion_value = sol_dict['total'] is None\nassert assertion_value", "solution": "\nsol_dict['total'] = None\nprogress_bar = tqdm(infinite(), total=sol_dict['total'])\nfor progress in progress_bar:\n    progress_bar.set_description(f\"Processing {progress}\")", "type_of_change": "argument change", "name_of_class_or_func": "tqdm", "additional_dependencies": "", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["tqdm.tqdm", "progress_bar.set_description", "infinite"], "release_date": "2018-10", "docs": ["https://tqdm.github.io/docs/tqdm/", "https://tqdm.github.io/releases/"]}
{"python_version": "3.7", "library": "tqdm", "version": "4.29", "problem": "Iterate over an infinite iterable.", "starting_code": "from tqdm import tqdm\n\ndef infinite():\n    i = 0\n    while True:\n        yield i\n        i += 1\n        if i == 1000:\n          return\n\n# Define the total in sol_dict['total'] and use it.\nsol_dict = {\"total\":0}", "example_id": "324", "test": "assertion_value = sol_dict['total'] == float('inf')\nassert assertion_value", "solution": "\nsol_dict['total'] = float('inf')\nprogress_bar = tqdm(infinite(), total=sol_dict['total'])\nfor progress in progress_bar:\n    progress_bar.set_description(f\"Processing {progress}\")", "type_of_change": "argument change", "name_of_class_or_func": "tqdm", "additional_dependencies": "", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["tqdm.tqdm", "infinite", "progress_bar.set_description", "float"], "release_date": "2019-01", "docs": ["https://tqdm.github.io/docs/tqdm/", "https://tqdm.github.io/releases/"]}
{"python_version": "3.7", "library": "kymatio", "version": "0.3.0", "problem": "Implement the function to define and run a 2d scattering transform in Torch. Return a tuple of the Scattering object and the result of Scattering on a.", "starting_code": "import kymatio\nimport torch\nfrom kymatio import Scattering2D\nfrom kymatio.scattering2d.frontend.torch_frontend import ScatteringTorch2D\nfrom typing import Tuple\n\ndef compute_scattering(a: torch.Tensor) -> Tuple[torch.Tensor, ScatteringTorch2D]:\n    ", "example_id": "325", "test": "import kymatio\na = torch.ones((1, 3, 32, 32))\nS, S_a = compute_scattering(a)\nassertion_value = isinstance(S_a, torch.Tensor)\nassert assertion_value\nassertion_value = isinstance(S, kymatio.scattering2d.frontend.torch_frontend.ScatteringTorch2D)\nassert assertion_value", "solution": "\n\n    S = Scattering2D(2, (32, 32), frontend='torch')\n    S_a = S(a)\n    return S, S_a", "type_of_change": "argument change", "name_of_class_or_func": "Scattering2D", "additional_dependencies": "torch==1.4.0", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["kymatio.Scattering2D", "S"], "release_date": "2022-09", "docs": ["https://www.kymat.io/codereference.html?highlight=scattering2d#kymatio.torch.Scattering2D"]}
{"python_version": "3.7", "library": "matplotlib", "version": "3.4.0", "problem": "Implement the function to modify the axis of the figure to not visualize ticks on the x and y axis.", "starting_code": "import matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib.figure import Figure\nfrom matplotlib.axes import Axes\n\ndef modify(fig: Figure, ax: Axes) -> None:\n    ", "example_id": "326", "test": "import numpy as np \n\nfig, ax = plt.subplots()\nmodify(fig, ax)\nassertion_value =  np.array_equal(ax.get_xticks(), np.array([]))\nassert assertion_value\nassertion_value = (ax.get_xticks() == np.array([])).all()\nassert assertion_value\nassertion_value =  np.array_equal(ax.get_xticklabels(), np.array([]))\nassert assertion_value\nassertion_value = (ax.get_xticklabels() == np.array([])).all()\nassert assertion_value", "solution": "\n    ax.set_xticks([], minor=False)\n    ax.set_yticks([], minor=False)", "type_of_change": "argument change", "name_of_class_or_func": "matplotlib.pyplot.axis", "additional_dependencies": "numpy==1.18.1 pyparsing==2.3.1 packaging==19.0", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["ax.set_yticks", "ax.set_xticks"], "release_date": "2021-03", "docs": ["https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.axis.html"]}
{"python_version": "3.7", "library": "matplotlib", "version": "3.2.0", "problem": "Implement the function to modify the axis of the figure to not visualize major and minor ticks on the x and y axis, with no labels.", "starting_code": "import matplotlib.pyplot as plt\nfrom matplotlib.figure import Figure\nfrom matplotlib.axes import Axes\n\ndef modify(fig: Figure, ax: Axes) -> None:\n", "example_id": "327", "test": "import numpy as np \n\nfig, ax = plt.subplots()\nmodify(fig, ax)\n\nassertion_value = np.array_equal(ax.get_xticks(), np.array([]))\n\nassert  assertion_value\nassertion_value = (ax.get_xticks() == np.array([])).all()\nassert assertion_value\nassertion_value = np.array_equal(ax.get_xticklabels(), np.array([]))\nassert assertion_value\nassertion_value = (ax.get_xticklabels() == np.array([])).all()\nassert assertion_value\n", "solution": "\n    ax.set_xticks([], False)\n    ax.set_yticks([], False)", "type_of_change": "argument change", "name_of_class_or_func": "matplotlib.pyplot.axis", "additional_dependencies": "numpy==1.18.1 pyparsing==2.3.1 packaging==19.0", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["ax.set_yticks", "ax.set_xticks"], "release_date": "2020-03", "docs": ["https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.axis.html"]}
{"python_version": "3.7", "library": "matplotlib", "version": "3.5.0", "problem": "Implement the function to modify the axis of the figure to not visualize major and minor ticks on the x and y axis, with no labels.", "starting_code": "import matplotlib.pyplot as plt\nfrom matplotlib.figure import Figure\nfrom matplotlib.axes import Axes\n\ndef modify(fig: Figure, ax: Axes) -> None:\n", "example_id": "328", "test": "import numpy as np \n\nfig, ax = plt.subplots()\nmodify(fig, ax)\nassertion_value = np.array_equal(ax.get_xticks(), np.array([]))\nassert assertion_value\nassertion_value = (ax.get_xticks() == np.array([])).all()\nassert assertion_value\n\nassertion_value = np.array_equal(ax.get_xticklabels(), np.array([]))\nassert assertion_value\n\nassertion_value = (ax.get_xticklabels() == np.array([])).all()\nassert assertion_value", "solution": "\n    ax.set_xticks([], [], minor=False)\n    ax.set_yticks([], [], minor=False)", "type_of_change": "argument change", "name_of_class_or_func": "matplotlib.pyplot.axis", "additional_dependencies": "numpy==1.18.1 pyparsing==2.3.1", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["ax.set_yticks", "ax.set_xticks"], "release_date": "2021-11", "docs": ["https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.axis.html"]}
{"python_version": "3.7", "library": "matplotlib", "version": "3.5.0", "problem": "Implement a function to use Seaborn style.", "starting_code": "import matplotlib.pyplot as plt\n\ndef use_seaborn() -> None:\n    ", "example_id": "329", "test": "use_seaborn()\n\ncycle = plt.rcParams['axes.prop_cycle']\nfrom cycler import cycler\na = cycler('color', ['#4C72B0', '#55A868', '#C44E52', '#8172B2', '#CCB974', '#64B5CD'])\nassert cycle==a", "solution": "\n    plt.style.use(\"seaborn\")", "type_of_change": "argument change", "name_of_class_or_func": "matplotlib.pyplot.style.use", "additional_dependencies": "numpy==1.18.1 pyparsing==2.3.1", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["matplotlib.pyplot.style.use"], "release_date": "2021-11", "docs": ["https://matplotlib.org/stable/api/style_api.html#matplotlib.style.use"]}
{"python_version": "3.10", "library": "matplotlib", "version": "3.8.0", "problem": "Implement a function to use Seaborn style.", "starting_code": "import matplotlib.pyplot as plt\n\ndef use_seaborn() -> None:\n    ", "example_id": "330", "test": "use_seaborn()\n\ncycle = plt.rcParams['axes.prop_cycle']\nfrom cycler import cycler\na = cycler('color', ['#4C72B0', '#55A868', '#C44E52', '#8172B2', '#CCB974', '#64B5CD'])\nassert cycle==a", "solution": "\n    plt.style.use(\"seaborn-v0_8\")\n", "type_of_change": "argument change", "name_of_class_or_func": "matplotlib.pyplot.style.use", "additional_dependencies": "pyparsing==2.3.1", "functional": 1, "webdev": 0, "solution_api_call": true, "api_calls": ["matplotlib.pyplot.style.use"], "release_date": "2023-09", "docs": ["https://matplotlib.org/stable/api/style_api.html", "https://matplotlib.org/stable/users/prev_whats_new/whats_new_3.8.0.html"]}
