{"example_id": "0", "answer": "import torch\ndef log_ndtr(input_tensor: torch.Tensor) -> torch.Tensor:\n    import numpy as np\n    from scipy.stats import norm\n    output = torch.from_numpy(norm.logcdf(input_tensor.numpy()))\n    return output"}
{"example_id": "1", "answer": "import torch\ndef gamma_ln(input_tensor: torch.Tensor) -> torch.Tensor:\n    import numpy as np\n    from scipy.special import gammaln as scipy_gammaln\n    output = torch.from_numpy(scipy_gammaln(input_tensor.numpy()))\n    return output"}
{"example_id": "2", "answer": "import torch\ndef erf(input_tensor: torch.Tensor) -> torch.Tensor:\n    import numpy as np\n    from scipy.special import erf as scipy_erf\n    output = torch.from_numpy(scipy_erf(input_tensor.numpy()))\n    return output"}
{"example_id": "3", "answer": "import torch\ndef erfc(input_tensor: torch.Tensor) -> torch.Tensor:\n    import numpy as np\n    from scipy.special import erfc as scipy_erfc\n    output = torch.from_numpy(scipy_erfc(input_tensor.numpy()))\n    return output"}
{"example_id": "4", "answer": "import torch\ndef bessel_i0(input_tensor: torch.Tensor) -> torch.Tensor:\n    import numpy as np\n    from scipy.special import i0 as scipy_i0\n    output = torch.from_numpy(scipy_i0(input_tensor.numpy()))\n    return output"}
{"example_id": "5", "answer": "import torch\ndef bessel_i1(input_tensor: torch.Tensor) -> torch.Tensor:\n    import numpy as np\n    from scipy.special import i1 as scipy_i1\n    output = torch.from_numpy(scipy_i1(input_tensor.numpy()))\n    return output"}
{"example_id": "6", "answer": "import torch\ndef gamma_ln(input_tensor: torch.Tensor) -> torch.Tensor:\n     return torch.special.gammaln(input_tensor)"}
{"example_id": "7", "answer": "import torch\ndef erf(input_tensor: torch.Tensor) -> torch.Tensor:\n    return torch.special.erf(input_tensor)"}
{"example_id": "8", "answer": "import torch\ndef erfc(input_tensor: torch.Tensor) -> torch.Tensor:\n    return torch.special.erfc(input_tensor)"}
{"example_id": "9", "answer": "import torch\ndef bessel_i0(input_tensor: torch.Tensor) -> torch.Tensor:\n    return torch.special.i0(input_tensor)"}
{"example_id": "10", "answer": "import torch\ndef bessel_i1(input_tensor: torch.Tensor) -> torch.Tensor:\n    return torch.special.i1(input_tensor)"}
{"example_id": "11", "answer": "import torch\ndef invert_mask(tensor1: torch.Tensor, tensor2: torch.Tensor) -> torch.BoolTensor:\n    return ~(tensor1 < tensor2)"}
{"example_id": "12", "answer": "import torch\ndef log_ndtr(input_tensor: torch.Tensor) -> torch.Tensor:\n    return torch.special.log_ndtr(input_tensor)"}
{"example_id": "13", "answer": "import torch\ndef invert_mask(tensor1: torch.Tensor, tensor2: torch.Tensor) -> torch.BoolTensor:\n    return ~(tensor1 < tensor2).bool()"}
{"example_id": "14", "answer": "import torch\ndef stft(audio_signal: torch.Tensor, n_fft: int) -> torch.Tensor:\n    return torch.stft(audio_signal, n_fft=n_fft, return_complex=False)"}
{"example_id": "15", "answer": "import torch\ndef stft(audio_signal: torch.Tensor, n_fft: int) -> torch.Tensor:\n    return torch.view_as_real(torch.stft(audio_signal, n_fft=n_fft, return_complex=True))"}
{"example_id": "16", "answer": "import torch\ndef istft(spectrogram: torch.Tensor, signal: torch.Tensor, n_fft: int, hop_length: int, win_length: int, normalized=False) -> torch.Tensor:\n    return torch.istft(spectrogram, n_fft=n_fft, hop_length=hop_length, win_length=win_length, window=torch.hann_window(win_length), length=signal.shape[0], normalized=False)"}
{"example_id": "17", "answer": "import torch\ndef istft(spectrogram: torch.Tensor, signal: torch.Tensor, n_fft: int, hop_length: int, win_length: int, normalized=False) -> torch.Tensor:\n    return torch.istft(torch.view_as_complex(spectrogram), n_fft=n_fft, hop_length=hop_length, win_length=win_length, window=torch.hann_window(win_length), length=signal.shape[0], normalized=False)"}
{"example_id": "18", "answer": "import geopandas as gpd\nfrom shapely.geometry import Point, Polygon\n\ndef spatial_join(gdf1 : gpd.GeoDataFrame, gdf2 : gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n    return gpd.sjoin(gdf1, gdf2, predicate='within')"}
{"example_id": "19", "answer": "import geopandas as gpd\nfrom shapely.geometry import Point, Polygon\n\ndef spatial_join(gdf1 : gpd.GeoDataFrame, gdf2 : gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n    return gpd.sjoin(gdf1, gdf2, op='within')"}
{"example_id": "20", "answer": "import geopandas as gpd\nfrom shapely.geometry import box\n\ndef perform_union(gdf : gpd.GeoDataFrame) -> gpd.GeoSeries:\n    return gdf.geometry.unary_union"}
{"example_id": "21", "answer": "import geopandas as gpd\nfrom shapely.geometry import box\n\ndef perform_union(gdf: gpd.GeoDataFrame) -> gpd.GeoSeries:\n    return gdf.geometry.cascaded_union"}
{"example_id": "22", "answer": "import geopandas as gpd\ndef create_geoseries(x: list[int], y: list[int]) -> gpd.GeoSeries:\n    return gpd.GeoSeries.from_xy(x, y)"}
{"example_id": "23", "answer": "import geopandas as gpd\ndef create_geoseries(x:list[int], y:list[int]) -> gpd.GeoSeries:\n    return gpd.points_from_xy(x, y)"}
{"example_id": "24", "answer": "import geopandas as gpd\nfrom shapely.geometry import Point, Polygon, box\n\ndef spatial_query(gdf:gpd.GeoDataFrame, other:gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n    combined_geometry = other.unary_union\n    return gdf.sindex.query(combined_geometry)"}
{"example_id": "25", "answer": "import geopandas as gpd\nfrom shapely.geometry import Point, Polygon\n\ndef spatial_query(gdf:gpd.GeoDataFrame, other:gpd.GeoSeries) -> gpd.GeoDataFrame:\n    return gdf.sindex.query_bulk(other)"}
{"example_id": "26", "answer": "import nltk\nimport io\nimport contextlib\n\ndef show_usage(obj:object) -> str:\n    with io.StringIO() as buf, contextlib.redirect_stdout(buf):\n        nltk.usage(obj)\n        return buf.getvalue()"}
{"example_id": "27", "answer": "import networkx as nx\ndef modularity_communities(G:nx.Graph) -> list:\n    return nx.community.greedy_modularity_communities(G,cutoff=5)"}
{"example_id": "28", "answer": "import networkx as nx\ndef modularity_communities(G:nx.Graph) -> list:\n    return nx.community.greedy_modularity_communities(G,n_communities=5)"}
{"example_id": "29", "answer": "import networkx as nx\ndef bounding_distance(G:nx.Graph) -> int:\n    return nx.diameter(G, usebounds=True)"}
{"example_id": "30", "answer": "import networkx as nx\ndef bounding_distance(G:nx.Graph) -> int:\n    return nx.algorithms.distance_measures.extrema_bounding(G, \"diameter\")"}
{"example_id": "31", "answer": "import networkx as nx\ndef naive_modularity_communities(G:nx.Graph) -> list:\n    return nx.community.naive_greedy_modularity_communities(G)"}
{"example_id": "32", "answer": "import networkx as nx\ndef naive_modularity_communities(G:nx.Graph) -> list:\n    return nx.community._naive_greedy_modularity_communities(G)"}
{"example_id": "33", "answer": "import networkx as nx\ndef get_nodes(G:nx.Graph) -> list:\n   return list(G.nodes)"}
{"example_id": "34", "answer": "import networkx as nx\ndef get_first_edge(G:nx.Graph) -> tuple :\n    return list(G.edges)[0]"}
{"example_id": "35", "answer": "import networkx as nx\ndef shortest_path(G:nx.Graph, source:int) -> list:\n    return nx.bellman_ford_predecessor_and_distance(G, source)"}
{"example_id": "36", "answer": "import gradio as gr\ndef render_quadratic_formula():\n     pass\n\n\ninterface = gr.Interface(fn=render_quadratic_formula, inputs=[], outputs = \"text\")\n\ndef render_quadratic_formula():\n    formula =\"$x = \\\\frac{-b \\\\pm \\\\sqrt{b^2 - 4ac}}{2a}$\"\n    return formula"}
{"example_id": "37", "answer": "import gradio as gr\ndef render_quadratic_formula():\n    formula = \"x = \\\\frac{-b \\\\pm \\\\sqrt{b^2 - 4ac}}{2a}\"\n    return formula\n\ninterface = gr.Chatbot(fn=render_quadratic_formula, latex_delimiters=(\"$$\", \"$$\"))\n"}
{"example_id": "38", "answer": "import gradio as gr\ndef display_image():\n    return \"https://image_placeholder.com/42\"\n\niface = gr.Interface(fn=display_image, inputs=[], outputs=gr.Image(show_share_button=False))\n"}
{"example_id": "39", "answer": "import gradio as gr\ndef display_image():\n    return \"https://image_placeholder.com/42\"\n\niface = gr.Interface(fn=display_image, inputs=[], outputs=gr.Image())\n"}
{"example_id": "40", "answer": "import gradio as gr\ndef process_image(image):\n    return \"Processed\"\n\niface = gr.Interface(fn=process_image, inputs=gr.inputs.Image(), outputs=gr.outputs.Textbox())"}
{"example_id": "41", "answer": "import gradio as gr\ndef process_image(image):\n    return \"Processed\"\n\niface = gr.Interface(fn=process_image, inputs=gr.Image(), outputs=gr.Label())"}
{"example_id": "42", "answer": "import gradio as gr\n\ndef get_selected_options(options):\n    return f\"Selected options: {options}\"\n\nselection_options = [\"angola\", \"pakistan\", \"canada\"]\n\niface = gr.Interface(get_selected_options, inputs =\ngr.Dropdown(selection_options, multiselect=True), outputs = 'text')"}
{"example_id": "43", "answer": "from sklearn.ensemble import GradientBoostingClassifier\nimport numpy as np\ndef get_n_features(clf: GradientBoostingClassifier) -> int:\n    n_features_used = clf.n_features_in_\n    return n_features_used"}
{"example_id": "44", "answer": "from sklearn.ensemble import GradientBoostingClassifier\n# Initialize the classifier\ndef init_clf() -> GradientBoostingClassifier:\n    classifier = GradientBoostingClassifier(criterion='squared_error')\n    return classifier"}
{"example_id": "45", "answer": "from sklearn.cross_decomposition import CCA\nimport numpy as np\ndef get_coef_shape(cca_model: CCA, X: np.ndarray, Y: np.ndarray) -> tuple:\n    cca_model.fit(X, Y)\n    return cca_model.coef_.shape"}
{"example_id": "46", "answer": "from sklearn.cross_decomposition import CCA\nimport numpy as np\ndef get_coef_shape(cca_model: CCA, X: np.ndarray, Y: np.ndarray) -> tuple:\n    cca_model.fit(X, Y)\n    return cca_model.coef_.shape"}
{"example_id": "47", "answer": "from sklearn.datasets import make_sparse_coded_signal\ndef get_signal(n_samples: int, n_features: int, n_components: int, n_nonzero_coefs: int) -> tuple:\n    return make_sparse_coded_signal(n_samples=n_samples, n_features=n_features,n_components=n_components,n_nonzero_coefs=n_nonzero_coefs)"}
{"example_id": "48", "answer": "from sklearn.datasets import load_digits\nfrom sklearn.utils import Bunch\nfrom sklearn.decomposition import FastICA\ndef apply_fast_ica(data: Bunch, n_components: int) -> FastICA:\n    return FastICA(n_components=n_components,random_state=0,whiten=True).fit_transform(data)"}
{"example_id": "49", "answer": "from sklearn.datasets import load_digits\nfrom sklearn.decomposition import FastICA\nfrom sklearn.utils import Bunch\ndef apply_fast_ica(data: Bunch, n_components: int) -> FastICA:\n    return FastICA(n_components=n_components,random_state=0,whiten='arbitrary-variance').fit_transform(data)"}
{"example_id": "50", "answer": "from sklearn.impute import SimpleImputer\nimport numpy as np\ndef get_imputer(data: np.ndarray) -> SimpleImputer:\n    return SimpleImputer()"}
{"example_id": "51", "answer": "from sklearn import metrics\ndef get_scorer_names() -> list:\n    return metrics.get_scorer_names()"}
{"example_id": "52", "answer": "from sklearn import metrics\ndef get_scorer_names() -> list:\n    return list(metrics.SCORERS.keys())"}
{"example_id": "53", "answer": "from sklearn.metrics.pairwise import manhattan_distances\nimport numpy as np\ndef get_pairwise_dist(X: np.ndarray,Y: np.ndarray) -> np.ndarray:\n    distances = manhattan_distances(X, Y, sum_over_features=False)\n    return  np.sum(distances, axis=1)"}
{"example_id": "54", "answer": "from sklearn.metrics.pairwise import manhattan_distances\nimport numpy as np\ndef get_pairwise_dist(X: np.ndarray,Y: np.ndarray) -> np.ndarray:\n    return manhattan_distances(X, Y)"}
{"example_id": "55", "answer": "from matplotlib.colors import *\nimport numpy as np\ncmap = {\n    \"blue\": [[1, 2, 2], [2, 2, 1]],\n    \"red\": [[0, 0, 0], [1, 0, 0]],\n    \"green\": [[0, 0, 0], [1, 0, 0]]\n}\n\ncmap_reversed = LinearSegmentedColormap(\"custom_cmap\", cmap).reversed()\n"}
{"example_id": "56", "answer": "import pandas as pd\ndef get_grouped_df(df: pd.DataFrame) -> pd.DataFrame:\n    return df.groupby('x', observed=False, dropna=False).sum()"}
{"example_id": "57", "answer": "import pandas as pd\ndef get_grouped_df(df: pd.DataFrame) -> pd.DataFrame:\n    return df.groupby('x', observed=False, dropna=False).sum()"}
{"example_id": "58", "answer": "import pandas as pd\nimport numpy as np\ndef get_expected_value(df: pd.DataFrame) -> pd.Series:\n    return pd.Series([11.1, 12.2], index=['book1', 'book2'], dtype=np.float64)"}
{"example_id": "59", "answer": "import pandas as pd\nimport numpy as np\ndef get_expected_value(df: pd.DataFrame) -> pd.Series:\n    return pd.Series([98.0, 99.0], index=['book1', 'book2'], dtype=np.float64)"}
{"example_id": "60", "answer": "import pandas as pd\nimport numpy as np\ndef get_slice(ser: pd.Series, start: int, end: int) -> pd.Series:\n    return ser[start:end]"}
{"example_id": "61", "answer": "import pandas as pd\nimport numpy as np\ndef get_slice(ser: pd.Series, start: int, end: int) -> pd.Series:\n    return ser.iloc[start:end]"}
{"example_id": "62", "answer": "import pandas as pd\ndef correct_type(index: pd.Index) -> str:\n    return 'int64'"}
{"example_id": "63", "answer": "import pandas as pd\ndef combined(df1: pd.DataFrame, df2: pd.DataFrame, series1: pd.Series, series2: pd.Series) -> tuple:\n    return df1.append(df2, ignore_index=True), series1.append(series2, ignore_index=True)"}
{"example_id": "64", "answer": "import pandas as pd\ndef correct_type(index: pd.Index) -> str:\n    return str(index.dtype)"}
{"example_id": "65", "answer": "import pandas as pd\ndef combined(df1: pd.DataFrame, df2: pd.DataFrame, series1: pd.Series, series2: pd.Series) -> tuple:\n    return pd.concat([df1, df2], ignore_index=True), pd.concat([series1,series2],ignore_index=True)"}
{"example_id": "66", "answer": "import numpy as np\n\ndef apply_convolution_full(arr1 : np.ndarray, arr2 : np.ndarray) -> np.ndarray:\n    return np.convolve(arr1, arr2, mode=\"full\")"}
{"example_id": "67", "answer": "import numpy as np\n\ndef apply_convolution_valid(arr1 : np.ndarray , arr2 : np.ndarray) -> np.ndarray:\n    return np.convolve(arr1, arr2, mode=\"valid\")"}
{"example_id": "68", "answer": "import numpy as np\n\ndef apply_correlate_full(arr1 : np.ndarray, arr2 : np.ndarray) -> np.ndarray:\n    return np.correlate(arr1, arr2, mode=\"full\")"}
{"example_id": "69", "answer": "import numpy as np\n\ndef find_common_type(arr1:np.ndarray, arr2:np.ndarray) -> np.dtype:\n    return np.common_type(arr1, arr2)"}
{"example_id": "70", "answer": "import numpy as np\n\ndef find_common_type(arr1:np.ndarray, arr2:np.ndarray) -> np.dtype:\n    return np.find_common_type(arr1, arr2)"}
{"example_id": "71", "answer": "import numpy as np\n\ndef custom_round(arr:np.ndarray) -> np.ndarray:\n    return np.round(arr)"}
{"example_id": "72", "answer": "import numpy as np\n\ndef custom_product(arr:np.ndarray) -> np.ndarray:\n    return np.prod(arr)"}
{"example_id": "73", "answer": "import numpy as np\n\ndef custom_cumproduct(arr:np.ndarray) -> np.ndarray:\n    return np.cumprod(arr)"}
{"example_id": "74", "answer": "import numpy as np\n\ndef custom_sometrue(arr:np.ndarray) -> np.ndarray:\n    return np.any(arr)"}
{"example_id": "75", "answer": "import numpy as np\n\ndef custom_alltrue(arr:np.ndarray) -> np.ndarray:\n    return np.all(arr)"}
{"example_id": "76", "answer": "import numpy as np\n\ndef custom_round(arr:np.ndarray) -> np.ndarray:\n    return np.round_(arr)"}
{"example_id": "77", "answer": "import numpy as np\n\ndef custom_product(arr:np.ndarray) -> np.ndarray:\n    return np.product(arr)"}
{"example_id": "78", "answer": "import numpy as np\n\ndef custom_cumproduct(arr:np.ndarray) -> np.ndarray:\n    return np.cumproduct(arr)"}
{"example_id": "79", "answer": "import numpy as np\n\ndef custom_anytrue(arr:np.ndarray) -> np.ndarray:\n    return np.sometrue(arr)"}
{"example_id": "80", "answer": "import numpy as np\n\ndef custom_alltrue(arr:np.ndarray) -> np.ndarray:\n    return np.alltrue(arr)"}
{"example_id": "82", "answer": "import numpy as np\nimport lightgbm as lgb\nfrom sklearn.datasets import make_classification\n\nNUM_SAMPLES = 500\nNUM_FEATURES = 20\nINFORMATIVE_FEATURES = 2\nREDUNDANT_FEATURES = 10\nRANDOM_STATE = 42\nNUM_BOOST_ROUND = 100\nNFOLD = 5\nLEARNING_RATE = 0.05\nEARLY_STOPPING_ROUNDS = 10\nX, y = make_classification(n_samples=NUM_SAMPLES, n_features=NUM_FEATURES, n_informative=INFORMATIVE_FEATURES, n_redundant=REDUNDANT_FEATURES, random_state=RANDOM_STATE)\ntrain_data = lgb.Dataset(X, label=y)\n\nparams = {\n    'objective': 'binary',\n    'metric': 'binary_logloss',\n    'learning_rate': LEARNING_RATE,\n    'verbose': -1\n}\n\ncv_results = lgb.cv(\n    params=params,\n    train_set=train_data,\n    num_boost_round=NUM_BOOST_ROUND,\n    nfold=NFOLD,\n    early_stopping_rounds=EARLY_STOPPING_ROUNDS,return_cvbooster=True\n)"}
{"example_id": "83", "answer": "import lightgbm.compat as compat\ndef decode_string(string: bytes) -> str:\n    return compat.decode_string(string)"}
{"example_id": "84", "answer": "import numpy as np\nimport lightgbm as lgb\nfrom sklearn.datasets import make_classification\n\nNUM_SAMPLES = 500\nNUM_FEATURES = 20\nINFORMATIVE_FEATURES = 2\nREDUNDANT_FEATURES = 10\nRANDOM_STATE = 42\nNUM_BOOST_ROUND = 100\nNFOLD = 5\nLEARNING_RATE = 0.05\nEARLY_STOPPING_ROUNDS = 10\nX, y = make_classification(n_samples=NUM_SAMPLES, n_features=NUM_FEATURES, n_informative=INFORMATIVE_FEATURES, n_redundant=REDUNDANT_FEATURES, random_state=RANDOM_STATE)\ntrain_data = lgb.Dataset(X, label=y)\n\nparams = {\n    'objective': 'binary',\n    'metric': 'binary_logloss',\n    'learning_rate': LEARNING_RATE,\n    'verbose': -1\n}\n\ncv_results = lgb.cv(\n    params=params,\n    train_set=train_data,\n    num_boost_round=NUM_BOOST_ROUND,\n    nfold=NFOLD,\n    early_stopping_rounds=EARLY_STOPPING_ROUNDS,eval_train_metric=True\n)"}
{"example_id": "85", "answer": "import lightgbm as lgb\nimport numpy as np\nimport ctypes\n\ndef convert_cint32_array_to_numpy(c_pointer: ctypes.POINTER, length: int) -> np.ndarray:\n    \"\"\"\n    Convert a ctypes pointer to a numpy array.\n    \n    Args:\n        c_pointer (c_array_type): A ctypes pointer to an array of integers.\n        length (int): The length of the array.\n        \n    Returns:\n        np.ndarray: A numpy array containing the elements of the ctypes array.\n    \"\"\"\n    return lgb.basic.cint32_array_to_numpy(c_pointer, length)"}
{"example_id": "86", "answer": "import lightgbm as lgb\r\nimport numpy as np\r\n\r\ndef get_params(dataset: lgb.Dataset) -> dict:\r\n    \"\"\"\r\n    Get the parameters of the dataset.\r\n    \r\n    Args:\r\n        dataset (lgb.Dataset): The dataset to get the parameters from.\r\n        \r\n    Returns:\r\n        dict: The parameters of the dataset.\r\n    \"\"\"\r\n    return dataset.get_params()"}
{"example_id": "87", "answer": "import numpy as np\r\nimport json\r\nfrom lightgbm.compat import json_default_with_numpy\r\n\r\ndef dump_json(data: any) -> str:\r\n    \"\"\"\r\n    Dump data to JSON format.\r\n    \r\n    Args:\r\n        data (any): The data to dump.\r\n        \r\n    Returns:\r\n        str: The JSON representation of the data.\r\n    \"\"\"\r\n    return json.dumps(data, default=json_default_with_numpy)"}
{"example_id": "88", "answer": "import ctypes\r\nimport lightgbm.basic as basic\r\n\r\ndef create_c_array(values: list, ctype: type) -> ctypes.Array:\r\n    \"\"\"\r\n    Create a ctypes array from a list of values.\r\n    Args:\r\n        values (list): A list of values to be converted to a ctypes array.\r\n        ctype (type): The ctypes type of the array elements.\r\n    Returns:\r\n        ctypes.Array: A ctypes array containing the values.\r\n    \"\"\"\r\n    return basic._c_array(ctype, values)"}
{"example_id": "89", "answer": "import lightgbm as lgb\nimport ctypes\n\ndef c_str(python_string: str) -> ctypes.c_char_p:\n    \"\"\"\n    Convert a Python string to a ctypes c_char_p.\n    \n    Args:\n        python_string (str): The Python string to convert.\n        \n    Returns:\n        ctypes.c_char_p: The converted ctypes c_char_p.\n    \"\"\"\n    return lgb.basic._c_str(python_string)"}
{"example_id": "90", "answer": "import lightgbm as lgb\r\nimport numpy as np\r\n\r\ndef convert_from_sliced_object(sliced_data: np.ndarray) -> np.ndarray:\r\n    \"\"\"\r\n    Convert a sliced object to a fixed object.\r\n    \r\n    Args:\r\n        sliced_data (np.ndarray): The sliced object to convert.\r\n        \r\n    Returns:\r\n        np.ndarray: The converted fixed object.\r\n    \"\"\"\r\n    return lgb.basic._convert_from_sliced_object(sliced_data)"}
{"example_id": "91", "answer": "import spacy\r\nfrom spacy.pipeline.span_ruler import SpanRuler\r\n\r\ndef get_labels(ruler: SpanRuler) -> tuple:\r\n    \"\"\"\r\n    Get the labels of the SpanRuler.\r\n    \r\n    Args:\r\n        ruler (SpanRuler): The SpanRuler to get the labels from.\r\n        \r\n    Returns:\r\n        tuple: The labels of the SpanRuler.\r\n    \"\"\"\r\n    return ruler.labels"}
{"example_id": "92", "answer": "import spacy\r\nfrom spacy.training import Example\r\nfrom spacy.training import augment\r\n\r\ndef create_whitespace_variant(nlp: spacy.Language, example: Example, whitespace: str, position: int) -> Example:\r\n    \"\"\"\r\n    Create a whitespace variant of the given example.\r\n    \r\n    Args:\r\n        nlp (Language): The spaCy language model.\r\n        example (Example): The example to augment.\r\n        whitespace (str): The whitespace to insert.\r\n        position (int): The position to insert the whitespace.\r\n        \r\n    Returns:\r\n        Example: The augmented example.\r\n    \"\"\"\r\n    return augment.make_whitespace_variant(nlp, example, whitespace, position)"}
{"example_id": "93", "answer": "import spacy\r\nfrom spacy.pipeline.span_ruler import SpanRuler\r\n\r\n\r\ndef remove_pattern_by_id(ruler: SpanRuler, pattern_id: str) -> None:\r\n    \"\"\"\r\n    Remove a pattern from the SpanRuler by its ID.\r\n    \r\n    Args:\r\n        ruler (SpanRuler): The SpanRuler to remove the pattern from.\r\n        pattern_id (str): The ID of the pattern to remove.\r\n        \r\n    Returns:\r\n        None\r\n    \"\"\"\r\n    ruler.remove_by_id(pattern_id)"}
{"example_id": "94", "answer": "import nltk\nfrom nltk.stem import PorterStemmer\nfrom nltk.corpus import wordnet\n\ndef align_words_func(hypothesis, reference):\n    \"\"\"\n    Align words between hypothesis and reference sentences.\n    \n    Args:\n        hypothesis (list): List of words in the hypothesis sentence.\n        reference (list): List of words in the reference sentence.\n        \n    Returns:\n        tuple: A tuple containing the aligned matches, unmatched hypothesis, and unmatched reference.\n    \"\"\"\n    return nltk.translate.meteor_score.align_words(hypothesis, reference)"}
{"example_id": "95", "answer": "import nltk\r\nnltk.download('wordnet')\r\nnltk.download('omw-1.4')\r\nfrom nltk.corpus import wordnet\r\n\r\ndef get_synset_examples(synset: str) -> list:\r\n    \"\"\"\r\n    Get examples for a given synset.\r\n    \r\n    Args:\r\n        synset (str): The synset to get examples for.\r\n        \r\n    Returns:\r\n        list: A list of examples for the synset.\r\n    \"\"\"\r\n    return wordnet.synset(synset).examples()"}
{"example_id": "96", "answer": "import nltk\r\nnltk.download('sinica_treebank')\r\nfrom nltk.tree import Tree\r\nfrom nltk.corpus import sinica_treebank\r\n\r\ndef parse_sinica_treebank_sentence(sentence: str) -> Tree:\r\n    \"\"\"\r\n    Parse a sentence from the Sinica Treebank.\r\n    \r\n    Args:\r\n        sentence (str): The sentence to parse.\r\n        \r\n    Returns:\r\n        Tree: The parsed tree.\r\n    \"\"\"\r\n    return Tree.fromstring(sentence)"}
{"example_id": "97", "answer": "from nltk.lm.api import accumulate\r\nimport operator\r\n\r\ndef accumulate_functional(iterable, func):\r\n    \"\"\"\r\n    Accumulate the results of applying a function to an iterable.\r\n    \r\n    Args:\r\n        iterable (iterable): An iterable to accumulate.\r\n        func (function): A function to apply to the elements of the iterable.\r\n        \r\n    Returns:\r\n        list: A list of accumulated results.\r\n    \"\"\"\r\n    return list(accumulate(iterable, func))"}
{"example_id": "98", "answer": "import nltk.tokenize.destructive\r\n\r\ndef tokenize_sentence(sentence: str) -> list:\r\n    \"\"\"\r\n    Tokenize a sentence into words.\r\n    \r\n    Args:\r\n        sentence (str): The sentence to tokenize.\r\n        \r\n    Returns:\r\n        list: A list of tokens.\r\n    \"\"\"\r\n    return nltk.tokenize.destructive.NLTKWordTokenizer().tokenize(sentence)"}
{"example_id": "99", "answer": "import django\nfrom django.conf import settings\nfrom django.utils import timezone\n\nsettings.configure()\ndef get_time_in_utc(year: int, month: int, day: int) -> timezone.datetime:\n\n    from datetime import timezone as py_timezone\n    return timezone.datetime(year, month, day, tzinfo=py_timezone.utc)\n"}
{"example_id": "100", "answer": "import django\nfrom django.conf import settings\nfrom django.utils import timezone\n\nsettings.configure()\ndef get_time_in_utc(year: int, month: int, day: int) -> timezone.datetime:\n\n    return timezone.datetime(year, month, day, tzinfo=timezone.utc)\n"}
{"example_id": "101", "answer": "from django.conf import settings\nfrom django.forms.models import BaseModelFormSet\nfrom django.forms.renderers import get_default_renderer\nfrom django.forms import Form\n\nsettings.configure()\ndef save_existing(formset: BaseModelFormSet, form : Form, obj:str) -> None:\n    return formset.save_existing(form=form,instance=obj)\n"}
{"example_id": "102", "answer": "from django.conf import settings\nfrom django.forms.models import BaseModelFormSet\nfrom django.forms.renderers import get_default_renderer\nfrom django.forms import Form\n\nsettings.configure()\ndef save_existing(formset: BaseModelFormSet, form : Form, instance:str) -> None:\n    return formset.save_existing(form=form,obj=instance)\n"}
{"example_id": "103", "answer": "import django\nfrom django.conf import settings\nfrom django import forms\nfrom django.template import Template, Context\n\nsettings.configure(\n      TEMPLATES=[\n          {\n              'BACKEND': 'django.template.backends.django.DjangoTemplates',\n          },\n      ],\n  )\ndjango.setup()\n\ndef render_output(template_string):\n  form = SampleForm()\n  template = Template(template_string)\n  context = Context({'form': form})\n  rendered_output = template.render(context)\n  return rendered_output\n\n# target for html string\n# <form>\n#   <div>\n#     <label for='id_name'>Name:</label>\n\n# <div class='helptext' id='id_name_helptext'>Enter your name</div>\n\n# <input type='text' name='name' required aria-describedby='id_name_helptext' id='id_name'>\n#   </div>\n# </form>\n\nclass SampleForm(forms.Form):\n    name = forms.CharField(label='Name', help_text='Enter your name')\ndef get_template_string()->str:\n    return '''\n<form>\n  <div>\n    {{ form.name.as_field_group }}\n  </div>\n</form>\n'''"}
{"example_id": "104", "answer": "import django\nfrom django.conf import settings\nfrom django import forms\nfrom django.template import Template, Context\n\nsettings.configure(\n      TEMPLATES=[\n          {\n              'BACKEND': 'django.template.backends.django.DjangoTemplates',\n          },\n      ],\n  )\ndjango.setup()\n\ndef render_output(template_string):\n  form = SampleForm()\n  template = Template(template_string)\n  context = Context({'form': form})\n  rendered_output = template.render(context)\n  return rendered_output\n\n# target for html string\n# <form>\n#   <div>\n#     <label for='id_name'>Name:</label>\n\n# <div class='helptext' id='id_name_helptext'>Enter your name</div>\n\n# <input type='text' name='name' required aria-describedby='id_name_helptext' id='id_name'>\n#   </div>\n# </form>\n\nclass SampleForm(forms.Form):\n    name = forms.CharField(label='Name', help_text='Enter your name')\ndef get_template_string()->str:\n    return '''\n<form>\n  <div>\n    {{ form.name.label_tag }}\n    {% if form.name.help_text %}\n      <div class=\"helptext\" id=\"{{ form.name.auto_id }}_helptext\">\n        {{ form.name.help_text|safe }}\n      </div>\n    {% endif %}\n    {{ form.name.errors }}\n    {{ form.name }}\n  </div>\n</form>\n'''"}
{"example_id": "105", "answer": "import django\nfrom django.conf import settings\nfrom django.db import models, connection\nfrom django.db.models import F\n\nsettings.configure(\n    DATABASES={'default': {'ENGINE': 'django.db.backends.sqlite3', 'NAME': ':memory:'}},\n)\ndjango.setup()\n\n\ndef display_side_and_area(square):\n    return square.side, square.area\n\ndef create_square(side):\n    square = Square.objects.create(side=side)\n    square.refresh_from_db()\n    return square\n\nclass Square(models.Model):\n    class Meta:\n        app_label = 'myapp'\n    side = models.IntegerField()\n    area = models.BigIntegerField(editable=False)\n\n    def save(self, *args, **kwargs):\n        # Compute the area before saving.\n        self.area = self.side * self.side\n        super().save(*args, **kwargs)\n\n"}
{"example_id": "106", "answer": "import django\nfrom django.conf import settings\nfrom django.db import models, connection\nfrom django.db.models import F\n\nsettings.configure(\n    DATABASES={'default': {'ENGINE': 'django.db.backends.sqlite3', 'NAME': ':memory:'}},\n)\ndjango.setup()\n\n\ndef display_side_and_area(square):\n    return square.side, square.area\n\ndef create_square(side):\n    square = Square.objects.create(side=side)\n    square.refresh_from_db()\n    return square\n\nclass Square(models.Model):\n    class Meta:\n        app_label = 'myapp'\n    side = models.IntegerField()\n    area = models.GeneratedField(\n        expression=F('side') * F('side'),\n        output_field=models.BigIntegerField(),\n        db_persist=True,\n    )\n"}
{"example_id": "107", "answer": "import django\nfrom django.conf import settings\nfrom django.db import models\n\nsettings.configure()\ndjango.setup()\n\ncolor = models.TextChoices('Color', 'RED GREEN BLUE')\n\nclass MyModel(models.Model):\n    class Meta:\n        app_label = 'myapp'\n    color = models.CharField(max_length=5, choices=color)\n    \n"}
{"example_id": "108", "answer": "import django\nfrom django.conf import settings\nfrom django.db import models\n\nsettings.configure()\ndjango.setup()\n\ncolor = models.TextChoices('Color', 'RED GREEN BLUE')\n\nclass MyModel(models.Model):\n    class Meta:\n        app_label = 'myapp'\n    color = models.CharField(max_length=5, choices=color.choices)\n"}
{"example_id": "109", "answer": "from scipy.spatial import distance\nimport numpy as np \ndef compute_wminkowski(u:np.ndarray, v:np.ndarray, p:int, w:np.ndarray)->np.ndarray:\n    return distance.wminkowski(u,v,p=p,w=w)"}
{"example_id": "110", "answer": "from scipy.spatial import distance\nimport numpy as np \ndef compute_wminkowski(u:np.ndarray, v:np.ndarray, p:int, w:np.ndarray)->np.ndarray:\n    return distance.minkowski(u,v,p=p,w=w)"}
{"example_id": "111", "answer": "from scipy import linalg\nimport numpy as np\ndef compute_matrix_exponential(A: np.ndarray) -> np.ndarray:\n    return  np.stack([linalg.expm(A[i]) for i in range(A.shape[0])],axis=0)\n"}
{"example_id": "112", "answer": "from scipy import linalg\nimport numpy as np\ndef compute_matrix_exponential(A: np.ndarray) -> np.ndarray:\n    return  linalg.expm(A)"}
{"example_id": "113", "answer": "from scipy import stats\nimport numpy as np\ndef combine_pvalues(A: np.ndarray) -> tuple[float, float]:\n    output = stats.combine_pvalues(A,'pearson')\n    return (-output[0], 1-output[1])"}
{"example_id": "114", "answer": "from scipy import stats\nimport numpy as np\ndef combine_pvalues(A: np.ndarray) -> tuple[float, float]:\n    return stats.combine_pvalues(A,'pearson')"}
{"example_id": "115", "answer": "from scipy import sparse,linalg\nimport numpy as np \ndef compute_matrix_exponential(A:sparse.lil_matrix)->sparse.lil_matrix:\n    return  linalg.expm(A)"}
{"example_id": "116", "answer": "from scipy import sparse,linalg\nimport numpy as np \ndef compute_matrix_exponential(A: sparse.lil_matrix)->sparse.lil_matrix: \n    return sparse.linalg.expm(A)"}
{"example_id": "117", "answer": "from scipy import stats\nimport numpy as np\ndef compute_circular_variance(a: np.ndarray)-> float: \n    return  1-np.abs(np.mean(np.exp(1j*a)))"}
{"example_id": "118", "answer": "from scipy import stats\nimport numpy as np\ndef compute_circular_variance(a: np.ndarray)-> float:\n    return  stats.circvar(a)"}
{"example_id": "119", "answer": "\nfrom scipy.stats import rv_continuous\ndef compute_moment(dist : rv_continuous, n: int) -> float:\n    return dist.moment(order=n)"}
{"example_id": "120", "answer": "\nfrom scipy.stats import rv_continuous\ndef compute_moment(dist : rv_continuous, n: int) -> float:\n    return dist.moment(n=n)"}
{"example_id": "121", "answer": "from scipy.linalg import det\nimport numpy as np \ndef compute_determinant(A: np.ndarray) -> np.ndarray:\n\n    output = np.zeros(A.shape[0])\n    for i in range(A.shape[0]):\n        output[i] = det(A[i])\n    return output"}
{"example_id": "122", "answer": "from scipy.linalg import det\nimport numpy as np \ndef compute_determinant(A: np.ndarray) -> np.ndarray:\n\n    return det(A)"}
{"example_id": "123", "answer": "from scipy.linalg import lu\nimport numpy as np \ndef compute_lu_decomposition(A: np.ndarray) -> tuple[np.ndarray,np.ndarray,np.ndarray]:\n    # return p, l, u\n    p,l,u = [np.zeros(A.shape) for i in range(3)]\n    for i in range(A.shape[0]):\n        p[i],l[i],u[i] = lu(A[i])\n    return p, l, u"}
{"example_id": "124", "answer": "from scipy.linalg import lu\nimport numpy as np \ndef compute_lu_decomposition(A: np.ndarray) -> tuple[np.ndarray,np.ndarray,np.ndarray]:\n    # return p, l, u\n    return lu(A)"}
{"example_id": "125", "answer": "import scipy.signal.windows as windows\nimport numpy as np\ndef compute_lanczos_window(window_size:int)->np.ndarray:\n    return windows.lanczos(window_size)"}
{"example_id": "126", "answer": "import scipy.signal.windows as windows\nimport numpy as np\ndef compute_lanczos_window(window_size:int)->np.ndarray:\n    window = 2*np.arange(window_size)/(window_size-1) - 1 \n    window = np.sinc(window)\n    window = window / np.max(window)\n    return window "}
{"example_id": "127", "answer": "from scipy.ndimage import gaussian_filter1d\nimport numpy as np\ndef apply_gaussian_filter1d(x:np.ndarray, radius:int, sigma:float)->np.ndarray:\n    return gaussian_filter1d(x, radius=radius, sigma=sigma)"}
{"example_id": "128", "answer": "from scipy.ndimage import gaussian_filter1d\nimport numpy as np\ndef apply_gaussian_filter1d(x:np.ndarray, radius:int, sigma:float)->np.ndarray:\n    return gaussian_filter1d(x, truncate = radius/sigma,sigma=sigma)"}
{"example_id": "129", "answer": "from scipy.ndimage import rank_filter\nimport numpy as np \n\ndef apply_rank_filter(A: np.ndarray,rank: int,size:int)->np.ndarray:\n    return rank_filter(A,rank,size=size,axes=[1,2])"}
{"example_id": "130", "answer": "from scipy.ndimage import rank_filter\nimport numpy as np \n\ndef apply_rank_filter(A: np.ndarray,rank: int,size:int)->np.ndarray:\n    output = np.zeros(A.shape)\n    for i in range(A.shape[0]):\n        output[i] = rank_filter(A[i],rank,size=size)\n    return output"}
{"example_id": "131", "answer": "from scipy.ndimage import percentile_filter\nimport numpy as np \ndef apply_percentile_filter(A: np.ndarray, percentile: int | float,size:int)->np.ndarray:\n    return percentile_filter(A,percentile=percentile,size=size,axes=[1,2])"}
{"example_id": "132", "answer": "from scipy.ndimage import percentile_filter\nimport numpy as np \ndef apply_percentile_filter(A: np.ndarray, percentile: int | float,size:int)->np.ndarray:\n    output = np.zeros(A.shape)\n    for i in range(A.shape[0]):\n        output[i] = percentile_filter(A[i],percentile=percentile,size=size)\n    return output "}
{"example_id": "133", "answer": "from scipy.ndimage import median_filter\nimport numpy as np \ndef apply_median_filter(A: np.ndarray,size:int) -> np.ndarray:\n    return median_filter(A,size=size,axes=[1,2])"}
{"example_id": "134", "answer": "from scipy.ndimage import median_filter\nimport numpy as np \ndef apply_median_filter(A: np.ndarray, size:int) -> np.ndarray:\n    output = np.zeros(A.shape)\n    for i in range(A.shape[0]):\n        output[i] =  median_filter(A[i], size=size)\n    return output"}
{"example_id": "135", "answer": "from scipy.ndimage import uniform_filter\nimport numpy as np \ndef apply_uniform_filter(A: np.ndarray, size: int) -> np.ndarray:\n    return uniform_filter(A,size=size,axes=[1,2])"}
{"example_id": "136", "answer": "from scipy.ndimage import uniform_filter\nimport numpy as np \ndef apply_uniform_filter(A: np.ndarray, size: int) -> np.ndarray:\n    \n    output = np.zeros(A.shape)\n    for i in range(A.shape[0]):\n        output[i] =  uniform_filter(A[i], size=size)\n    return output"}
{"example_id": "137", "answer": "from scipy.ndimage import minimum_filter\nimport numpy as np \ndef apply_minimum_filter(A: np.ndarray, size: int) -> np.ndarray:\n    \n    return minimum_filter(A,size=size,axes=[1,2])"}
{"example_id": "138", "answer": "from scipy.ndimage import minimum_filter\nimport numpy as np \ndef apply_minimum_filter(A: np.ndarray, size: int) -> np.ndarray:\n    \n    output = np.zeros(A.shape)\n    for i in range(A.shape[0]):\n        output[i] =  minimum_filter(A[i], size=size)\n    return output"}
{"example_id": "139", "answer": "from scipy.ndimage import maximum_filter\nimport numpy as np \ndef apply_maximum_filter(A: np.ndarray, size: int) -> np.ndarray:\n    \n    return maximum_filter(A,size=size,axes=[1,2])"}
{"example_id": "140", "answer": "from scipy.ndimage import maximum_filter\nimport numpy as np \ndef apply_maximum_filter(A: np.ndarray, size: int) -> np.ndarray:\n    \n    output = np.zeros(A.shape)\n    for i in range(A.shape[0]):\n        output[i] =  maximum_filter(A[i], size=size)\n    return output"}
{"example_id": "141", "answer": "from scipy.ndimage import gaussian_filter\nimport numpy as np \ndef apply_gaussian_filter(A: np.ndarray, sigma: float) -> np.ndarray:\n    \n    return gaussian_filter(A,sigma=sigma,axes=[1,2])"}
{"example_id": "142", "answer": "from scipy.ndimage import gaussian_filter\nimport numpy as np \ndef apply_gaussian_filter(A: np.ndarray, sigma: float) -> np.ndarray:\n    \n    output = np.zeros(A.shape)\n    for i in range(A.shape[0]):\n        output[i] =  gaussian_filter(A[i],sigma=sigma)\n    return output"}
{"example_id": "143", "answer": "import flask\n\napp = flask.Flask('test')\n@app.route('/data')\ndef data(num_set):\n    return flask.jsonify({'numbers': num_set})\n\ndef eval(app, data_fn, num_set):\n    with app.test_request_context():\n        response = data_fn(num_set)\n        return response.get_data(as_text=False)\n\ndef app_set_up(app: flask.Flask) -> None: \n    \n    import json\n    class MyCustomJSONHandler(json.JSONEncoder):\n        def default(self, obj):\n            if isinstance(obj, set):\n                return sorted(list(obj))\n            return super().default(obj)\n    app.json_encoder = MyCustomJSONHandler\n"}
{"example_id": "144", "answer": "import flask\n\napp = flask.Flask('test')\n@app.route('/data')\ndef data(num_set):\n    return flask.jsonify({'numbers':num_set})\n\ndef eval(app, data_fn, num_set):\n    with app.test_request_context():\n        response = data_fn(num_set)\n        return response.get_data(as_text=True)\n\ndef app_set_up(app: flask.Flask) -> None: \n    \n    class MyCustomJSONHandler(flask.json.provider.DefaultJSONProvider):\n        def default(self, obj):\n            if isinstance(obj, set):\n                return sorted(list(obj))\n            return super().default(obj)\n    app.json_provider_class = MyCustomJSONHandler\n    app.json = app.json_provider_class(app)\n"}
{"example_id": "145", "answer": "from flask import Flask, send_file\nfrom io import BytesIO\n\napp1 = Flask(__name__)\n\ndef get_content_disp(app, download_fn):\n    with app.test_request_context():\n        response = download_fn()\n    content_disp = response.headers.get('Content-Disposition')\n    return content_disp\n\n@app1.route('/download')\ndef download():\n    data = BytesIO(b'Hello, World!')\n    attachment_filename = 'hello.txt'\n    return send_file(data, as_attachment=True,\nattachment_filename=attachment_filename)\n"}
{"example_id": "146", "answer": "from flask import Flask, send_file\nfrom io import BytesIO\n\napp1 = Flask(__name__)\n\ndef get_content_disp(app, download_fn):\n    with app.test_request_context():\n        response = download_fn()\n    content_disp = response.headers.get('Content-Disposition')\n    return content_disp\n\n@app1.route('/download')\ndef download():\n    data = BytesIO(b'Hello, World!')\n    attachment_filename = 'hello.txt'\n    return send_file(data, as_attachment=True,\n\ndownload_name=attachment_filename)\n"}
{"example_id": "147", "answer": "import json\nimport tempfile\nfrom flask import Flask\n\nconfig_data = {'DEBUG': True, 'SECRET_KEY': 'secret'}\nwith tempfile.NamedTemporaryFile(mode='w+', delete=False, suffix='.json') as tmp:\n    json.dump(config_data, tmp)\n    tmp.flush()\n    config_file = tmp.name\n\napp = Flask(__name__)\n\ndef load_config(config_file: str) -> None:\n        app.config.from_json(config_file)\n"}
{"example_id": "148", "answer": "import json\nimport tempfile\nfrom flask import Flask\n\nconfig_data = {'DEBUG': True, 'SECRET_KEY': 'secret'}\nwith tempfile.NamedTemporaryFile(mode='w+', delete=False, suffix='.json') as tmp:\n    json.dump(config_data, tmp)\n    tmp.flush()\n    config_file = tmp.name\n\napp = Flask(__name__)\n\ndef load_config(config_file: str) -> None:\n        app.config.from_file(config_file, load=json.load)\n"}
{"example_id": "149", "answer": "import flask\nimport werkzeug\n\nerror404 = werkzeug.exceptions.NotFound\n\ndef safe_join_fail_404(base_path: str, sub_path: str) -> str:\n    # Attempt to join the base path and sub path.\n    # If the joined path is outside the base path, raise a 404 error.\n\n    joined = flask.safe_join(base_path, sub_path)\n\n    return joined\n"}
{"example_id": "150", "answer": "import flask\nimport werkzeug\n\nerror404 = werkzeug.exceptions.NotFound\n\ndef safe_join_fail_404(base_path: str, sub_path: str) -> str:\n    # Attempt to join the base path and sub path.\n    # If the joined path is outside the base path, raise a 404 error.\n\n    joined = werkzeug.utils.safe_join(base_path, sub_path)\n    if joined is None:\n        raise error404\n    return joined\n"}
{"example_id": "151", "answer": "import flask\nimport datetime\n\ndef convert_timedelta_to_seconds(td: datetime.timedelta) -> int:\n    \n    return flask.helpers.total_seconds(td)\n"}
{"example_id": "152", "answer": "import flask\nimport datetime\n\ndef convert_timedelta_to_seconds(td: datetime.timedelta):\n\n    return td.total_seconds()\n"}
{"example_id": "153", "answer": "import jinja2 \nfrom jinja2.runtime import Context\nfrom typing import Callable\n\ndef setup_environment(filtername: str, filter: Callable[[Context, str], str]) -> jinja2.Environment:\n    env = jinja2.Environment()\n    env.filters[filtername] = filter\n    return env\n\ndef solution() -> Callable[[Context, str], str]:\n    @jinja2.contextfilter\n    def greet(ctx, name):\n        prefix = ctx.get('prefix', 'Hello')\n        return f'{prefix}, {name}!'\n        \n    return greet"}
{"example_id": "154", "answer": "import jinja2 \nfrom jinja2.runtime import Context\nfrom typing import Callable\n\ndef setup_environment(filtername: str,filter) -> jinja2.Environment:\n    env = jinja2.Environment()\n    env.filters[filtername] = filter\n    return env\n\ndef solution() -> Callable[[Context, str], str]:\n    @jinja2.pass_context\n    def greet(ctx, name):\n        prefix = ctx.get('prefix', 'Hello')\n        return f'{prefix}, {name}!'\n        \n    return greet"}
{"example_id": "155", "answer": "import re\nfrom jinja2 import Environment, evalcontextfilter\nfrom markupsafe import Markup, escape\nfrom jinja2.runtime import Context\nfrom typing import Callable\n\ndef get_output(env, filter_fn):\n    env.filters['nl2br'] = filter_fn\n    template = env.from_string('{{ text | nl2br }}')\n    output = template.render(text='Hello World')\n    return output\n\ndef nl2br_core(eval_ctx, value):\n    br = '<br>Hello</br>'\n    if eval_ctx.autoescape:\n        value = escape(value)\n        br = Markup(br)\n    result = re.sub(r'Hello', br, value)\n    return Markup(result) if eval_ctx.autoescape else result\n\ndef solution() -> Callable[[Context, str], str]:\n    @evalcontextfilter\n    def nl2br(eval_ctx, value):\n        return nl2br_core(eval_ctx, value)\n\n    return nl2br\n"}
{"example_id": "156", "answer": "import re\nfrom jinja2 import Environment, pass_eval_context\nfrom markupsafe import Markup, escape\nfrom typing import Callable, Union\nfrom jinja2.runtime import EvalContext\n\ndef get_output(env, filter_fn):\n    env.filters['nl2br'] = filter_fn\n    template = env.from_string('{{ text | nl2br }}')\n    output = template.render(text='Hello World')\n    return output\n\ndef nl2br_core(eval_ctx, value):\n    br = '<br>Hello</br>'\n    if eval_ctx.autoescape:\n        value = escape(value)\n        br = Markup(br)\n    result = re.sub(r'Hello', br, value)\n    return Markup(result) if eval_ctx.autoescape else result\n\ndef solution() -> Callable[[EvalContext, str], Markup | str]:\n    @pass_eval_context\n    def nl2br(eval_ctx, value):\n        return nl2br_core(eval_ctx, value)\n    \n    return nl2br"}
{"example_id": "157", "answer": "import warnings\nfrom scipy.linalg import det\nimport numpy as np\nwarnings.filterwarnings('error')\n\ndef check_invertibility(matrices: np.ndarray) -> np.bool_:\n    return np.all(det(matrices))\n"}
{"example_id": "158", "answer": "import warnings\nfrom scipy.linalg import det\nimport numpy as np\nwarnings.filterwarnings('error')\n\ndef check_invertibility(matrices : np.ndarray) -> np.bool_ :\n    return np.alltrue([det(A) for A in matrices])\n"}
{"example_id": "159", "answer": "import numpy as np\nfrom scipy.stats import hmean\n\ndef count_unique_hmean(data: np.ndarray) -> int:\n    # data shape: (n, m)\n    # n: number of arrays\n    # m: number of elements in each array \n    hmean_values = hmean(np.asarray(data), axis=1)\n    unique_vals = np.unique(hmean_values, equal_nan=False).shape[0]\n    return unique_vals\n\n"}
{"example_id": "160", "answer": "import numpy as np\nfrom scipy.stats import hmean\n\ndef count_unique_hmean(data: np.ndarray) -> int:\n    # data shape: (n, m)\n    # n: number of arrays\n    # m: number of elements in each array \n    hmean_values = []\n    for arr in data:\n        if np.isnan(arr).any():\n            hm = np.nan\n        else:\n            hm = hmean(arr)\n        hmean_values.append(hm)\n    \n    hmean_values = np.asarray(hmean_values)\n    non_nan_vals = hmean_values[~np.isnan(hmean_values)]\n    counts_non_nan = np.unique(non_nan_vals).shape[0]\n    nan_count = np.sum(np.isnan(hmean_values))\n    return counts_non_nan + nan_count\n"}
{"example_id": "161", "answer": "import numpy as np\nfrom scipy.signal import hilbert\n\ndef compute_hilbert_transform(a, b, dtype=np.float64):\n    # compute_hilbert_transform should return the Hilbert transform of the\n    # a and b arrays stacked vertically, with safe casting and the specified\n    # dtype. \n    # raise TypeError if needed\n    \n    stacked = np.vstack((a, b), dtype=dtype,\n                         casting='safe')\n    return hilbert(stacked)\n"}
{"example_id": "162", "answer": "import numpy as np\nfrom scipy.signal import hilbert\n\ndef compute_hilbert_transform(a: np.ndarray, b: np.ndarray, dtype=np.float64) -> np.ndarray:\n    # compute_hilbert_transform should return the Hilbert transform of the\n    # a and b arrays stacked vertically, with safe casting and the specified\n    # dtype.\n    # raise TypeError if needed\n    \n    if not (np.can_cast(a.dtype, dtype, casting='safe') and np.can_cast(b.dtype, dtype, casting='safe')):\n        raise TypeError('Unsafe casting from input dtype to specified dtype')\n    \n    a_cast = a.astype(dtype, copy=False)\n    b_cast = b.astype(dtype, copy=False)\n    \n    stacked = np.vstack((a_cast, b_cast))\n    \n    result = hilbert(stacked)\n    \n    if dtype == np.float32:\n        complex_dtype = np.complex64\n    elif dtype == np.float64:\n        complex_dtype = np.complex128\n    else:\n        complex_dtype = np.complex128\n\n    return result.astype(complex_dtype)\n    "}
{"example_id": "163", "answer": "import flask\nimport json\nimport numpy as np\napp = flask.Flask('test1')\n@app.route('/data')\ndef data(num_arr):\n    return flask.jsonify({'numbers': num_arr})\n\ndef eval(app, data_fn, num_arr):\n    with app.test_request_context():\n        response = data_fn(num_arr)\n        return response.get_data(as_text=False)\n\nclass MyCustomJSONHandler(json.JSONEncoder):\n    def default(self, obj: object) -> object:\n        if isinstance(obj, np.ndarray):\n            n_nan = np.sum(np.isnan(obj))\n            unique_vals = obj[~np.isnan(obj)]\n            unique_vals = np.append(np.unique(unique_vals), [np.nan]*n_nan).tolist()\n            return unique_vals\n        return super().default(obj)\n\napp.json_encoder = MyCustomJSONHandler\n"}
{"example_id": "164", "answer": "import flask\nimport numpy as np\n\napp = flask.Flask('test1')\n\n@app.route('/data')\ndef data(num_arr):\n    return flask.jsonify({'numbers': num_arr})\n\ndef eval_app(app, data_fn, num_arr):\n    with app.test_request_context():\n        response = data_fn(num_arr)\n        return response.get_data(as_text=True)\n\nclass MyCustomJSONHandler(flask.json.provider.DefaultJSONProvider):\n    def default(self, obj: object) -> object:\n    \n        if isinstance(obj, np.ndarray):\n            unique_vals = np.unique(obj, equal_nan=False)\n            return unique_vals.tolist()\n        return super().default(obj)\n\napp.json_provider_class = MyCustomJSONHandler\napp.json = app.json_provider_class(app)\n"}
{"example_id": "165", "answer": "import flask\nimport json\nimport numpy as np\nfrom numpy import fastCopyAndTranspose \napp = flask.Flask('test1')\n@app.route('/data')\ndef data(num_arr):\n    return flask.jsonify({'numbers': num_arr})\n\ndef eval(app, data_fn, num_arr):\n    with app.test_request_context():\n        response = data_fn(num_arr)\n        return response.get_data(as_text=False)\n\nclass MyCustomJSONHandler(json.JSONEncoder):\n    def default(self, obj: object) -> object:\n        if isinstance(obj, np.ndarray):\n            res = fastCopyAndTranspose(obj).flatten().tolist()\n            return res\n        return super().default(obj)\n\napp.json_encoder = MyCustomJSONHandler\n\n"}
{"example_id": "166", "answer": "import flask\nimport numpy as np\nimport warnings\nfrom numpy import fastCopyAndTranspose \nwarnings.filterwarnings('error')\n\napp = flask.Flask('test1')\n\n@app.route('/data')\ndef data(num_list):\n    return flask.jsonify({'numbers': num_list})\n\ndef eval_app(app, data_fn, num_arr):\n    with app.test_request_context():\n        response = data_fn(num_arr)\n        return response.get_data(as_text=True)\n\nclass MyCustomJSONHandler(flask.json.provider.DefaultJSONProvider):\n    def default(self, obj: object) -> object:\n        if isinstance(obj, np.ndarray):\n            res = obj.T.copy().flatten().tolist()\n            return res\n        return super().default(obj)\n\napp.json_provider_class = MyCustomJSONHandler\napp.json = app.json_provider_class(app)\n"}
{"example_id": "167", "answer": "import flask\nimport werkzeug\nimport numpy as np\n\nerror404 = werkzeug.exceptions.NotFound\n\ndef stack_and_save(arr_list: list[np.ndarray],base_path : str,sub_path : str, casting_policy: str, out_dtype: type) -> tuple[str, np.ndarray]:\n    # Attempt to join the base path and sub path.\n    # If the joined path is outside the base path, raise a 404 error.\n    # stack the arrays in arr_list with the casting policy and the out_dtype.\n    # if the out_dtype is not compatible with the casting policy, raise a TypeError\n    # and out_dtype could be np.float32 or np.float64\n    # casting policy could be safe or unsafe\n    # Return the joined path and the stacked array to be saved \n    joined = flask.safe_join(base_path, sub_path)\n    casted_list = []\n\n    for arr in arr_list:\n        if not np.can_cast(arr.dtype, out_dtype, casting=casting_policy):\n            raise TypeError('Cannot cast array')\n        casted_list.append(arr.astype(out_dtype, copy=False))\n    \n    stacked = np.vstack(casted_list)\n    return joined, stacked\n"}
{"example_id": "168", "answer": "import flask\nimport werkzeug\nimport numpy as np\n\nerror404 = werkzeug.exceptions.NotFound\n\ndef stack_and_save(arr_list: list[np.ndarray],base_path : str,sub_path : str, casting_policy: str, out_dtype: type) -> tuple[str, np.ndarray]:\n    # Attempt to join the base path and sub path.\n    # If the joined path is outside the base path, raise a 404 error.\n    # stack the arrays in arr_list with the casting policy and the out_dtype.\n    # if the out_dtype is not compatible with the casting policy, raise a TypeError\n    # and out_dtype could be np.float32 or np.float64\n    # casting policy could be safe or unsafe\n    # Return the joined path and the stacked array to be saved \n    joined = werkzeug.utils.safe_join(base_path, sub_path)\n    if joined is None:\n        raise error404\n    stacked = np.vstack(arr_list,casting=casting_policy,dtype=out_dtype)\n    return joined, stacked\n"}
{"example_id": "169", "answer": "import flask\nimport numpy as np\nfrom scipy import linalg\n\napp = flask.Flask('test1')\n@app.route('/data')\ndef data(num_list):\n    return flask.jsonify({'numbers': num_list})\ndef eval_app(app, data_fn, num_arr):\n    with app.test_request_context():\n        response = data_fn(num_arr)\n        return response.get_data(as_text=True)\n\nclass MyCustomJSONHandler(flask.json.provider.DefaultJSONProvider):\n    def default(self, obj):\n        if isinstance(obj, np.ndarray) and len(obj.shape)==3 and obj.shape[-1]==obj.shape[-2] : \n\n            res = linalg.det(obj)\n            return res.tolist()\n        return super().default(obj)\n\napp.json_provider_class = MyCustomJSONHandler\napp.json = app.json_provider_class(app) "}
{"example_id": "170", "answer": "import flask\nimport json\nimport numpy as np\nfrom scipy import linalg\n\napp = flask.Flask('test1')\n@app.route('/data')\ndef data(num_arr):\n    return flask.jsonify({'numbers': num_arr})\n\ndef eval(app, data_fn, num_arr):\n    with app.test_request_context():\n        response = data_fn(num_arr)\n        return response.get_data(as_text=False)\n\nclass MyCustomJSONHandler(json.JSONEncoder):\n    def default(self, obj: object) -> object:\n        if isinstance(obj, np.ndarray) and len(obj.shape)==3 and obj.shape[-1]==obj.shape[-2] :\n            res = np.zeros(obj.shape[0])\n            for i in range(obj.shape[0]):\n                res[i] = linalg.det(obj[i])\n            return res.tolist()\n        return super().default(obj)\n\napp.json_encoder = MyCustomJSONHandler\n"}
{"example_id": "171", "answer": "import flask\nimport numpy as np\nfrom scipy.stats import hmean\n\napp = flask.Flask('test1')\n\n@app.route('/data')\ndef data(num_list):\n    return flask.jsonify({'numbers': num_list})\n\ndef eval_app(app, data_fn, num_arr):\n    with app.test_request_context():\n        response = data_fn(num_arr)\n        return response.get_data(as_text=True)\n\nclass MyCustomJSONHandler(flask.json.provider.DefaultJSONProvider):\n    def default(self, obj: object) -> object:\n        if isinstance(obj, np.ndarray):\n            res = hmean(obj,axis=1).tolist()\n            return res\n        return super().default(obj)\n\napp.json_provider_class = MyCustomJSONHandler\napp.json = app.json_provider_class(app)\n"}
{"example_id": "172", "answer": "import flask\nimport json\nimport numpy as np\nfrom scipy.stats import hmean\n\napp = flask.Flask('test1')\n@app.route('/data')\ndef data(num_arr):\n    return flask.jsonify({'numbers': num_arr})\n\ndef eval(app, data_fn, num_arr):\n    with app.test_request_context():\n        response = data_fn(num_arr)\n        return response.get_data(as_text=False)\n\nclass MyCustomJSONHandler(json.JSONEncoder):\n    def default(self, obj: object) -> object:\n        if isinstance(obj, np.ndarray):\n            res = np.zeros((obj.shape[0],1))\n            for i_arr in range(obj.shape[0]):\n                if np.isnan(obj[i_arr]).any():\n                    res[i_arr] = np.nan\n                else:\n                    res[i_arr]  = hmean(obj[i_arr])\n            res = res.flatten().tolist()\n            return res\n        return super().default(obj)\n\napp.json_encoder = MyCustomJSONHandler\n"}
{"example_id": "173", "answer": "import flask\nimport werkzeug\nfrom scipy import linalg\nimport numpy as np\n\nerror404 = werkzeug.exceptions.NotFound\n\ndef save_exponential(A: np.ndarray, base_path: str, sub_path: str) -> tuple[str, np.ndarray]:\n    # Attempt to join the base path and sub path.\n    # If the joined path is outside the base path, raise a 404 error.\n    # compute the exponential of the batched matrices (m, m) in A (n,m,m)\n    # return the save_path and the exponential of the matrices\n    \n    joined = werkzeug.utils.safe_join(base_path, sub_path)\n    if joined is None:\n        raise error404\n    output = linalg.expm(A)\n    return joined, output\n"}
{"example_id": "174", "answer": "import flask\nimport werkzeug\nfrom scipy import linalg\nimport numpy as np\n\nerror404 = werkzeug.exceptions.NotFound\n\ndef save_exponential(A: np.ndarray, base_path: str, sub_path: str) -> tuple[str, np.ndarray]:\n    # Attempt to join the base path and sub path.\n    # If the joined path is outside the base path, raise a 404 error.\n    # compute the exponential of the batched matrices (m, m) in A (n,m,m)\n    # return the save_path and the exponential of the matrices\n    \n    joined = flask.safe_join(base_path, sub_path)\n    output = np.zeros(A.shape)\n    for i in range(A.shape[0]):\n        output[i] = linalg.expm(A[i])\n    return joined, output\n"}
{"example_id": "175", "answer": "from typing import List\nfrom sympy.stats import Die, sample\nimport sympy.stats.rv \n\ndef custom_generateRandomSampleDice(dice: sympy.stats.rv.RandomSymbol, X: int) -> List[int]:\n    return [sample(dice) for i in range(X)]"}
{"example_id": "176", "answer": "import sympy\nfrom sympy.matrices.expressions.fourier import DFT\n\ndef custom_computeDFT(n: int) -> sympy.ImmutableDenseMatrix:\n    return DFT(n).as_explicit()"}
{"example_id": "177", "answer": "from typing import Tuple\nfrom sympy import laplace_transform, symbols, eye\nimport sympy\n\ndef custom_laplace_transform(t: sympy.Symbol, z: sympy.Symbol) -> Tuple[sympy.Matrix, sympy.Expr, bool]:\n    return laplace_transform(eye(2), t, z, legacy_matrix=False)"}
{"example_id": "178", "answer": "import sympy.physics.quantum\nimport sympy\ndef custom_trace(n: int) -> sympy.physics.quantum.trace.Tr:\n    return sympy.physics.quantum.trace.Tr(n)"}
{"example_id": "179", "answer": "import sympy\n\ndef custom_preorder_traversal(expr: sympy.Expr) -> sympy.core.basic.preorder_traversal:\n    return sympy.preorder_traversal(expr)"}
{"example_id": "180", "answer": "from sympy.parsing.mathematica import parse_mathematica\nfrom sympy import Function, Max, Min\nimport sympy\n\ndef custom_parse_mathematica(expr : str) -> int:\n    return parse_mathematica(expr).replace(Function(\"F\"), lambda *x: Max(*x)*Min(*x))"}
{"example_id": "181", "answer": "from sympy.physics.mechanics import Body, PinJoint\nimport sympy.physics.mechanics\n\ndef custom_pinJoint(parent: sympy.physics.mechanics.Body, child: sympy.physics.mechanics.Body) -> sympy.physics.mechanics.PinJoint:\n    return PinJoint('pin', parent, child, parent_point=parent.frame.x,child_point=-child.frame.x)"}
{"example_id": "182", "answer": "from sympy.physics.mechanics import Body, PinJoint\nimport sympy.physics.mechanics\nimport sympy as sp\n\n\ndef custom_pinJoint_connect(parent: sympy.physics.mechanics.Body, child: sympy.physics.mechanics.Body) -> sympy.physics.mechanics.PinJoint:\n    return PinJoint('pin', parent, child, parent_point=parent.frame.x,child_point=-child.frame.x)"}
{"example_id": "183", "answer": "from sympy import *\n\ndef custom_check_carmichael(n: int) -> bool:\n    return is_carmichael(n)"}
{"example_id": "184", "answer": "from sympy import *\n\ndef custom_function(n: int, k : int) -> int:\n    return divisor_sigma(n, k)"}
{"example_id": "185", "answer": "from sympy import GF\nfrom sympy.polys.domains.finitefield import FiniteField\n\n\ndef custom_function(K: FiniteField, a: FiniteField) -> int:\n    return K.to_int(a)"}
{"example_id": "186", "answer": "from sympy import symbols\nfrom sympy.physics.mechanics import ReferenceFrame\nimport sympy.physics.vector\n\ndef custom_generateInertia(N: sympy.physics.vector.frame.ReferenceFrame, Ixx: sympy.Symbol, Iyy: sympy.Symbol, Izz: sympy.Symbol) -> sympy.physics.vector.dyadic.Dyadic:\n    from sympy.physics.mechanics import inertia\n    return inertia(N, Ixx, Iyy, Izz)"}
{"example_id": "187", "answer": "from sympy import *\nimport sympy\n\ndef custom_function(eq: sympy.Equality) -> sympy.Expr:\n    return eq.lhs - eq.rhs"}
{"example_id": "188", "answer": "from sympy import symbols, Poly\nimport sympy\n\ndef custom_generatePolyList(poly: sympy.Poly) -> list[int]:\n    return poly.rep.to_list()"}
{"example_id": "189", "answer": "from sympy import symbols\nfrom sympy.physics.mechanics import (\nParticle, PinJoint, PrismaticJoint, RigidBody)\nimport sympy\nimport sympy.physics.mechanics\n\ndef custom_motion(wall: sympy.physics.mechanics.RigidBody, slider: sympy.physics.mechanics.PrismaticJoint, pin: sympy.physics.mechanics.PinJoint) -> sympy.Matrix:\n    from sympy.physics.mechanics import System\n    system = System.from_newtonian(wall)\n    system.add_joints(slider, pin)\n    return system.form_eoms()\n"}
{"example_id": "190", "answer": "from sympy.physics.mechanics import *\nimport sympy.physics.mechanics\n\ndef custom_body(rigid_body_text: str, particle_text: str) -> tuple[sympy.physics.mechanics.RigidBody, sympy.physics.mechanics.Particle]:\n    return RigidBody(rigid_body_text), Particle(particle_text)"}
{"example_id": "191", "answer": "from sympy import Indexed, Symbol\nimport sympy\nfrom typing import Set\n\ndef custom_symbol(index: Indexed) -> set[Symbol]:\n    return index.free_symbols"}
{"example_id": "192", "answer": "from sympy import Matrix\nimport sympy\n\ndef custom_create_matrix(first: sympy.Matrix, second: sympy.Matrix) -> list[int]:\n    return Matrix([first, second])"}
{"example_id": "193", "answer": "from sympy import Matrix\nimport sympy\n\ndef custom_function(matrix: sympy.Matrix) -> list[int]:\n    return matrix.flat()"}
{"example_id": "194", "answer": "from sympy import Matrix\nimport sympy\n\ndef custom_function(matrix: sympy.Matrix) -> list[int]:\n    return matrix.todok()"}
{"example_id": "195", "answer": "import sympy\n\ndef custom_bottom_up(expr: sympy.Expr) -> int:\n    return sympy.bottom_up(expr, lambda x: x.doit())"}
{"example_id": "196", "answer": "import sympy\n\n\ndef custom_use(expr: sympy.Expr) -> int:\n    return sympy.use(expr, lambda x: x.doit())"}
{"example_id": "197", "answer": "import sympy\n\ndef custom_is_perfect_square(n: int) -> bool:\n    return sympy.ntheory.primetest.is_square(n)"}
{"example_id": "198", "answer": "import sympy\n\ndef custom_is_prime(n: int) -> bool:\n    return sympy.isprime(n)"}
{"example_id": "199", "answer": "import sympy\n\ndef custom_divides(n: int, p: int) -> bool:\n    return n % p == 0"}
{"example_id": "200", "answer": "from sympy import Matrix, symbols, Array\nimport sympy\n\ndef custom_array_to_matrix(array: sympy.Array) -> sympy.Matrix:\n    from sympy.tensor.array.expressions.from_array_to_matrix import convert_array_to_matrix\n    return convert_array_to_matrix(array)\n"}
{"example_id": "201", "answer": "import sympy\n\ndef custom_jacobi_symbols(a: int, n: int) -> int:\n    return sympy.jacobi_symbol(a, n)"}
{"example_id": "202", "answer": "import sympy\n\ndef custom_npartitions(n: int) -> int:\n    return sympy.functions.combinatorial.numbers.partition(n)"}
{"example_id": "203", "answer": "import sympy\n\ndef custom_primefactors(n: int) -> int: \n    return sympy.primeomega(n)"}
{"example_id": "204", "answer": "import sympy\n\ndef custom_prime_counting(n: int) -> int:\n return sympy.primepi(n)\n"}
{"example_id": "205", "answer": "import sympy\n\ndef custom_totient(n: int) -> int:\n    return sympy.totient(n)"}
{"example_id": "206", "answer": "import sympy\n\ndef custom_mobius(n: int) -> int:\n    return sympy.mobius(n)"}
{"example_id": "207", "answer": "import sympy\n\ndef custom_legendre(a: int, n: int) -> int:\n    return sympy.legendre_symbol(a, n)"}
{"example_id": "208", "answer": "import seaborn as sns\nimport pandas as pd\nfrom matplotlib.axes import Axes\n\ndef custom_pointplot(data: pd.DataFrame) -> Axes:\n    return sns.pointplot(x='x', y='y', data=data, markers=\"o\", linestyles=\"none\")"}
{"example_id": "209", "answer": "import seaborn as sns\nimport pandas as pd\nfrom matplotlib.axes import Axes\n\ndef custom_pointplot(data: pd.DataFrame) -> Axes:\n    return sns.pointplot(x='x', y='y', data=data, err_kws={\"linewidth\": 2})"}
{"example_id": "210", "answer": "import seaborn as sns\nimport pandas as pd\nfrom matplotlib.axes import Axes\n\ndef custom_violinplot(data: pd.DataFrame) -> Axes:\n    return sns.violinplot(x='x', y='y', data=data, bw_adjust=1.5)"}
{"example_id": "211", "answer": "import seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.axes import Axes\n\ndef custom_violinplot(data: pd.DataFrame) -> Axes:\n    return sns.violinplot(x='x', y='y', data=data, bw_method=\"scott\")"}
{"example_id": "212", "answer": "import seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.axes import Axes\n\n\ndef custom_barplot(data: pd.DataFrame) -> Axes:\n    return sns.barplot(x='x', y='y', data=data, err_kws={'color': 'red', 'linewidth': 2})"}
{"example_id": "213", "answer": "import seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.axes import Axes\n\n\ndef custom_boxenplot(data: pd.DataFrame) -> Axes:\n    return sns.boxenplot(x='x', y='y', data=data, width_method='exponential')"}
{"example_id": "214", "answer": "import seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom matplotlib.axes import Axes\n\n\ndef custom_set_axis_labels(data: pd.DataFrame) -> Axes:\n    ax = sns.scatterplot(x='x', y='y', data=data)\n    ax.set(xlabel=\"My X Label\", ylabel=\"My Y Label\")\n    return ax"}
{"example_id": "215", "answer": "import numpy as np\n\n\ndef custom_iqr(data: np.ndarray) -> float:\n    from scipy.stats import iqr\n    return iqr(data)\n"}
{"example_id": "216", "answer": "import time\nimport mitmproxy.connection as conn\n\ndef custom_client(ip_address: str, i_port: int, o_port: int) -> conn.Client:\n    return  conn.Client(\n    peername=(ip_address, i_port),\n    sockname=(ip_address, o_port),\n    timestamp_start=time.time()\n)"}
{"example_id": "217", "answer": "import mitmproxy.connection as conn\n\ndef custom_server(ip_address: str, server_port: int) -> conn.Server:\n    return conn.Server(address=(ip_address, server_port))"}
{"example_id": "218", "answer": "import contextlib\n\nclass DummyServerConn:\n    def __init__(self, sockname):\n        self.sockname = sockname\n\nclass ConnectionLogger:\n    pass\n        \n\ndef solution() -> None:\n    def  server_connected(self, server_conn: DummyServerConn) -> None:\n        print(server_conn.sockname)\n\n    ConnectionLogger.server_connected = server_connected"}
{"example_id": "219", "answer": "import contextlib\n\nclass DummyServerConn:\n    def __init__(self, sockname):\n        self.sockname = sockname\n\nclass ConnectionLogger:\n    pass\n        \n\ndef solution() -> None:\n    def  server_connect(self, server_conn: DummyServerConn) -> None:\n        print(server_conn.sockname)\n\n    ConnectionLogger.server_connect = server_connect\n"}
{"example_id": "220", "answer": "import contextlib\n\nclass DummyServerConn:\n    def __init__(self, sockname):\n        self.sockname = sockname\n\nclass ConnectionLogger:\n    pass\n        \n\ndef solution() -> None:\n    def server_disconnected(self, server_conn: DummyServerConn) -> None:\n        print(server_conn.sockname)\n\n    ConnectionLogger.server_disconnected = server_disconnected"}
{"example_id": "221", "answer": "import contextlib\n\nclass DummyClientConn:\n    def __init__(self, peername):\n        self.peername = peername\n\nclass ConnectionLogger:\n    pass\n\ndef solution() -> None:\n    def client_connected(self, client_conn: DummyClientConn) -> None:\n        print(client_conn.peername)\n\n    ConnectionLogger.client_connected = client_connected"}
{"example_id": "222", "answer": "import contextlib\n\nclass DummyClientConn:\n    def __init__(self, peername):\n        self.peername = peername\n\nclass ConnectionLogger:\n    pass\n\ndef solution() -> None:\n    def client_disconnected(self, client_conn) -> None:\n        print(client_conn.peername)\n        \n    ConnectionLogger.client_disconnected = client_disconnected"}
{"example_id": "223", "answer": "import contextlib\n\nclass DummyLogEntry:\n    def __init__(self, msg):\n        self.msg = msg\n\nclass MyAddon:\n    pass\n\ndef solution() -> None:\n    def add_log(self, entry):\n        print(f\"{entry.msg}\")\n    \n    MyAddon.add_log = add_log"}
{"example_id": "224", "answer": "import types\n\nclass DummyCert:\n    def __init__(self, hostname):\n        self.cert_pem = f\"Dummy certificate for {hostname}\"\n        self.key_pem = f\"Dummy key for {hostname}\"\n\nclass DummyCA:\n    def __init__(self, path):\n        self.path = path\n\n    def get_cert(self, hostname):\n        return DummyCert(hostname)\n\ncerts = types.ModuleType(\"certs\")\ncerts.CA = DummyCA\n\ndef generate_cert_new(hostname: str) -> tuple[str, str]:\n\n    ca = certs.CA(\"dummy/path\")\n    cert_obj = ca.get_cert(hostname)\n    return cert_obj.cert_pem, cert_obj.key_pem"}
{"example_id": "225", "answer": "from mitmproxy.http import Headers\n\ndef custom_function(header_name: bytes, initial_value: bytes) -> Headers:\n    return Headers([(header_name, initial_value)])"}
{"example_id": "226", "answer": "import pytest\n\n@pytest.hookimpl(tryfirst=False)\ndef pytest_runtest_call():\n    pass"}
{"example_id": "227", "answer": "import pytest\n\n@pytest.hookimpl(hookwrapper=True)\ndef pytest_runtest_setup():\n    yield"}
{"example_id": "228", "answer": "import pytest\nimport pathlib\n\n@pytest.hookimpl()\ndef pytest_ignore_collect(collection_path:pathlib.Path):\n    pass"}
{"example_id": "229", "answer": "import pytest\nimport pathlib\n\n@pytest.hookimpl()\ndef pytest_collect_file(file_path:pathlib.Path):\n    pass"}
{"example_id": "230", "answer": "import pytest\nimport pathlib\n\n@pytest.hookimpl()\ndef pytest_pycollect_makemodule(module_path:pathlib.Path):\n    pass"}
{"example_id": "231", "answer": "import pytest\nimport pathlib\n\n@pytest.hookimpl()\ndef pytest_report_header(start_path:pathlib.Path):\n    pass\n"}
{"example_id": "232", "answer": "import pytest\nimport pathlib\n\n@pytest.hookimpl()\ndef pytest_report_collectionfinish(start_path:pathlib.Path):\n    pass"}
{"example_id": "233", "answer": "import pytest\n\nclass CustomItem(pytest.Item):\n    def __init__(self, *, additional_arg, **kwargs):\n        super().__init__(**kwargs)\n        self.additional_arg = additional_arg"}
{"example_id": "234", "answer": "import pytest\n\ndef foo(a, b):\n    return (10 * a - b + 7) // 3\n\n@pytest.mark.parametrize(\n    [\"a\", \"b\", \"result\"],\n    [\n        [1, 2, 5],\n        [2, 3, 8],\n        [5, 3, 18],\n    ],\n)\ndef test_foo(a: int, b: int, result: int) -> None:\n    assert foo(a, b) == result"}
{"example_id": "237", "answer": "from falcon import stream\n\nimport io\nclass DummyRequest:\n    def __init__(self, data: bytes):\n        self.stream = io.BytesIO(data)\n        self.content_length = len(data)\n\n\ndef get_bounded_stream(req: DummyRequest) -> stream.BoundedStream:\n    return stream.BoundedStream(req.stream, req.content_length)"}
{"example_id": "238", "answer": "import falcon\n\n\ndef custom_body(resp: falcon.Response, info: str) -> falcon.Response:\n    resp.text = info\n    return resp"}
{"example_id": "239", "answer": "import falcon\nfrom falcon import HTTPStatus\n\n\ndef custom_body(status: falcon.HTTPStatus, info:str) -> falcon.HTTPStatus:\n    status.text = info\n    return status"}
{"example_id": "240", "answer": "from falcon import Response\n\ndef custom_body_length(resp: Response, info):\n    resp.content_length = len(info)\n    return resp"}
{"example_id": "241", "answer": "from falcon import Response\nimport falcon\n\ndef custom_data(resp: falcon.Response, info: str) -> str:\n    resp.data = info\n    return resp.render_body()"}
{"example_id": "242", "answer": "import falcon\nfrom falcon import HTTPError\n\n\ndef custom_http_error(title: str, description: str) -> bytes:\n    return HTTPError(falcon.HTTP_400, title, description).to_json()"}
{"example_id": "243", "answer": "from typing import Dict, Any\nimport falcon.testing as testing\n\ndef custom_environ(info: str) -> Dict[str, Any]:\n    return testing.create_environ(root_path=info)"}
{"example_id": "244", "answer": "from falcon.stream import BoundedStream\n\ndef custom_writable(bstream: BoundedStream) -> bool:\n    return bstream.writable()"}
{"example_id": "245", "answer": "import falcon.app_helpers as app_helpers\n\nclass ExampleMiddleware:\n    def process_request(self, req, resp):\n        pass\n\ndef custom_middleware_variable() -> list[ExampleMiddleware]:\n    return [ExampleMiddleware()]"}
{"example_id": "246", "answer": "from typing import Dict, Any\nimport falcon.testing as testing\n\ndef custom_environ(v: str) -> Dict[str, Any]:\n    return testing.create_environ(http_version=v)"}
{"example_id": "247", "answer": "from falcon import Response\nimport falcon\n\ndef custom_append_link(resp: falcon.Response, link: str, rel: str) -> falcon.Response:\n    resp.append_link(link, rel, crossorigin='anonymous')\n    return resp"}
{"example_id": "248", "answer": "import falcon\n\ndef custom_falcons() -> falcon.App:\n    return falcon.App()"}
{"example_id": "249", "answer": "from falcon import Response\nimport falcon\n\n\ndef custom_link(resp: Response, link_rel: str, link_href: str) -> falcon.Response:\n    resp.append_link(link_href, link_rel)\n    return resp"}
{"example_id": "250", "answer": "import json\nfrom falcon import Request\nfrom falcon.testing import create_environ\n\ndef custom_media(req: Request) -> dict[str, str]:\n    return req.get_media()"}
{"example_id": "251", "answer": "from typing import NoReturn\nimport falcon \n\n\ndef raise_too_large_error(error_message: str) -> NoReturn:\n    raise falcon.HTTPPayloadTooLarge(error_message)"}
{"example_id": "252", "answer": "from falcon.uri import parse_query_string\n\n\ndef custom_parse_query(qs : str) -> dict:\n    return parse_query_string(qs, keep_blank=True, csv=False)"}
{"example_id": "253", "answer": "from falcon import Request\n\ndef custom_get_param(req: Request) -> dict[str, str]:\n    return req.get_param_as_json(\"foo\")"}
{"example_id": "254", "answer": "import falcon\nimport logging\nfrom typing import Any, Dict\n\ndef handle_error(req: falcon.Request, resp: falcon.Response, ex: Exception, params: Dict[str, Any]) -> None:\n    req_path = getattr(req, \"path\", \"unknown\")\n    resp.media = {\n        \"error\": str(ex),\n        \"details\": {\n            \"request\": req_path,\n            \"params\": params,\n        }\n    }\n    resp.status = falcon.HTTP_500"}
{"example_id": "255", "answer": "from falcon import Request\n\ndef custom_get_dpr(req: Request) -> int:\n    return req.get_param_as_int(\"dpr\", min_value=0, max_value=3)"}
{"example_id": "256", "answer": "from falcon import Request\nfrom falcon.util.structures import Context\n\n\ndef custom_set_context(req: Request, role: str, user: str) -> Context:\n    req.context.role = role\n    req.context.user = user\n    return req.context\n"}
{"example_id": "257", "answer": "class CustomRouter:\n    def __init__(self):\n        self.routes = {}\n\n        \ndef solution() -> None:\n    \n    def add_route(self, uri_template, resource, **kwargs):\n        from falcon.routing import map_http_methods\n        method_map = map_http_methods(resource, kwargs.get('fallback', None))\n        self.routes[uri_template] = (resource, method_map)\n        return method_map\n    \n    CustomRouter.add_route = add_route"}
{"example_id": "258", "answer": "import asyncio\nimport os\nimport signal\nfrom typing import Callable\n\ndef custom_add_callback_from_signal(callback: Callable[[], None], signum: int) -> None:\n    loop = asyncio.get_event_loop()\n    loop.add_signal_handler(signum, callback)\n"}
{"example_id": "259", "answer": "import tornado.wsgi\nimport tornado.httpserver\nimport tornado.ioloop\nimport tornado.httpclient\nimport concurrent.futures\nimport socket\n\nfrom typing import Callable, Dict, List, Any, Iterable\n\nWSGIAppType = Callable[\n    [Dict[str, Any], Callable[[str, List[tuple[str, str]]], None]],\n    Iterable[bytes]\n]\n\n# A simple WSGI application that returns \"Hello World\"\ndef simple_wsgi_app(environ, start_response):\n    status = \"200 OK\"\n    headers = [(\"Content-Type\", \"text/plain\")]\n    start_response(status, headers)\n    return [b\"Hello World\"]\n\ndef find_free_port():\n    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:\n        sock.bind((\"\", 0))\n        return sock.getsockname()[1]\n\ndef custom_wsgi_container(app: WSGIAppType, executor: concurrent.futures.Executor) -> tornado.wsgi.WSGIContainer:\n\n    return tornado.wsgi.WSGIContainer(app, executor=executor)"}
{"example_id": "260", "answer": "import tornado.ioloop\nimport tornado.web\nimport tornado.httpserver\nimport tornado.websocket\nimport tornado.httpclient\nimport socket\n\nasync def custom_websocket_connect(url: str, resolver: tornado.netutil.Resolver ) -> tornado.websocket.WebSocketClientConnection:\n\n    return await tornado.websocket.websocket_connect(url, resolver=resolver)"}
{"example_id": "261", "answer": "import tornado.web\nimport tornado.ioloop\nimport tornado.httpserver\nimport tornado.httpclient\nimport socket\n\nCOOKIE_SECRET = \"MY_SECRET_KEY\"\n\nclass GetCookieHandler(tornado.web.RequestHandler):\n    def get(self) -> None:\n        cookie_value =self.get_signed_cookie(\"mycookie\")\n        if cookie_value:\n            self.write(cookie_value.decode())\n"}
{"example_id": "262", "answer": "import tornado.web\nimport tornado.ioloop\nimport tornado.httpserver\nimport tornado.httpclient\nimport socket\n\nCOOKIE_SECRET = \"MY_SECRET_KEY\"\n\nclass SetCookieHandler(tornado.web.RequestHandler):\n    def get(self) -> None:\n        self.set_signed_cookie(\"mycookie\", \"testvalue\")\n        self.write(\"Cookie set\")"}
{"example_id": "263", "answer": "import asyncio\nimport tornado.auth\nimport asyncio\n\nclass DummyAuth(tornado.auth.OAuth2Mixin):\n    async def async_get_user_info(self, access_token: str) -> dict[str, str]:\n        return {\"user\": \"test\", \"token\": access_token}"}
{"example_id": "264", "answer": "import tornado.httputil\n\nclass DummyConnection:\n    def __init__(self):\n        self.buffer = []\n\n    def write(self, chunk):\n        self.buffer.append(chunk)\n\nreq = tornado.httputil.HTTPServerRequest(method=\"GET\", uri=\"/\")\nreq.connection = DummyConnection()\n\ndef custom_write(request: tornado.httputil.HTTPServerRequest, text: str) -> list[str]:\n    request.connection.write(text)\n    return request.connection.buffer"}
{"example_id": "265", "answer": "import tornado.ioloop\n\ndef custom_get_ioloop() -> tornado.ioloop.IOLoop:\n    return tornado.ioloop.IOLoop.current()"}
{"example_id": "266", "answer": "import plotly.graph_objects as go\n\n\ndef custom_fig(x_data: list[str], y_data: list[int]) -> go.Figure:\n    return go.Figure(data=[go.Bar(x=x_data,y=y_data,orientation=\"v\")])"}
{"example_id": "267", "answer": "import plotly.graph_objects as go\n\ndef custom_fig(fig: go.Figure) -> go.Figure:\n    return fig.add_annotation(\n        x=0.5,\n        y=0.5,\n        text=\"Example Annotation\",\n        xref=\"paper\",\n        yref=\"paper\",\n        showarrow=False\n    )"}
{"example_id": "268", "answer": "import plotly.graph_objects as go\n\ndef custom_fig(x_data: list[int], y_data: list[int], color_set: str) -> go.Figure:\n    return go.Figure(data=go.Scatter(\n    x=x_data,\n    y=y_data,\n    error_y=dict(\n        color=color_set\n    )\n))"}
{"example_id": "269", "answer": "import plotly.graph_objects as go\n\ndef custom_fig(fig: go.Figure) -> go.Figure:\n    return fig.update_layout(\n    scene_camera=dict(\n        eye=dict(x=1.25, y=1.25, z=1.25)\n    )\n)"}
{"example_id": "270", "answer": "import plotly\nimport plotly.graph_objects as go\n\ndef custom_make_subplots(rows: int, cols: int) -> go.Figure:\n    return plotly.subplots.make_subplots(rows=rows, cols=cols)"}
{"example_id": "271", "answer": "import plotly\nimport plotly.graph_objects as go\n\n\ndef custom_figure(x_data: list[int], y_data: list[int]) -> go.Figure:\n    import plotly.graph_objects\n    fig = plotly.graph_objects.Figure()\n    fig.add_trace(plotly.graph_objects.Scatter(x=x_data, y=y_data))\n    return fig"}
{"example_id": "272", "answer": "import plotly\ndef custom_chart_studio_usage() -> bool:\n    import chart_studio.plotly\n    return hasattr(chart_studio.plotly, \"plot\")"}
{"example_id": "273", "answer": "import plotly\ndef custom_api_usage() -> str:\n    import chart_studio.api\n    return chart_studio.api.__name__"}
{"example_id": "274", "answer": "import plotly.graph_objs as go\n\ndef custom_scatter(custom_color: str) -> go.Figure:\n    return go.Figure(data=[go.Scatter(x=[0],y=[0],marker=go.scatter.Marker(color=custom_color)) ])"}
{"example_id": "275", "answer": "import numpy as np\nimport librosa\nfrom scipy.spatial.distance import cdist\n\ndef compute_dtw(X: np.ndarray, Y: np.ndarray) -> np.ndarray:\n    \n\n\n    dist_matrix = cdist(X.T, Y.T, metric='euclidean')\n    return librosa.dtw(C=dist_matrix, metric='invalid')[0]"}
{"example_id": "276", "answer": "import numpy as np\nimport librosa\nfrom scipy.spatial.distance import cdist\n\ndef compute_dtw(X: np.ndarray, Y: np.ndarray) -> np.ndarray:\n    \n\n\n    dist_matrix = cdist(X.T, Y.T, metric='euclidean')\n    return librosa.sequence.dtw(C=dist_matrix, metric='invalid')[0]"}
{"example_id": "277", "answer": "import librosa\nimport numpy as np\n\ndef compute_rms(y: np.ndarray) -> np.float32:\n    \n\n    return librosa.feature.rmse(y=y)"}
{"example_id": "278", "answer": "import librosa\nimport numpy as np\n\ndef compute_rms(y: np.ndarray) -> np.float32:\n    \n\n    return librosa.feature.rms(y=y)"}
{"example_id": "279", "answer": "import librosa\nimport numpy as np\n\ndef compute_fill_diagonal(mut_x: np.ndarray, radius: float) -> np.ndarray:\n    \n\n    return librosa.fill_off_diagonal(mut_x,  radius)"}
{"example_id": "280", "answer": "import librosa\nimport numpy as np\n\ndef compute_fill_diagonal(mut_x: np.ndarray, radius: float) -> np.ndarray:\n    \n\n    return librosa.util.fill_off_diagonal(mut_x,  radius)"}
{"example_id": "281", "answer": "import librosa\nimport numpy as np\nfrom typing import Tuple\n\ndef compute_extraction(y: np.ndarray, sr: int) -> Tuple[np.ndarray, bool]:\n    \n\n    M_from_y = librosa.feature.melspectrogram(y=y, sr=sr) \n    return M_from_y, M_from_y.dtype == np.float64"}
{"example_id": "282", "answer": "import librosa\nimport numpy as np\nfrom typing import Tuple\n\ndef compute_extraction(y: np.ndarray, sr: int) -> Tuple[np.ndarray, bool]:\n    \n\n    M_from_y = librosa.feature.melspectrogram(y=y, sr=sr) \n    return M_from_y, M_from_y.dtype == np.float32\n"}
{"example_id": "283", "answer": "import librosa\nimport numpy as np\nimport soundfile as sf \n\n\n# Save the stream in variable stream. Save each stream block with the array stream_blocks\ndef compute_stream(filename, y, sr, n_fft, hop_length):\n    stream_blocks = []\n\n    stream = sf.blocks(filename, blocksize=n_fft + 15 * hop_length, overlap=n_fft - hop_length,  fill_value=0)\n\n    for c, block in enumerate(stream):\n        y = librosa.to_mono(block.T)\n        D = librosa.stft(y, n_fft=n_fft, hop_length=hop_length, center=False)\n        stream_blocks.append(D)\n\n    return stream, stream_blocks"}
{"example_id": "284", "answer": "import librosa\nimport numpy as np\n\n# Save the stream in variable stream. Save each stream block with the array stream_blocks\ndef compute_stream(y, sr, n_fft, hop_length):\n    stream_blocks = []\n\n\n    stream =  librosa.stream(filename, block_length=16,\n                        frame_length=n_fft,\n                        hop_length=hop_length,\n                        mono=True,\n                        fill_value=0)\n\n    for c, y_block in enumerate(stream):\n        stream_blocks.append(librosa.stft(y_block, n_fft=n_fft, hop_length=hop_length, center=False))\n    return stream, stream_blocks"}
{"example_id": "285", "answer": "import librosa\nimport numpy as np\nfrom librosa import istft, stft\nfrom typing import Union, Optional\n\nDTypeLike = Union[np.dtype, type]\n\ndef compute_griffinlim(y: np.ndarray, sr: int, S: np.ndarray, random_state: int, n_iter: int, hop_length: Optional[int], win_length: Optional[int], window: str, center: bool, dtype: DTypeLike, length: Optional[int], pad_mode: str, n_fft: int) -> np.ndarray:\n    \"\"\"\n    Compute waveform from a linear scale magnitude spectrogram using the Griffin-Lim transformation.\n\n    Parameters:\n    y: Audio timeseries.\n    sr: Sampling rate.\n    S: short-time Fourier transform magnitude matrix.\n    random_state: Random state for the random number generator.\n    n_iter: Number of iterations.\n    hop_length: Hop length.\n    win_length: Window length.\n    window: Window function.\n    center: If True, the signal y is padded so that frame t is centered at y[t * hop_length]. If False, then frame t begins at y[t * hop_length].\n    dtype: Data type of the output.\n    length: Length of the output signal.\n    pad_mode: Padding mode.\n    n_fft: FFT size.\n\n    Returns:\n        The Griffin-Lim waveform.        \n    \"\"\"\n    rng = np.random.RandomState(seed=random_state)\n\n\n    angles = np.exp(2j * np.pi * rng.rand(*S.shape))\n    \n    rebuilt = 0.\n    \n    for _ in range(n_iter):\n        tprev = rebuilt\n    \n        inverse = istft(S * angles, hop_length=hop_length, win_length=win_length,\n        window=window, center=center, dtype=dtype, length=length)\n    \n        rebuilt = stft(inverse, n_fft=n_fft, hop_length=hop_length,\n        win_length=win_length, window=window, center=center,\n        pad_mode=pad_mode)\n    \n        angles[:] = rebuilt - (momentum / (1 + momentum)) * tprev\n        angles[:] /= np.abs(angles) + 1e-16\n    return istft(S * angles, hop_length=hop_length, win_length=win_length,window=window, center=center, dtype=dtype, length=length)"}
{"example_id": "286", "answer": "import librosa\nimport numpy as np\nfrom librosa import istft, stft\nfrom typing import Union, Optional\n\nDTypeLike = Union[np.dtype, type]\n\ndef compute_griffinlim(y: np.ndarray, sr: int, S: np.ndarray, random_state: int, n_iter: int, hop_length: Optional[int], win_length: Optional[int], window: str, center: bool, dtype: DTypeLike, length: Optional[int], pad_mode: str, n_fft: int) -> np.ndarray:\n    \"\"\"\n    Compute waveform from a linear scale magnitude spectrogram using the Griffin-Lim transformation.\n\n    Parameters:\n        y: Audio timeseries.\n        sr: Sampling rate.\n        S: short-time Fourier transform magnitude matrix.\n        random_state: Random state for the random number generator.\n        n_iter: Number of iterations.\n        hop_length: Hop length.\n        win_length: Window length.\n        window: Window function.\n        center: If True, the signal y is padded so that frame t is centered at y[t * hop_length]. If False, then frame t begins at y[t * hop_length].\n        dtype: Data type of the output.\n        length: Length of the output signal.\n        pad_mode: Padding mode.\n        n_fft: FFT size.\n\n    Returns:\n        The Griffin-Lim waveform.        \n    \"\"\"    \n    rng = np.random.RandomState(seed=random_state)\n\n\n    return librosa.griffinlim(S, n_iter, hop_length, win_length, window, center, dtype, length, pad_mode, momentum, random_state)"}
{"example_id": "287", "answer": "import librosa\nimport numpy as np\n\ndef compute_lpc_coef(y: np.ndarray, sr: int, order: int) -> np.ndarray:\n    \"\"\"\n    Compute the Linear Prediction Coefficients of an audio signal.\n\n    Parameters:\n        y: The audio signal.\n        sr: The sampling rate of the audio signal in Hertz.\n        order: Order of the linear filter.\n\n    Returns:\n        LP prediction error coefficients, i.e. filter denominator polynomial.\n    \"\"\"\n    \n    \n    dtype = y.dtype.type\n    ar_coeffs = np.zeros(order+1, dtype=dtype)\n    ar_coeffs[0] = dtype(1)\n    ar_coeffs_prev = np.zeros(order+1, dtype=dtype)\n    ar_coeffs_prev[0] = dtype(1)\n    fwd_pred_error = y[1:]\n    bwd_pred_error = y[:-1]\n    den = np.dot(fwd_pred_error, fwd_pred_error) \\\n          + np.dot(bwd_pred_error, bwd_pred_error)\n    for i in range(order):\n        if den <= 0:\n            raise FloatingPointError('numerical error, input ill-conditioned?')\n        reflect_coeff = dtype(-2) * np.dot(bwd_pred_error, fwd_pred_error) / dtype(den)\n        ar_coeffs_prev, ar_coeffs = ar_coeffs, ar_coeffs_prev\n        for j in range(1, i + 2):\n            ar_coeffs[j] = ar_coeffs_prev[j] + reflect_coeff * ar_coeffs_prev[i - j + 1]\n        fwd_pred_error_tmp = fwd_pred_error\n        fwd_pred_error = fwd_pred_error + reflect_coeff * bwd_pred_error\n        bwd_pred_error = bwd_pred_error + reflect_coeff * fwd_pred_error_tmp\n        q = dtype(1) - reflect_coeff**2\n        den = q*den - bwd_pred_error[-1]**2 - fwd_pred_error[0]**2\n        fwd_pred_error = fwd_pred_error[1:]\n        bwd_pred_error = bwd_pred_error[:-1]\n    return ar_coeffs"}
{"example_id": "288", "answer": "import librosa\nimport numpy as np\n\ndef compute_lpc_coef(y: np.ndarray, sr: int, order: int) -> np.ndarray:\n    \"\"\"\n    Compute the Linear Prediction Coefficients of an audio signal.\n\n    Parameters:\n        y: The audio signal.\n        sr: The sampling rate of the audio signal in Hertz.\n        order: Order of the linear filter.\n\n    Returns:\n        LP prediction error coefficients, i.e. filter denominator polynomial.\n    \"\"\"\n\n\n    return librosa.lpc(y, order)"}
{"example_id": "289", "answer": "import librosa\nimport numpy as np\nfrom librosa.core.spectrum import stft\n\ndef compute_fourier_tempogram(oenv: np.ndarray, sr: int, hop_length: int) -> np.ndarray:\n    \"\"\"\n    Compute the Fourier tempogram: the short-time Fourier transform of the onset strength envelope.\n\n    Parameters:\n       oenv: The onset strength envelope.\n       sr: The sampling rate of the audio signal in Hertz.\n       hop_length: The number of samples between successive frames.\n\n    Returns:\n       The computed Fourier tempogram.\n    \"\"\"\n\n    return stft(oenv, n_fft=384, hop_length=1, center=True, window=\"hann\")"}
{"example_id": "290", "answer": "import librosa\nimport numpy as np\n\ndef compute_fourier_tempogram(oenv: np.ndarray, sr: int, hop_length: int) -> np.ndarray:\n    \"\"\"\n    Compute the Fourier tempogram: the short-time Fourier transform of the onset strength envelope.\n\n    Parameters:\n       oenv: The onset strength envelope.\n       sr: The sampling rate of the audio signal in Hertz.\n       hop_length: The number of samples between successive frames.\n\n    Returns:\n       The computed Fourier tempogram.\n    \"\"\"\n    \n\n    return librosa.feature.fourier_tempogram(onset_envelope=oenv, sr=sr, hop_length=hop_length)"}
{"example_id": "291", "answer": "import librosa\nimport numpy as np\nfrom librosa.core.spectrum import stft, istft\nfrom typing import Optional\n\n\ndef compute_plp(\n    y: np.ndarray,\n    sr: int,\n    hop_length: int,\n    win_length: int,\n    tempo_min: Optional[float],\n    tempo_max: Optional[float],\n    onset_env: np.ndarray\n) -> np.ndarray:\n    \"\"\"\n    Compute the Predominant Local Pulse (PLP) of an audio signal.\n    \n    Parameters:\n        y: The audio signal.\n        sr: The sampling rate of the audio signal in Hertz.\n        hop_length: The number of samples between successive frames.\n        win_length: The length (in samples) of the analysis window.\n        tempo_min: The minimum tempo (in BPM) for consideration.\n        tempo_max: The maximum tempo (in BPM) for consideration.\n        onset_env: The onset envelope of the audio signal.\n        \n    Returns:\n        The computed PLP (Predominant Local Pulse) values.\n    \"\"\"\n\n\n    ftgram = stft(onset_env, n_fft=win_length, hop_length=1, center=True, window=\"hann\")\n    \n    tempo_frequencies = np.fft.rfftfreq(n=win_length, d=(sr * 60 / float(hop_length)))\n\n    ftmag = np.abs(ftgram)\n    peak_values = ftmag.max(axis=0, keepdims=True)\n    ftgram[ftmag < peak_values] = 0\n\n    ftgram[:] /= peak_values\n\n    pulse = istft(ftgram, hop_length=1, length=len(onset_env))\n\n    np.clip(pulse, 0, None, pulse)\n    return librosa.util.normalize(pulse)"}
{"example_id": "292", "answer": "import librosa\nimport numpy as np\nfrom librosa.core.spectrum import stft, istft\nfrom typing import Optional\n\n\ndef compute_plp(\n    y: np.ndarray,\n    sr: int,\n    hop_length: int,\n    win_length: int,\n    tempo_min: Optional[float],\n    tempo_max: Optional[float],\n    onset_env: np.ndarray\n) -> np.ndarray:\n    \"\"\"\n    Compute the Predominant Local Pulse (PLP) of an audio signal.\n    \n    Parameters:\n        y: The audio signal.\n        sr: The sampling rate of the audio signal in Hertz.\n        hop_length: The number of samples between successive frames.\n        win_length: The length (in samples) of the analysis window.\n        tempo_min: The minimum tempo (in BPM) for consideration.\n        tempo_max: The maximum tempo (in BPM) for consideration.\n        onset_env: The onset envelope of the audio signal.\n        \n    Returns:\n        The computed PLP (Predominant Local Pulse) values.\n    \"\"\"\n\n    return librosa.beat.plp(onset_envelope=onset_env, sr=sr, tempo_min=tempo_min, tempo_max=tempo_max)"}
{"example_id": "293", "answer": "import librosa\nimport numpy as np\n\ndef compute_times_like(y: np.ndarray, sr: int, hop_length: int, D: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute the times vector of a spectrogram.\n\n    Parameters:\n        y: The audio signal.\n        sr: The sampling rate of the audio signal in Hertz.\n        hop_length: The number of samples between successive frames.\n        D: The spectrogram.\n\n    Returns:\n        The computed times vector.\n    \"\"\"\n    \n\n    if np.isscalar(D):\n        frames = np.arange(D) # type: ignore\n    else:\n        frames = np.arange(D.shape[-1]) # type: ignore\n    offset = 0\n    samples = (np.asanyarray(frames) * hop_length + offset).astype(int)\n\n    return np.asanyarray(samples) / float(sr)"}
{"example_id": "294", "answer": "import librosa\nimport numpy as np\n\ndef compute_times_like(y: np.ndarray, sr: int, hop_length: int, D: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute the times vector of a spectrogram.\n\n    Parameters:\n        y: The audio signal.\n        sr: The sampling rate of the audio signal in Hertz.\n        hop_length: The number of samples between successive frames.\n        D: The spectrogram.\n\n    Returns:\n        The computed times vector.\n    \"\"\"\n    \n\n    return librosa.times_like(D, sr=sr)"}
{"example_id": "295", "answer": "import librosa\nimport numpy as np\n\ndef compute_samples_like(y: np.ndarray, sr: int, D: np.ndarray, hop_length: int) -> np.ndarray:\n    \"\"\"\n    Compute the samples vector of a spectrogram.\n\n    Parameters:\n        y: The audio signal.\n        sr: The sampling rate of the audio signal in Hertz.\n        D: The spectrogram.\n    \n    Returns:\n        The computed samples vector.\n    \"\"\"\n    \n    \n    \n\n    if np.isscalar(D):\n        frames = np.arange(D) # type: ignore\n    else:\n        frames = np.arange(D.shape[-1]) # type: ignore\n    offset = 0\n    return (np.asanyarray(frames) * hop_length + offset).astype(int)\n "}
{"example_id": "296", "answer": "import librosa\nimport numpy as np\n\ndef compute_samples_like(y: np.ndarray, sr: int, D: np.ndarray, hop_length: int) -> np.ndarray:\n    \"\"\"\n    Compute the samples vector of a spectrogram.\n\n    Parameters:\n        y: The audio signal.\n        sr: The sampling rate of the audio signal in Hertz.\n        D: The spectrogram.\n    \n    Returns:\n        The computed samples vector.\n    \"\"\"\n    \n\n    return librosa.samples_like(D)"}
{"example_id": "297", "answer": "import librosa\nimport numpy as np\n\ndef compute_tone(frequency: int, sr: int, length: int) -> np.ndarray:\n    \"\"\"\n    Constructs a pure tone (cosine) signal at a given frequency.\n\n    Parameters:\n        frequency: The frequency of the tone in Hz.\n        sr: The sampling rate of the signal in Hz.\n        length: The length of the signal in samples.\n\n    Returns:\n        np.ndarray: The pure tone signal.\n    \"\"\"\n    \n\n    phi = -np.pi * 0.5\n    return np.cos(2 * np.pi * frequency * np.arange(length) / sr + phi)"}
{"example_id": "298", "answer": "import librosa\nimport numpy as np\n\ndef compute_tone(frequency: int, sr: int, length: int) -> np.ndarray:\n    \"\"\"\n    Constructs a pure tone (cosine) signal at a given frequency.\n\n    Parameters:\n        frequency: The frequency of the tone in Hz.\n        sr: The sampling rate of the signal in Hz.\n        length: The length of the signal in samples.\n\n    Returns:\n        np.ndarray: The pure tone signal.\n    \"\"\"\n    \n\n    return librosa.tone(frequency, sr=sr, length=length)"}
{"example_id": "299", "answer": "import librosa\nimport numpy as np\n\ndef compute_chirp(fmin: int, fmax: int, duration: int, sr: int, linear: bool) -> np.ndarray:\n    \"\"\"\n    Constructs a \u201cchirp\u201d or \u201csine-sweep\u201d signal. The chirp sweeps from frequency fmin to fmax (in Hz).\n\n    Parameters:\n        fmin: The minimum frequency of the chirp in Hz.\n        fmax: The maximum frequency of the chirp in Hz.\n        duration: The duration of the chirp in seconds.\n        sr: The sampling rate of the signal in Hz.\n\n    Returns:\n        np.ndarray: The chirp signal.\n    \"\"\"\n    \n\n    import scipy\n    period = 1.0 / sr\n    phi = -np.pi * 0.5\n\n    method = \"linear\" if linear else \"logarithmic\"\n\n    return scipy.signal.chirp(np.arange(int(duration * sr)) / sr, fmin, duration, fmax, method=method, phi=phi / np.pi * 180, )"}
{"example_id": "300", "answer": "import librosa\nimport numpy as np\n\ndef compute_chirp(fmin: int, fmax: int, duration: int, sr: int, linear: bool) -> np.ndarray:\n    \"\"\"\n    Constructs a \u201cchirp\u201d or \u201csine-sweep\u201d signal. The chirp sweeps from frequency fmin to fmax (in Hz).\n\n    Parameters:\n        fmin: The minimum frequency of the chirp in Hz.\n        fmax: The maximum frequency of the chirp in Hz.\n        duration: The duration of the chirp in seconds.\n        sr: The sampling rate of the signal in Hz.\n\n    Returns:\n        np.ndarray: The chirp signal.\n    \"\"\"    \n    \n\n    return librosa.chirp(fmin=fmin, fmax=fmax, duration=duration, sr=sr)"}
{"example_id": "301", "answer": "import librosa\nimport numpy as np\n\ndef compute_shear(E: np.ndarray, factor: int, axis: int) -> np.ndarray:\n    \n\n\n    E_shear = np.empty_like(E)\n    for i in range(E.shape[1]):\n        E_shear[:, i] = np.roll(E[:, i], factor * i)\n    return E_shear\n    "}
{"example_id": "302", "answer": "import librosa\nimport numpy as np\n\ndef compute_shear(E: np.ndarray, factor: int, axis: int) -> np.ndarray:\n    \n\n\n    return librosa.util.shear(E, factor=factor, axis=axis)"}
{"example_id": "303", "answer": "import librosa\nimport numpy as np\n\ndef compute_localmin(x: np.ndarray, axis: int) -> np.ndarray:\n    \n\n    return librosa.util.localmax(-x, axis=axis)"}
{"example_id": "304", "answer": "import librosa\nimport numpy as np\n\ndef compute_localmin(x: np.ndarray, axis: int) -> np.ndarray:\n    \n\n    return librosa.util.localmin(x, axis=axis)"}
{"example_id": "305", "answer": "import librosa\nimport numpy as np\nimport scipy\nfrom typing import Optional\n\ndef compute_yin(sr: int, fmin: int, fmax: int, duration: float, period: float, phi: float, method: str, y: np.ndarray, frame_length: int, center: bool, pad_mode: str, win_length: Optional[int], hop_length: Optional[int], trough_threshold: float) -> np.ndarray:\n    \"\"\"\n    Calculates the fundamental frequency (F0) estimation using the YIN algorithm.\n\n    Parameters:\n        sr: The sampling rate of the audio signal in Hertz.\n        fmin: The minimum frequency to consider in Hz.\n        fmax: The maximum frequency to consider in Hz.\n        duration: The duration of the audio signal in seconds.\n        period: The period of the fundamental frequency in seconds.\n        phi: The phase of the fundamental frequency in radians.\n        method: Interpolation method.\n        y: The audio signal.\n        frame_length: The length of the frame in samples.\n        center: If True, the signal y is padded so that frame t is centered at y[t * hop_length].\n        pad_mode: Padding mode.\n        win_length: Window length.\n        hop_length: Hop length.\n        trough_threshold: Absolute threshold for peak estimation.\n\n    Returns:\n        The estimated fundamental frequency in Hz.\n    \"\"\"\n    \n    \n\n\n    \n    # Set the default window length if it is not already specified.\n    if win_length is None:\n        win_length = frame_length // 2\n\n\n    # Set the default hop if it is not already specified.\n    if hop_length is None:\n        hop_length = frame_length // 4\n\n    # Pad the time series so that frames are centered\n    if center:\n        y = np.pad(y, frame_length // 2, mode=pad_mode)\n\n    # Frame audio.\n    y_frames = librosa.util.frame(y, frame_length=frame_length, hop_length=hop_length)\n\n    # Calculate minimum and maximum periods\n    min_period = max(int(np.floor(sr / fmax)), 1)\n    max_period = min(int(np.ceil(sr / fmin)), frame_length - win_length - 1)\n\n    # Calculate cumulative mean normalized difference function.\n    # Autocorrelation.\n    a = np.fft.rfft(y_frames, frame_length, axis=0)\n    b = np.fft.rfft(y_frames[win_length::-1, :], frame_length, axis=0)\n    acf_frames = np.fft.irfft(a * b, frame_length, axis=0)[win_length:]\n    acf_frames[np.abs(acf_frames) < 1e-6] = 0\n\n    # Energy terms.\n    energy_frames = np.cumsum(y_frames ** 2, axis=0)\n    energy_frames = energy_frames[win_length:, :] - energy_frames[:-win_length, :]\n    energy_frames[np.abs(energy_frames) < 1e-6] = 0\n\n    # Difference function.\n    yin_frames = energy_frames[0, :] + energy_frames - 2 * acf_frames\n\n    # Cumulative mean normalized difference function.\n    yin_numerator = yin_frames[min_period : max_period + 1, :]\n    tau_range = np.arange(1, max_period + 1)[:, None]\n    cumulative_mean = np.cumsum(yin_frames[1 : max_period + 1, :], axis=0) / tau_range\n    yin_denominator = cumulative_mean[min_period - 1 : max_period, :]\n    yin_frames = yin_numerator / (yin_denominator + librosa.util.tiny(yin_denominator))\n\n    parabolic_shifts = np.zeros_like(yin_frames)\n    parabola_a = (yin_frames[:-2, :] + yin_frames[2:, :] - 2 * yin_frames[1:-1, :]) / 2\n    parabola_b = (yin_frames[2:, :] - yin_frames[:-2, :]) / 2\n    parabolic_shifts[1:-1, :] = -parabola_b / (2 * parabola_a + librosa.util.tiny(parabola_a))\n    parabolic_shifts[np.abs(parabolic_shifts) > 1] = 0\n\n    # Find local minima.\n    is_trough = librosa.util.localmax(-yin_frames, axis=0)\n    is_trough[0, :] = yin_frames[0, :] < yin_frames[1, :]\n\n    # Find minima below peak threshold.\n    is_threshold_trough = np.logical_and(is_trough, yin_frames < trough_threshold)\n\n    # Absolute threshold.\n    # \"The solution we propose is to set an absolute threshold and choose the\n    # smallest value of tau that gives a minimum of d' deeper than\n    # this threshold. If none is found, the global minimum is chosen instead.\"\n    global_min = np.argmin(yin_frames, axis=0)\n    yin_period = np.argmax(is_threshold_trough, axis=0)\n    no_trough_below_threshold = np.all(~is_threshold_trough, axis=0)\n    yin_period[no_trough_below_threshold] = global_min[no_trough_below_threshold]\n\n    # Refine peak by parabolic interpolation.\n    yin_period = (\n     min_period\n     + yin_period\n     + parabolic_shifts[yin_period, range(yin_frames.shape[1])]\n    )\n\n    # Convert period to fundamental frequency.\n    return sr / yin_period"}
{"example_id": "306", "answer": "import librosa\nimport numpy as np\nimport scipy\nfrom typing import Optional\n\ndef compute_yin(sr: int, fmin: int, fmax: int, duration: float, period: float, phi: float, method: str, y: np.ndarray, frame_length: int, center: bool, pad_mode: str, win_length: Optional[int], hop_length: Optional[int], trough_threshold: float) -> np.ndarray:\n    \"\"\"\n    Calculates the fundamental frequency (F0) estimation using the YIN algorithm.\n\n    Parameters:\n        sr: The sampling rate of the audio signal in Hertz.\n        fmin: The minimum frequency to consider in Hz.\n        fmax: The maximum frequency to consider in Hz.\n        duration: The duration of the audio signal in seconds.\n        period: The period of the fundamental frequency in seconds.\n        phi: The phase of the fundamental frequency in radians.\n        method: Interpolation method.\n        y: The audio signal.\n        frame_length: The length of the frame in samples.\n        center: If True, the signal y is padded so that frame t is centered at y[t * hop_length].\n        pad_mode: Padding mode.\n        win_length: Window length.\n        hop_length: Hop length.\n        trough_threshold: Absolute threshold for peak estimation.\n\n    Returns:\n        The estimated fundamental frequency in Hz.\n    \"\"\"\n\n\n    return librosa.yin(y, fmin=fmin, fmax=fmax, sr=sr)"}
{"example_id": "307", "answer": "import librosa\nimport numpy as np\nimport scipy\nfrom typing import Union, Optional, Tuple\n\nDTypeLike = Union[np.dtype, type]\n\ndef compute_pyin(freq: int, sr: int, y: np.ndarray, fmin: int, fmax: int, frame_length: int, center: bool, pad_mode: str, win_length: Optional[int], hop_length: Optional[int], n_thresholds: int, beta_parameters: Tuple[int], boltzmann_parameter: int, resolution: float, max_transition_rate: float, switch_prob: float, no_trough_prob: float, fill_na: DTypeLike) -> np.ndarray:\n    \"\"\"\n    Calculates the fundamental frequency estimation using probabilistic YIN.\n\n    Parameters:\n        freq: The frequency of the fundamental frequency in Hz.\n        sr: The sampling rate of the audio signal in Hertz.\n        y: The audio signal.\n        fmin: The minimum frequency to consider in Hz.\n        fmax: The maximum frequency to consider in Hz.\n        frame_length: The length of the frame in samples.\n        center: If True, the signal y is padded so that frame t is centered at y[t * hop_length].\n        pad_mode: Padding mode.\n        win_length: Window length.\n        hop_length: Hop length.\n        n_thresholds: Number of thresholds.\n        beta_parameters: Beta parameters.\n        boltzmann_parameter: Boltzmann parameter.\n        resolution: Resolution.\n        max_transition_rate: Maximum transition rate.\n        switch_prob: Switch probability.\n        no_trough_prob: No trough probability.\n        fill_na: Fill NA value.\n\n    Returns:\n        Time series of fundamental frequencies in Hertz.\n    \"\"\"\n    \n\n\n    if win_length is None:\n        win_length = frame_length // 2\n    \n    if hop_length is None:\n        hop_length = frame_length // 4\n    \n    if center:\n        y = np.pad(y, frame_length // 2, mode=pad_mode)\n    \n    y_frames = librosa.util.frame(y, frame_length=frame_length, hop_length=hop_length)\n    \n    min_period = max(int(np.floor(sr / fmax)), 1)\n    max_period = min(int(np.ceil(sr / fmin)), frame_length - win_length - 1)\n    \n    a = np.fft.rfft(y_frames, frame_length, axis=0)\n    b = np.fft.rfft(y_frames[win_length::-1, :], frame_length, axis=0)\n    acf_frames = np.fft.irfft(a * b, frame_length, axis=0)[win_length:]\n    acf_frames[np.abs(acf_frames) < 1e-6] = 0\n    \n    energy_frames = np.cumsum(y_frames ** 2, axis=0)\n    energy_frames = energy_frames[win_length:, :] - energy_frames[:-win_length, :]\n    energy_frames[np.abs(energy_frames) < 1e-6] = 0\n    \n    yin_frames = energy_frames[0, :] + energy_frames - 2 * acf_frames\n    \n    yin_numerator = yin_frames[min_period : max_period + 1, :]\n    tau_range = np.arange(1, max_period + 1)[:, None]\n    cumulative_mean = np.cumsum(yin_frames[1 : max_period + 1, :], axis=0) / tau_range\n    yin_denominator = cumulative_mean[min_period - 1 : max_period, :]\n    yin_frames = yin_numerator / (yin_denominator + librosa.util.tiny(yin_denominator))\n    \n    parabolic_shifts = np.zeros_like(yin_frames)\n    parabola_a = (yin_frames[:-2, :] + yin_frames[2:, :] - 2 * yin_frames[1:-1, :]) / 2\n    parabola_b = (yin_frames[2:, :] - yin_frames[:-2, :]) / 2\n    parabolic_shifts[1:-1, :] = -parabola_b / (2 * parabola_a + librosa.util.tiny(parabola_a))\n    parabolic_shifts[np.abs(parabolic_shifts) > 1] = 0\n    \n    thresholds = np.linspace(0, 1, n_thresholds + 1)\n    beta_cdf = scipy.stats.beta.cdf(thresholds, beta_parameters[0], beta_parameters[1])\n    beta_probs = np.diff(beta_cdf)\n    \n    yin_probs = np.zeros_like(yin_frames)\n    for i, yin_frame in enumerate(yin_frames.T):\n        is_trough = librosa.util.localmax(-yin_frame, axis=0)\n        is_trough[0] = yin_frame[0] < yin_frame[1]\n        (trough_index,) = np.nonzero(is_trough)\n    \n        if len(trough_index) == 0:\n            continue\n        \n        trough_heights = yin_frame[trough_index]\n        trough_thresholds = trough_heights[:, None] < thresholds[None, 1:]\n    \n        trough_positions = np.cumsum(trough_thresholds, axis=0) - 1\n        n_troughs = np.count_nonzero(trough_thresholds, axis=0)\n        trough_prior = scipy.stats.boltzmann.pmf(\n            trough_positions, boltzmann_parameter, n_troughs\n        )\n        trough_prior[~trough_thresholds] = 0\n    \n        probs = np.sum(trough_prior * beta_probs, axis=1)\n        global_min = np.argmin(trough_heights)\n        n_thresholds_below_min = np.count_nonzero(~trough_thresholds[global_min, :])\n        probs[global_min] += no_trough_prob * np.sum(\n            beta_probs[:n_thresholds_below_min]\n        )\n    \n        yin_probs[trough_index, i] = probs\n    \n    yin_period, frame_index = np.nonzero(yin_probs)\n    \n\n    period_candidates = min_period + yin_period\n    period_candidates = period_candidates + parabolic_shifts[yin_period, frame_index]\n    f0_candidates = sr / period_candidates\n    \n    n_bins_per_semitone = int(np.ceil(1.0 / resolution))\n    n_pitch_bins = int(np.floor(12 * n_bins_per_semitone * np.log2(fmax / fmin))) + 1\n    \n\n    max_semitones_per_frame = round(max_transition_rate * 12 * hop_length / sr)\n    transition_width = max_semitones_per_frame * n_bins_per_semitone + 1\n\n    transition = librosa.sequence.transition_local(\n        n_pitch_bins, transition_width, window=\"triangle\", wrap=False\n    )\n\n    transition = np.block(\n        [\n            [(1 - switch_prob) * transition, switch_prob * transition],\n            [switch_prob * transition, (1 - switch_prob) * transition],\n        ]\n    )\n    \n    bin_index = 12 * n_bins_per_semitone * np.log2(f0_candidates / fmin)\n    bin_index = np.clip(np.round(bin_index), 0, n_pitch_bins).astype(int)\n    \n    observation_probs = np.zeros((2 * n_pitch_bins, yin_frames.shape[1]))\n    observation_probs[bin_index, frame_index] = yin_probs[yin_period, frame_index]\n    voiced_prob = np.clip(np.sum(observation_probs[:n_pitch_bins, :], axis=0), 0, 1)\n    observation_probs[n_pitch_bins:, :] = (1 - voiced_prob[None, :]) / n_pitch_bins\n    \n    p_init = np.zeros(2 * n_pitch_bins)\n    p_init[n_pitch_bins:] = 1 / n_pitch_bins\n    \n    states = librosa.sequence.viterbi(observation_probs, transition, p_init=p_init)\n    \n    freqs = fmin * 2 ** (np.arange(n_pitch_bins) / (12 * n_bins_per_semitone))\n    f0 = freqs[states % n_pitch_bins]\n    voiced_flag = states < n_pitch_bins\n    if fill_na is not None:\n        f0[~voiced_flag] = fill_na\n    \n    return f0"}
{"example_id": "308", "answer": "import librosa\nimport numpy as np\nimport scipy\nfrom typing import Union, Optional, Tuple\n\nDTypeLike = Union[np.dtype, type]\n\ndef compute_pyin(freq: int, sr: int, y: int, fmin: int, fmax: int, frame_length: int, center: bool, pad_mode: str, win_length: Optional[int], hop_length: Optional[int], n_thresholds: int, beta_parameters: Tuple[int], boltzmann_parameter: int, resolution: float, max_transition_rate: float, switch_prob: float, no_trough_prob: float, fill_na: DTypeLike) -> np.ndarray:\n    \"\"\"\n    Calculates the fundamental frequency estimation using probabilistic YIN.\n\n    Parameters:\n        freq: The frequency of the fundamental frequency in Hz.\n        sr: The sampling rate of the audio signal in Hertz.\n        y: The audio signal.\n        fmin: The minimum frequency to consider in Hz.\n        fmax: The maximum frequency to consider in Hz.\n        frame_length: The length of the frame in samples.\n        center: If True, the signal y is padded so that frame t is centered at y[t * hop_length].\n        pad_mode: Padding mode.\n        win_length: Window length.\n        hop_length: Hop length.\n        n_thresholds: Number of thresholds.\n        beta_parameters: Beta parameters.\n        boltzmann_parameter: Boltzmann parameter.\n        resolution: Resolution.\n        max_transition_rate: Maximum transition rate.\n        switch_prob: Switch probability.\n        no_trough_prob: No trough probability.\n        fill_na: Fill NA value.\n\n    Returns:\n        Time series of fundamental frequencies in Hertz.\n    \"\"\"    \n\n    return librosa.pyin(y, fmin=fmin, fmax=fmax, center=center)[0]"}
{"example_id": "309", "answer": "import librosa\nimport numpy as np\nimport scipy\nfrom typing import Union\n\nDTypeLike = Union[np.dtype, type]\n\n\ndef compute_vqt(y: np.ndarray, sr: int, hop_length: int, fmin: int, n_bins: int, gamma: int, bins_per_octave: int, tuning: float, filter_scale: int, norm: 1, sparsity: float, window: str, scale: bool, pad_mode: str, res_type: str, dtype: DTypeLike) -> np.ndarray:\n    \n   # How many octaves are we dealing with?\n    def dtype_r2c(d, default=np.complex64):\n        \"\"\"Find the complex numpy dtype corresponding to a real dtype.\n\n        This is used to maintain numerical precision and memory footprint\n        when constructing complex arrays from real-valued data\n        (e.g. in a Fourier transform).\n\n        A `float32` (single-precision) type maps to `complex64`,\n        while a `float64` (double-precision) maps to `complex128`.\n\n\n        Parameters\n        ----------\n        d : np.dtype\n            The real-valued dtype to convert to complex.\n            If ``d`` is a complex type already, it will be returned.\n\n        default : np.dtype, optional\n            The default complex target type, if ``d`` does not match a\n            known dtype\n\n        Returns\n        -------\n        d_c : np.dtype\n            The complex dtype\n\n        See Also\n        --------\n        dtype_c2r\n        numpy.dtype\n\n        \"\"\"\n        mapping = {\n            np.dtype(np.float32): np.complex64,\n            np.dtype(np.float64): np.complex128,\n            np.dtype(np.float): np.complex,\n        }\n\n        # If we're given a complex type already, return it\n        dt = np.dtype(d)\n        if dt.kind == \"c\":\n            return dt\n\n        # Otherwise, try to map the dtype.\n        # If no match is found, return the default.\n        return np.dtype(mapping.get(dt, default))\n\n    n_octaves = int(np.ceil(float(n_bins) / bins_per_octave))\n    n_filters = min(bins_per_octave, n_bins)\n\n    len_orig = len(y)\n\n    # Relative difference in frequency between any two consecutive bands\n    alpha = 2.0 ** (1.0 / bins_per_octave) - 1\n\n    if fmin is None:\n        # C1 by default\n        fmin = librosa.note_to_hz(\"C1\")\n\n    if tuning is None:\n        tuning = librosa.pitch.estimate_tuning(y=y, sr=sr, bins_per_octave=bins_per_octave)\n\n    if gamma is None:\n        gamma = 24.7 * alpha / 0.108\n\n    if dtype is None:\n        dtype = dtype_r2c(y.dtype)\n\n    # Apply tuning correction\n    fmin = fmin * 2.0 ** (tuning / bins_per_octave)\n\n    # First thing, get the freqs of the top octave\n    freqs = librosa.time_frequency.cqt_frequencies(n_bins, fmin, bins_per_octave=bins_per_octave)[\n        -bins_per_octave:\n    ]\n\n    fmin_t = np.min(freqs)\n    fmax_t = np.max(freqs)\n\n    # Determine required resampling quality\n    Q = float(filter_scale) / alpha\n    filter_cutoff = (\n        fmax_t * (1 + 0.5 * librosa.filters.window_bandwidth(window) / Q) + 0.5 * gamma\n    )\n    nyquist = sr / 2.0\n\n    auto_resample = False\n    if not res_type:\n        auto_resample = True\n        if filter_cutoff < librosa.audio.BW_FASTEST * nyquist:\n            res_type = \"kaiser_fast\"\n        else:\n            res_type = \"kaiser_best\"\n\n    downsample_count1 = max(\n        0, int(np.ceil(np.log2(librosa.audio.BW_FASTEST * nyquist / filter_cutoff)) - 1) - 1\n    )\n\n    def num_two_factors(x):\n        if x <= 0:\n            return 0\n        num_twos = 0\n        while x % 2 == 0:\n            num_twos += 1\n            x //= 2\n\n        return num_twos\n    num_twos=num_two_factors(hop_length)\n    downsample_count2 = max(0, num_twos - n_octaves + 1)\n    downsample_count = min(downsample_count1, downsample_count2)\n\n\n    vqt_resp = []\n\n    # Make sure our hop is long enough to support the bottom octave\n\n    num_twos=num_two_factors(hop_length)\n\n\n    #num_twos = __num_two_factors(hop_length)\n    if num_twos < n_octaves - 1:\n        raise ParameterError(\n            \"hop_length must be a positive integer \"\n            \"multiple of 2^{0:d} for {1:d}-octave CQT/VQT\".format(\n                n_octaves - 1, n_octaves\n            )\n        )\n\n    # Now do the recursive bit\n    my_y, my_sr, my_hop = y, sr, hop_length\n    def sparsify_rows(x, quantile=0.01, dtype=None):\n        \"\"\"Return a row-sparse matrix approximating the input\n\n        Parameters\n        ----------\n        x : np.ndarray [ndim <= 2]\n            The input matrix to sparsify.\n\n        quantile : float in [0, 1.0)\n            Percentage of magnitude to discard in each row of ``x``\n\n        dtype : np.dtype, optional\n            The dtype of the output array.\n            If not provided, then ``x.dtype`` will be used.\n\n        Returns\n        -------\n        x_sparse : ``scipy.sparse.csr_matrix`` [shape=x.shape]\n            Row-sparsified approximation of ``x``\n\n            If ``x.ndim == 1``, then ``x`` is interpreted as a row vector,\n            and ``x_sparse.shape == (1, len(x))``.\n\n        Raises\n        ------\n        ParameterError\n            If ``x.ndim > 2``\n\n            If ``quantile`` lies outside ``[0, 1.0)``\n        \"\"\"\n\n        if x.ndim == 1:\n            x = x.reshape((1, -1))\n\n        elif x.ndim > 2:\n            raise ParameterError(\n                \"Input must have 2 or fewer dimensions. \"\n                \"Provided x.shape={}.\".format(x.shape)\n            )\n\n        if not 0.0 <= quantile < 1:\n            raise ParameterError(\"Invalid quantile {:.2f}\".format(quantile))\n\n        if dtype is None:\n            dtype = x.dtype\n\n        x_sparse = scipy.sparse.lil_matrix(x.shape, dtype=dtype)\n\n        mags = np.abs(x)\n        norms = np.sum(mags, axis=1, keepdims=True)\n\n        mag_sort = np.sort(mags, axis=1)\n        cumulative_mag = np.cumsum(mag_sort / norms, axis=1)\n\n        threshold_idx = np.argmin(cumulative_mag < quantile, axis=1)\n\n        for i, j in enumerate(threshold_idx):\n            idx = np.where(mags[i] >= mag_sort[i, j])\n            x_sparse[i, idx] = x[i, idx]\n\n        return x_sparse.tocsr()\n\n    def cqt_filter_fft(\n        sr,\n        fmin,\n        n_bins,\n        bins_per_octave,\n        filter_scale,\n        norm,\n        sparsity,\n        hop_length=None,\n        window=\"hann\",\n        gamma=0.0,\n        dtype=np.complex,\n    ):\n        \"\"\"Generate the frequency domain constant-Q filter basis.\"\"\"\n\n        basis, lengths = librosa.filters.constant_q(\n            sr,\n            fmin=fmin,\n            n_bins=n_bins,\n            bins_per_octave=bins_per_octave,\n            filter_scale=filter_scale,\n            norm=norm,\n            pad_fft=True,\n            window=window,\n        )\n\n        # Filters are padded up to the nearest integral power of 2\n        n_fft = basis.shape[1]\n\n        if hop_length is not None and n_fft < 2.0 ** (1 + np.ceil(np.log2(hop_length))):\n\n            n_fft = int(2.0 ** (1 + np.ceil(np.log2(hop_length))))\n\n        # re-normalize bases with respect to the FFT window length\n        basis *= lengths[:, np.newaxis] / float(n_fft)\n\n        # FFT and retain only the non-negative frequencies\n        fft = librosa.get_fftlib()\n        fft_basis = fft.fft(basis, n=n_fft, axis=1)[:, : (n_fft // 2) + 1]\n\n        # sparsify the basis\n        fft_basis = sparsify_rows(fft_basis, quantile=sparsity, dtype=dtype)\n\n        return fft_basis, n_fft, lengths\n\n\n    def cqt_response(y, n_fft, hop_length, fft_basis, mode, dtype=None):\n        \"\"\"Compute the filter response with a target STFT hop.\"\"\"\n\n        # Compute the STFT matrix\n        D = librosa.stft(\n            y, n_fft=n_fft, hop_length=hop_length, window=\"ones\", pad_mode=mode, dtype=dtype\n        )\n\n        # And filter response energy\n        return fft_basis.dot(D)\n\n    # Iterate down the octaves\n    for i in range(n_octaves):\n        # Resample (except first time)\n        if i > 0:\n            if len(my_y) < 2:\n                raise ParameterError(\n                    \"Input signal length={} is too short for \"\n                    \"{:d}-octave CQT/VQT\".format(len_orig, n_octaves)\n                )\n\n            my_y = librosa.audio.resample(my_y, 2, 1, res_type=res_type, scale=True)\n\n            my_sr /= 2.0\n            my_hop //= 2\n\n        fft_basis, n_fft, _ = cqt_filter_fft(\n            my_sr,\n            fmin_t * 2.0 ** -i,\n            n_filters,\n            bins_per_octave,\n            filter_scale,\n            norm,\n            sparsity,\n            window=window,\n            gamma=gamma,\n            dtype=dtype,\n        )\n\n        # Re-scale the filters to compensate for downsampling\n        fft_basis[:] *= np.sqrt(2 ** i)\n\n        # Compute the vqt filter response and append to the stack\n        vqt_resp.append(\n            cqt_response(my_y, n_fft, my_hop, fft_basis, pad_mode, dtype=dtype)\n        )\n\n    def trim_stack(cqt_resp, n_bins, dtype):\n        \"\"\"Helper function to trim and stack a collection of CQT responses\"\"\"\n\n        max_col = min(c_i.shape[-1] for c_i in cqt_resp)\n        cqt_out = np.empty((n_bins, max_col), dtype=dtype, order=\"F\")\n\n        # Copy per-octave data into output array\n        end = n_bins\n        for c_i in cqt_resp:\n            # By default, take the whole octave\n            n_oct = c_i.shape[0]\n            # If the whole octave is more than we can fit,\n            # take the highest bins from c_i\n            if end < n_oct:\n                cqt_out[:end] = c_i[-end:, :max_col]\n            else:\n                cqt_out[end - n_oct : end] = c_i[:, :max_col]\n\n            end -= n_oct\n\n        return cqt_out\n\n    V = trim_stack(vqt_resp, n_bins, dtype)\n\n    if scale:\n        lengths = librosa.filters.constant_q_lengths(\n            sr,\n            fmin,\n            n_bins=n_bins,\n            bins_per_octave=bins_per_octave,\n            window=window,\n            filter_scale=filter_scale,\n        )\n        V /= np.sqrt(lengths[:, np.newaxis])\n    return V"}
{"example_id": "310", "answer": "import librosa\nimport numpy as np\nimport scipy\nfrom typing import Union, Optional\n\nDTypeLike = Union[np.dtype, type]\n\n\ndef compute_vqt(y: np.ndarray, sr: int) -> np.ndarray:\n    \n\n    return librosa.vqt(y, sr=sr)"}
{"example_id": "311", "answer": "import librosa\nimport numpy as np\nimport scipy\nfrom typing import Union, Optional\n\nDTypeLike = Union[np.dtype, type]\n\ndef compute_griffinlim_cqt(y: np.ndarray, sr: int, C, n_iter: int, hop_length: int, fmin: int, bins_per_octave: int, tuning: float, filter_scale: 1, norm: int, sparsity: float, window: str, scale: bool, pad_mode: str, res_type: str, dtype: DTypeLike, length: Optional[int], momentum: float, init: Optional[str]) -> np.ndarray:\n    rng = np.random.RandomState(seed=0)\n\n    if fmin is None:\n        fmin = librosa.note_to_hz(\"C1\")\n    \n    angles = np.empty(C.shape, dtype=np.complex64)\n    if init == \"random\":\n        angles[:] = np.exp(2j * np.pi * rng.rand(*C.shape))\n    elif init is None:\n        angles[:] = 1.0\n    \n    rebuilt = 0.0\n    \n    for _ in range(n_iter):\n        tprev = rebuilt\n    \n        inverse = librosa.constantq.icqt(\n        C * angles,\n        sr=sr,\n        hop_length=hop_length,\n        bins_per_octave=bins_per_octave,\n        fmin=fmin,\n        tuning=tuning,\n        filter_scale=filter_scale,\n        window=window,\n        length=length,\n        res_type=res_type,\n        )\n    \n        rebuilt = librosa.constantq.cqt(\n        inverse,\n        sr=sr,\n        bins_per_octave=bins_per_octave,\n        n_bins=C.shape[0],\n        hop_length=hop_length,\n        fmin=fmin,\n        tuning=tuning,\n        filter_scale=filter_scale,\n        window=window,\n        res_type=res_type,\n        )\n    \n        angles[:] = rebuilt - (momentum / (1 + momentum)) * tprev\n        angles[:] /= np.abs(angles) + 1e-16\n    \n    return  librosa.constantq.icqt(\n     C * angles,\n     sr=sr,\n     hop_length=hop_length,\n     bins_per_octave=bins_per_octave,\n     tuning=tuning,\n     filter_scale=filter_scale,\n     fmin=fmin,\n     window=window,\n     length=length,\n     res_type=res_type,\n    )\n    "}
{"example_id": "312", "answer": "import librosa\nimport numpy as np\nimport scipy\nfrom typing import Union, Optional\n\nDTypeLike = Union[np.dtype, type]\n\ndef compute_griffinlim_cqt(y: np.ndarray, sr: int, C, n_iter: int, hop_length: int, fmin: int, bins_per_octave: int, tuning: float, filter_scale: 1, norm: int, sparsity: float, window: str, scale: bool, pad_mode: str, res_type: str, dtype: DTypeLike, length: Optional[int], momentum: float, init: Optional[str]) -> np.ndarray:\n    rng = np.random.RandomState(seed=0)\n\n    return librosa.griffinlim_cqt(C, sr=sr, bins_per_octave=bins_per_octave, init=init)"}
{"example_id": "313", "answer": "import librosa\nimport numpy as np\nimport scipy\nimport scipy.optimize\nfrom typing import Union, Optional\n\nDTypeLike = Union[np.dtype, type]\n\n\ndef compute_mel_to_audio(y: np.ndarray, sr: int, S: np.ndarray, M: np.ndarray, n_fft: int, hop_length: Optional[int], win_length: Optional[int], window: str, center: bool, pad_mode: str, power: float, n_iter: int, length: Optional[int], dtype: DTypeLike) -> np.ndarray:\n    np.random.seed(seed=0)\n\n    def _nnls_obj(x, shape, A, B):\n        x = x.reshape(shape)\n\n        diff = np.dot(A, x) - B\n\n        value = 0.5 * np.sum(diff**2)\n\n        grad = np.dot(A.T, diff)\n\n        return value, grad.flatten()\n\n\n    def _nnls_lbfgs_block(A, B, x_init=None, **kwargs):\n        if x_init is None:\n            x_init = np.linalg.lstsq(A, B, rcond=None)[0]\n            np.clip(x_init, 0, None, out=x_init)\n\n        kwargs.setdefault('m', A.shape[1])\n\n        bounds = [(0, None)] * x_init.size\n        shape = x_init.shape\n\n        x, obj_value, diagnostics = scipy.optimize.fmin_l_bfgs_b(_nnls_obj, x_init,\n                                                                 args=(shape, A, B),\n                                                                 bounds=bounds,\n                                                                 **kwargs)\n        return x.reshape(shape)\n\n\n    def nnls(A, B, **kwargs):\n        if B.ndim == 1:\n            return scipy.optimize.nnls(A, B)[0]\n\n        n_columns = int((2**8 * 2**10)// (A.shape[-1] * A.itemsize))\n\n        if B.shape[-1] <= n_columns:\n            return _nnls_lbfgs_block(A, B, **kwargs).astype(A.dtype)\n\n        x = np.linalg.lstsq(A, B, rcond=None)[0].astype(A.dtype)\n        np.clip(x, 0, None, out=x)\n        x_init = x\n\n        for bl_s in range(0, x.shape[-1], n_columns):\n            bl_t = min(bl_s + n_columns, B.shape[-1])\n            x[:, bl_s:bl_t] = _nnls_lbfgs_block(A, B[:, bl_s:bl_t],\n                                                x_init=x_init[:, bl_s:bl_t],\n                                                **kwargs)\n        return x\n\n    rng = np.random.seed(seed=0)\n    def mel_to_stft(M, sr=22050, n_fft=2048, power=2.0, **kwargs):\n        mel_basis = librosa.filters.mel(sr, n_fft, n_mels=M.shape[0],\n                                **kwargs)\n\n        inverse = nnls(mel_basis, M)\n        return np.power(inverse, 1./power, out=inverse)\n\n\n    stft = mel_to_stft(M, sr=sr, n_fft=n_fft, power=power)\n    def griffinlim(S, n_iter=32, hop_length=None, win_length=None, window='hann', \n                   center=True, dtype=np.float32, length=None, pad_mode='reflect',\n                   momentum=0.99, random_state=None):\n        rng = np.random\n        n_fft = 2 * (S.shape[0] - 1)\n\n        angles = np.exp(2j * np.pi * rng.rand(*S.shape))\n\n        rebuilt = 0.\n\n        for _ in range(n_iter):\n            tprev = rebuilt\n            inverse = librosa.istft(S * angles, hop_length=hop_length, win_length=win_length,\n                             window=window, center=center, dtype=dtype, length=length)\n            rebuilt = librosa.stft(inverse, n_fft=n_fft, hop_length=hop_length,win_length=win_length, window=window, center=center, pad_mode=pad_mode)\n\n            angles[:] = rebuilt - (momentum / (1 + momentum)) * tprev\n            angles[:] /= np.abs(angles) + 1e-16\n        return librosa.istft(S * angles, hop_length=hop_length, win_length=win_length,\n                     window=window, center=center, dtype=dtype, length=length)\n    return griffinlim(stft, n_iter=n_iter, hop_length=hop_length, win_length=win_length,\n                          window=window, center=center, dtype=dtype, length=length,\n                          pad_mode=pad_mode)"}
{"example_id": "314", "answer": "import librosa\nimport numpy as np\nimport scipy\nimport scipy.optimize\nfrom typing import Union, Optional\n\nDTypeLike = Union[np.dtype, type]\n\n\ndef compute_mel_to_audio(y: np.ndarray, sr: int, S: np.ndarray, M: np.ndarray, n_fft: int, hop_length: Optional[int], win_length: Optional[int], window: str, center: bool, pad_mode: str, power: float, n_iter: int, length: Optional[int], dtype: DTypeLike) -> np.ndarray:\n    np.random.seed(seed=0)\n\n    return librosa.feature.inverse.mel_to_audio(M)"}
{"example_id": "315", "answer": "import librosa\nimport numpy as np\nimport scipy\n\ndef compute_mfcc_to_mel(mfcc: np.ndarray, n_mels: int=128, dct_type: int=2, norm: str='ortho', ref: float=1.0) -> np.ndarray:\n    \"\"\"\n    Invert Mel-frequency cepstral coefficients to approximate a Mel power spectrogram.\n\n    Parameters:\n        mfcc (np.ndarray): Mel-frequency cepstral coefficients.\n        n_mels (int): Number of Mel bands to generate.\n        dct_type (int): Type of DCT to use.\n        norm (str): Normalization to use.\n        ref: Reference power for (inverse) decibel calculation\n\n    Returns:\n        An approximate Mel power spectrum recovered from mfcc.        \n    \"\"\"\n    np.random.seed(seed=0)\n\n    logmel = scipy.fftpack.idct(mfcc, axis=0, type=dct_type, norm=norm, n=n_mels)\n    return librosa.db_to_power(logmel, ref=ref)\n"}
{"example_id": "316", "answer": "import librosa\nimport numpy as np\nimport scipy\n\ndef compute_mfcc_to_mel(mfcc: np.ndarray, n_mels: int=128, dct_type: int=2, norm: str='ortho', ref: float=1.0) -> np.ndarray:\n    \"\"\"\n    Invert Mel-frequency cepstral coefficients to approximate a Mel power spectrogram.\n\n    Parameters:\n        mfcc (np.ndarray): Mel-frequency cepstral coefficients.\n        n_mels (int): Number of Mel bands to generate.\n        dct_type (int): Type of DCT to use.\n        norm (str): Normalization to use.\n        ref: Reference power for (inverse) decibel calculation\n\n    Returns:\n        An approximate Mel power spectrum recovered from mfcc.        \n    \"\"\"    \n    np.random.seed(seed=0)\n\n    return librosa.feature.inverse.mfcc_to_mel(mfcc)"}
{"example_id": "317", "answer": "import numpy as np\nfrom PIL import Image, ImageChops\n\n\ndef imaging(img1: Image, img2: Image) -> Image:\n    \n\n    def create(imIn1, imIn2, mode=None):\n        if imIn1.shape != imIn2.shape:\n            return None\n        return np.empty_like(imIn1, dtype=np.uint8)\n\n    def imaging_overlay(imIn1, imIn2):\n        imOut = create(imIn1, imIn2)\n        if imOut is None:\n            return None\n        \n        ysize, xsize, _ = imOut.shape\n        for y in range(ysize):\n            for x in range(xsize):\n                for c in range(3):  # Loop over RGB channels\n                    in1, in2 = int(imIn1[y, x, c]), int(imIn2[y, x, c])\n                    if in1 < 128:\n                        imOut[y, x, c] = np.clip((in1 * in2) // 127, 0, 255)\n                    else:\n                        imOut[y, x, c] = np.clip(255 - (((255 - in1) * (255 - in2)) // 127), 0, 255)\n        \n        return imOut\n\n    return imaging_overlay(np.array(img1), np.array(img2))"}
{"example_id": "318", "answer": "import numpy as np\nfrom PIL import Image, ImageChops\n\ndef imaging(img1: Image, img2: Image) -> Image:\n    \n\n    def create(imIn1, imIn2, mode=None):\n        if imIn1.shape != imIn2.shape:\n            return None\n        return np.empty_like(imIn1, dtype=np.uint8)\n\n    def imaging_softlight(imIn1, imIn2):\n        if imIn1.shape != imIn2.shape:\n            return None\n        \n        imOut = create(imIn1, imIn2)\n        ysize, xsize, _ = imOut.shape\n        for y in range(ysize):\n            for x in range(xsize):\n                for c in range(3):  # Loop over RGB channels\n                    in1, in2 = int(imIn1[y, x, c]), int(imIn2[y, x, c])\n                    imOut[y, x, c] = int((((255 - in1) * (in1 * in2)) // 65536) +  (in1 * (255 - ((255 - in1) * (255 - in2) // 255))) // 255)\n        return imOut\n    return imaging_softlight(np.array(img1), np.array(img2))"}
{"example_id": "319", "answer": "import numpy as np\nfrom PIL import Image, ImageChops\n\n\ndef imaging(img1: Image, img2: Image) -> Image:\n    \n\n    def create(imIn1, imIn2, mode=None):\n        if imIn1.shape != imIn2.shape:\n            return None\n        return np.empty_like(imIn1, dtype=np.uint8)\n\n    def imaging_hardlight(imIn1, imIn2):\n        imOut = create(imIn1, imIn2)\n        if imOut is None:\n            return None\n        \n        ysize, xsize, _ = imOut.shape\n        for y in range(ysize):\n            for x in range(xsize):\n                for c in range(3):  # Loop over RGB channels\n                    in1, in2 = int(imIn2[y, x, c]), int(imIn1[y, x, c])\n                    if in1 < 128:\n                        imOut[y, x, c] = np.clip((in1 * in2) // 127, 0, 255)\n                    else:\n                        imOut[y, x, c] = np.clip(255 - (((255 - in1) * (255 - in2)) // 127), 0, 255)\n        \n        return imOut\n\n    return imaging_hardlight(np.array(img1), np.array(img2))"}
{"example_id": "320", "answer": "import numpy as np\nfrom PIL import Image, ImageChops\n\n\ndef imaging(img1: Image, img2: Image) -> Image:\n    \n    return ImageChops.overlay(img1, img2)"}
{"example_id": "321", "answer": "import numpy as np\nfrom PIL import Image, ImageChops\n\ndef imaging(img1: Image, img2: Image) -> Image:\n    return ImageChops.soft_light(img1, img2)"}
{"example_id": "322", "answer": "import numpy as np\nfrom PIL import Image, ImageChops\n\ndef imaging(img1: Image, img2: Image) -> Image:\n    return ImageChops.hard_light(img1, img2)"}
{"example_id": "323", "answer": "from tqdm import tqdm\n\ndef infinite():\n    i = 0\n    while True:\n        yield i\n        i += 1\n        if i == 1000:\n          return\n\n# Define the total in sol_dict['total'] and use it.\nsol_dict = {\"total\":0}\nsol_dict['total'] = None\nprogress_bar = tqdm(infinite(), total=sol_dict['total'])\nfor progress in progress_bar:\n    progress_bar.set_description(f\"Processing {progress}\")"}
{"example_id": "324", "answer": "from tqdm import tqdm\n\ndef infinite():\n    i = 0\n    while True:\n        yield i\n        i += 1\n        if i == 1000:\n          return\n\n# Define the total in sol_dict['total'] and use it.\nsol_dict = {\"total\":0}\nsol_dict['total'] = float('inf')\nprogress_bar = tqdm(infinite(), total=sol_dict['total'])\nfor progress in progress_bar:\n    progress_bar.set_description(f\"Processing {progress}\")"}
{"example_id": "325", "answer": "import kymatio\nimport torch\nfrom kymatio import Scattering2D\nfrom kymatio.scattering2d.frontend.torch_frontend import ScatteringTorch2D\nfrom typing import Tuple\n\ndef compute_scattering(a: torch.Tensor) -> Tuple[torch.Tensor, ScatteringTorch2D]:\n    \n\n    S = Scattering2D(2, (32, 32), frontend='torch')\n    S_a = S(a)\n    return S, S_a"}
{"example_id": "326", "answer": "import matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib.figure import Figure\nfrom matplotlib.axes import Axes\n\ndef modify(fig: Figure, ax: Axes) -> None:\n    \n    ax.set_xticks([], minor=False)\n    ax.set_yticks([], minor=False)"}
{"example_id": "327", "answer": "import matplotlib.pyplot as plt\nfrom matplotlib.figure import Figure\nfrom matplotlib.axes import Axes\n\ndef modify(fig: Figure, ax: Axes) -> None:\n\n    ax.set_xticks([], False)\n    ax.set_yticks([], False)"}
{"example_id": "328", "answer": "import matplotlib.pyplot as plt\nfrom matplotlib.figure import Figure\nfrom matplotlib.axes import Axes\n\ndef modify(fig: Figure, ax: Axes) -> None:\n\n    ax.set_xticks([], [], minor=False)\n    ax.set_yticks([], [], minor=False)"}
{"example_id": "329", "answer": "import matplotlib.pyplot as plt\n\ndef use_seaborn() -> None:\n    \n    plt.style.use(\"seaborn\")"}
{"example_id": "330", "answer": "import matplotlib.pyplot as plt\n\ndef use_seaborn() -> None:\n    \n    plt.style.use(\"seaborn-v0_8\")\n"}
